[
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Cuong Nguyen",
    "section": "",
    "text": "gantt\n    %% title Timeline\n    dateFormat YYYY-MM-DD\n    %% tickInterval 1month\n    axisFormat %y-%m\n    section 1\n        Research Fellow - University of Surrey      :surrey, 2024-03-07, 2025-03-31\n    section 0\n        Research Associate - University of Adelaide :adelaide, 2022-04-08, 2024-01-31\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html",
    "href": "posts/Gauss-Newton-matrix/index.html",
    "title": "Outer product approximation of Hessian matrix",
    "section": "",
    "text": "Hessian matrix is heavily studied in the optimization community. The purpose is to utilize the second order derivative to optimize a function of interest (also known as Newton’s method). In machine learning, especially Bayesian inference, Hessian matrix can be found in some applications, such as Laplace’s method which approximates a distribution by a Gaussian distribution. Although Hessian matrix provides additional information which improves the convergence rate in optimization or reduces a complicated distribution to a Gaussian distribution, calculating a Hessian matrix often increases computation complexity. In neural networks where the number of model parameters is very large, Hessian matrix is often intractable due to the limited computation and memory.\nMany efficient approximations of Hessian matrix have been developed to either reduce the running time complexity or decompose the Hessian matrix to reduce the amount of memory storage. Hessian-free approaches which utilizes the Hessian-vector product are also attracted much research interest. This post will present an approximation of Hessian matrix using the outer product. Note that this approximation represents an approximated Hessian matrix by a set of matrices whose sizes are reasonable to store in GPU memory. The trade-off is that the running time complexity to obtain the Hessian matrix is still quadractic. Note that this approximation is also known as Gauss-Newton matrix."
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html#notations",
    "href": "posts/Gauss-Newton-matrix/index.html#notations",
    "title": "Outer product approximation of Hessian matrix",
    "section": "1 Notations",
    "text": "1 Notations\nBefore going into details, let’s define some notations used:\n\n\\{x_{i}, t_{i}\\}_{i = 1}^{N} is the input and label of data-point i-th,\n\\mathbf{w} \\in \\mathbb{R}^{W} is the parameter of the model of interest, or the weight of a neural network,\n\\ell(.) \\in \\mathbb{R} is the loss function, e.g. MSE or cross-entropy,\n\\mathbf{f}(x_{i}, \\mathbf{w}) \\in \\mathbb{R}^{C} is the pre-nonlinearity output of the neural network at the final layer that has C hidden units,\n\\sigma\\left[ \\mathbf{f}\\left(x_{i}, \\mathbf{w}\\right) \\right] \\in \\mathbb{R}^{C} is the activation output at the final layer. For example, in regression, \\sigma(z) = z is the identity function, or in logistic regression, \\sigma(.) is the sigmoid function, while in multi-class classification, \\sigma(.) is the softmax function,\n\nThe loss function of interest is defined as the sum of losses over each data point: \nL = \\sum_{i = 1}^{N} \\ell\\left( \\sigma(\\mathbf{f}(x_{i}, \\mathbf{w}), t_{i}\\right).\n Note that in the following, we will omit the notation of the label t_{i} from the loss \\ell(.) to make the notation unclutered."
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html#derivation-of-the-approximated-hessian-matrix",
    "href": "posts/Gauss-Newton-matrix/index.html#derivation-of-the-approximated-hessian-matrix",
    "title": "Outer product approximation of Hessian matrix",
    "section": "2 Derivation of the approximated Hessian matrix",
    "text": "2 Derivation of the approximated Hessian matrix\nAn element of the Hessian matrix can then be written as: \n\\begin{aligned}\n\\mathbf{H}_{jk} & = \\frac{\\partial}{\\partial\\mathbf{w}_{k}} \\left( \\frac{\\partial L}{\\partial \\mathbf{w}_{j}} \\right) = \\frac{\\partial}{\\partial\\mathbf{w}_{k}} \\left( \\sum_{i=1}^{N} \\frac{\\partial \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{w}_{j}} \\right) \\\\\n& = \\frac{\\partial}{\\partial \\mathbf{w}_{k}} \\left( \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\frac{\\partial\\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})} \\frac{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j}} \\right) \\quad \\text{\\textcolor{ForestGreen}{(chain rule)}}\\\\\n& = \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\frac{\\partial}{\\partial \\mathbf{w}_{k}} \\left( \\frac{\\partial \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})} \\frac{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j}} \\right).\n\\end{aligned}\n\nApplying the chain rule for the first term gives: \n\\begin{aligned}\n\\mathbf{H}_{jk} & = \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\left[ \\sum_{l=1}^{C} \\left( \\frac{\\partial^{2} \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w}) \\, \\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})} \\frac{\\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{k}} \\right) \\frac{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j}} \\right] \\\\\n& \\qquad \\qquad \\quad + \\frac{\\partial \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})} \\frac{\\partial^{2} \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j} \\, \\partial \\mathbf{w}_{k}}.\n\\end{aligned}\n\nRearranging gives: \n\\begin{aligned}\n\\mathbf{H}_{jk} & = \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\frac{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j}} \\sum_{l=1}^{C} \\frac{\\partial^{2} \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w}) \\, \\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})} \\frac{\\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{k}} \\\\\n& \\quad + \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\underbrace{\\frac{\\partial \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}}_{\\approx 0} \\frac{\\partial^{2} \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j} \\, \\partial \\mathbf{w}_{k}}.\n\\end{aligned}\n\nNear the optimum, the scalar \\mathbf{f}_{c} would be very closed to its target \\mathbf{t}_{ic}. Hence, the derivative of the loss w.r.t. \\mathbf{f}_{c} is very small, and we can approximate the Hessian as: \n\\mathbf{H}_{jk} \\approx \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\frac{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j}} \\sum_{l=1}^{C} \\frac{\\partial^{2} \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w}) \\, \\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})} \\frac{\\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{k}}.\n\nRewriting this with matrix notation yields a much simpler formulation: \n\\boxed{\n\\mathbf{H} \\approx \\sum_{i=1}^{N} \\mathbf{J}_{fi}^{\\top} \\mathbf{H}_{\\sigma i} \\mathbf{J}_{fi},\n}\n where: \n\\begin{aligned}\n\\mathbf{J}_{fi} & = \\nabla_{\\mathbf{w}} \\mathbf{f}(x_{i}, \\mathbf{w}) \\in \\mathbb{R}^{C \\times W} \\quad \\text{\\textcolor{ForestGreen}{(Jacobian matrix of \\textbf{f} w.r.t. \\textbf{w})}}\\\\\n\\mathbf{H}_{\\sigma i} & = \\nabla_{\\mathbf{f}}^{2} \\ell\\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w} \\right) \\right] \\in \\mathbb{R}^{C \\times C} \\quad \\text{\\textcolor{ForestGreen}{(Hessian of loss w.r.t. \\textbf{f})}}.\n\\end{aligned}\n\nNote that the Hessian matrix \\mathbf{H}_{\\sigma} can be manually calculated.\n\nRemark. Instead of storing the Hessian matrix \\mathbf{H} with size {W \\times W} which needs a large amount of memory, we can store the two matrices \\{\\mathbf{J}_{fi}, \\mathbf{H}_{\\sigma i}\\}_{i=1}^{N}. This will reduce the amount of memory required. Of course, the trade-off is the increasing of the computation when performing the multiplication to obtain the Hessian matrix \\mathbf{H}.\n\nThe following section will present how to calculate the matrix \\mathbf{H}_{\\sigma} for some commonly-used losses."
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html#derivation-for-mathbfh_sigma",
    "href": "posts/Gauss-Newton-matrix/index.html#derivation-for-mathbfh_sigma",
    "title": "Outer product approximation of Hessian matrix",
    "section": "3 Derivation for \\mathbf{H}_{\\sigma}",
    "text": "3 Derivation for \\mathbf{H}_{\\sigma}\n\n3.1 Mean square error in regression\nIn the regression:\n\nC = 1\n\\sigma(.) is the identity function\n\\ell(f(x_{i}, \\mathbf{w}) = \\frac{1}{2} \\left( f(x_{i}, \\mathbf{w}) - t_{i} \\right)^{2}.\n\nHence, \\mathbf{H}_{\\sigma} = \\mathbf{I}_{1}, resulting in: \n\\boxed{\n    \\mathbf{H} = \\sum_{i=1}^{N} \\mathbf{J}_{fi}^{\\top} \\mathbf{J}_{fi},\n}\n which agrees with the results in (Bishop and Nasrabadi 2006 - Eq.(5.84)).\n\n\n3.2 Logistic regression\nIn this case:\n\nC = 1\n\\sigma(.) is the sigmoid function\n\\ell(\\sigma(f(x_{i}, \\mathbf{w})) = - t_{i} \\ln \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) - (1 - t_{i}) \\ln \\left( 1 - \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\right).\n\nThe first derivative is expressed as: \n\\frac{\\partial \\ell(\\sigma(f(x_{i}, \\mathbf{w}))}{\\partial f(x_{i}, \\mathbf{w})} = - t_{i} \\left( 1 - \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\right) + (1 - t_{i}) \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) = \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) - t_{i}.\n\nThe second derivative is therefore: \n\\frac{\\partial^{2} \\ell(\\sigma(f(x_{i}, \\mathbf{w}))}{\\partial f(x_{i}, \\mathbf{w})^{2}} = \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\left[ 1 - \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\right].\n\nHence: \n\\boxed{\n    \\mathbf{H} \\approx \\sum_{i=1}^{n} \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\left[ 1 - \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\right] \\mathbf{J}_{fi}^{\\top} \\mathbf{J}_{fi},\n}\n which agrees with the result derived in the literature (Bishop and Nasrabadi 2006 - Eq. (5.85)).\n\n\n3.3 Cross entropy loss in classification\nIn this case:\n\n\\sigma(\\mathbf{f}) is the softmax function,\n\\ell(\\sigma(\\mathbf{f}(x_{i}, \\mathbf{w}))) = -\\sum_{c=1}^{C} \\mathbf{t}_{ic} \\ln \\sigma_{c}(\\mathbf{f}(x_{i}, \\mathbf{w})).\n\nAccording to the definition of the softmax function: \n    \\sigma_{c} \\left( \\mathbf{f} \\right) = \\frac{\\exp(\\mathbf{f}_{c})}{\\sum_{k=1}^{C} \\exp(\\mathbf{f}_{k})}.\n\nHence, the derivative can be written as: \n    \\frac{\\partial \\sigma_{c}(\\mathbf{f})}{\\partial \\mathbf{f}_{c}} = \\frac{\\exp(\\mathbf{f}_{c}) \\sum_{k=1}^{C} \\exp(\\mathbf{f}_{k}) - \\exp(2 \\mathbf{f}_{c})}{\\left[ \\sum_{k=1}^{C} \\exp(\\mathbf{f}_{k}) \\right]^{2}} = \\sigma_{c}(\\mathbf{f}) \\left[ 1 - \\sigma_{c}(\\mathbf{f}) \\right],\n and \n    \\frac{\\partial \\sigma_{c}(\\mathbf{f})}{\\partial \\mathbf{f}_{k}} = - \\sigma_{c}(\\mathbf{f}) \\sigma_{k}(\\mathbf{f}), \\forall k \\neq j.\n\nAn element of the Jacobian vector of the loss w.r.t. \\mathbf{f} can be written as: \n\\begin{aligned}\n    \\frac{\\partial \\ell(\\sigma(\\mathbf{f}(x_{i}, \\mathbf{w})))}{\\partial \\mathbf{f}_{c}(x_{i}, \\mathbf{w})} & = - \\sum_{k=1}^{C} \\frac{\\mathbf{t}_{ik}}{\\sigma_{k}(\\mathbf{f})} \\frac{\\partial \\sigma_{k}(\\mathbf{f})}{\\partial \\mathbf{f}_{c}} \\\\\n    & = - \\mathbf{t}_{ic} \\left[ 1 - \\sigma_{c}(\\mathbf{f}) \\right] + \\sum_{\\substack{k=1\\\\k \\neq c}}^{C} \\mathbf{t}_{ik} \\sigma_{c}(\\mathbf{f}) \\\\\n    & = - \\mathbf{t}_{ic} + \\sigma_{c}(\\mathbf{f}) \\underbrace{\\sum_{k=1}^{C} \\mathbf{t}_{ik}}_{1}\\\\\n    & = \\sigma_{c}(\\mathbf{f}) - \\mathbf{t}_{ic}.\n\\end{aligned}\n\nHence, the Jacobian vector can be expressed as: \n    \\nabla_{\\mathbf{f}} \\ell(\\sigma(\\mathbf{f}(x_{i}, \\mathbf{w}))) = \\sigma(\\mathbf{f}(x_{i}, \\mathbf{w})) - \\mathbf{t}_{i}.\n\nThe Hessian matrix is given as: \n    \\nabla_{\\mathbf{f}}^{2} \\ell(\\sigma(\\mathbf{f}(x_{i}, \\mathbf{w}))) = \\nabla_{\\mathbf{f}} \\sigma(\\mathbf{f}(x_{i}, \\mathbf{w})).\n\nOr, in the explicit matrix form: \n    \\mathbf{H}_{\\sigma} = \\begin{bmatrix}\n    \\sigma_{1}(\\mathbf{f}) \\left[ 1 - \\sigma_{1}(\\mathbf{f}) \\right] & - \\sigma_{1}(\\mathbf{f}) \\sigma_{2}(\\mathbf{f}) & - \\sigma_{1}(\\mathbf{f}) \\sigma_{3}(\\mathbf{f}) & \\ldots & - \\sigma_{1}(\\mathbf{f}) \\sigma_{C}(\\mathbf{f})\\\\\n    - \\sigma_{2}(\\mathbf{f}) \\sigma_{1}(\\mathbf{f}) & \\sigma_{2}(\\mathbf{f}) \\left[ 1 - \\sigma_{2}(\\mathbf{f}) \\right] & - \\sigma_{2}(\\mathbf{f}) \\sigma_{3}(\\mathbf{f}) & \\ldots & - \\sigma_{2}(\\mathbf{f}) \\sigma_{C}(\\mathbf{f})\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n    - \\sigma_{C}(\\mathbf{f}) \\sigma_{1}(\\mathbf{f}) & - \\sigma_{C}(\\mathbf{f}) \\sigma_{2}(\\mathbf{f}) & - \\sigma_{C}(\\mathbf{f}) \\sigma_{3}(\\mathbf{f}) & \\ldots & \\sigma_{C}(\\mathbf{f}) \\left[ 1 - \\sigma_{C}(\\mathbf{f}) \\right]\n    \\end{bmatrix}."
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html#conclusion",
    "href": "posts/Gauss-Newton-matrix/index.html#conclusion",
    "title": "Outer product approximation of Hessian matrix",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nIn this post, we derive an approximation of the Hessian matrix. The Gauss-Newton matrix is a good approximation since it is positive-definite and more efficient to store under the form of a set of smaller matrices. Of course, we have not got away from the curse of dimensionality since the running time complexity to obtain the Hessian matrix is still quadratic w.r.t. the number of the model parameters. One final note is that one should use the approximated Hessian matrix with care since the approximation is assumed to be near the minimal value of the considered loss function."
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html#references",
    "href": "posts/Gauss-Newton-matrix/index.html#references",
    "title": "Outer product approximation of Hessian matrix",
    "section": "5 References",
    "text": "5 References\n\n\nBishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html",
    "href": "posts/bias-variance-decomposition/index.html",
    "title": "Bias - variance decomposition",
    "section": "",
    "text": "Bias and variance decomposition is one of the key tools to understand machine learning. However, conventional discussion about bias - variance decomposition revolves around the square loss (also known as mean square error). It is unclear whether such decomposition is still valid for some common loss functions, such as 0-1 loss or cross-entropy loss used in classification. This post is to present the decomposition for those losses following the unified framework of bias and variance decomposition from (Domingos 2000), its extended study on Bregman divergence with un-bounded support from (Pfau 2013) and the special case about Kullback-Leibler (KL) divergence (Heskes 1998)."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#notations",
    "href": "posts/bias-variance-decomposition/index.html#notations",
    "title": "Bias - variance decomposition",
    "section": "1 Notations",
    "text": "1 Notations\nThe notations are similar to the ones in (Domingos 2000), but for C-class classification.\n\nNotations used in the bias-variance decomposition.\n\n\n\n\n\n\nNotation\nDescription\n\n\n\n\n\\mathbf{x}\nan input instance in \\mathcal{X} \\subseteq \\in \\mathbb{R}^{d}\n\n\n\\Delta_{K}\nthe K-dimensional simplex \\equiv \\{\\mathbf{v} \\in \\mathbb{R}^{K + 1}_{+}: \\mathbf{v}^{\\top} \\pmb{1} = 1\\}\n\n\n\\Delta_{K}\nthe K-dimensional simplex \\equiv \\{\\mathbf{v} \\in \\mathbb{R}^{K + 1}_{+}: \\mathbf{v}^{\\top} \\pmb{1} = 1\\}\n\n\n\\mathbf{t}\na label instance: \\mathbf{t} \\sim p(\\mathbf{t} | \\mathbf{x}), for example: (i) one-hot vector if p(\\mathbf{t} | \\mathbf{x}) is a categorical distribution, or (ii) soft-label if p(\\mathbf{t} | \\mathbf{x}) is a Dirichlet or logistic normal distribution\n\n\n\\ell\nloss function \\ell: \\Delta_{C - 1} \\times \\Delta_{C - 1} \\to [0, +\\infty], e.g. 0-1 loss or cross-entropy loss\n\n\n\\mathbf{y}\npredicted label distribution: \\mathbf{y} = f(\\mathbf{x}) \\in \\Delta_{C - 1}\n\n\n\\mathcal{D}\nthe set of training sets"
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#terminologies",
    "href": "posts/bias-variance-decomposition/index.html#terminologies",
    "title": "Bias - variance decomposition",
    "section": "2 Terminologies",
    "text": "2 Terminologies\n\nDefinition 1 The optimal prediction \\mathbf{y}_{*} \\in \\Delta_{C - 1} of a target \\mathbf{t} is defined as follows: \n    \\mathbf{y}_{*} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell \\left( \\mathbf{t}, \\mathbf{y}^{\\prime} \\right) \\right].\n\n\n\nDefinition 2 The main model prediction for a loss function, \\ell, and the set of training sets, \\mathcal{D}, is defined as: \n    \\mathbf{y}_{m} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{\\mathcal{D}} \\left[ \\ell \\left(\\mathbf{y}, \\mathbf{y}^{\\prime} \\right) \\right].\n\n\n\nRemark. The defintions of optimal and main model predictions above assume that the loss function \\ell is symmetric in terms of the input arguments. For asymmetric loss function, such as Bregmand divergence or cross-entropy, the definitions of such predictions might be slightly changed at the order of the input arguments.\n\nGiven the definitions of \\mathbf{y}_{*} and \\mathbf{y}_{m}, the bias, variance and noise can be defined following the unified framework proposed in (Domingos 2000) as follows:\n\nDefinition 3 The bias of a learner on an example \\mathbf{x} is defined as: B(\\mathbf{x}) = \\ell \\left( \\mathbf{y}_{*}, \\mathbf{y}_{m} \\right).\n\n\nDefinition 4 The variance of a learner on an example \\mathbf{x} is defined as: V(\\mathbf{x}) = \\mathbb{E}_{\\mathcal{D}} \\left[ \\ell \\left( \\mathbf{y}_{m}, \\mathbf{y} \\right) \\right].\n\n\nDefinition 5 The noise of an example \\mathbf{x} is defined as: N(\\mathbf{x}) = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell(\\mathbf{t}, \\mathbf{y}_{*}) \\right].\n\nThe definitions of bias and variance above are quite intuitive comparing to other definitions in the literature. As \\mathbf{y}_{m} is the main model prediction, the bias B(\\mathbf{x}) measures the systematic deviation (loss) from the optimal (or true) label \\mathbf{y}_{*}, while the variance V(\\mathbf{x}) measures the loss induced due to the fluctuations of each model prediction \\mathbf{y} on different training datasets around the main prediction \\mathbf{y}_{m}. In addition, as the loss \\ell is non-negative, both the bias and variance are also non-negative.\nGiven the defintions of bias, variance and noise above, the unified decomposition proposed in (Domingos 2000) can be expressed as: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] & = \\textcolor{Crimson}{\\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m})} + c_{1} \\, \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}}[\\ell(\\mathbf{y}, \\mathbf{y}_{m})]} + c_{2} \\, \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})}[\\ell(\\mathbf{t}, \\mathbf{y_{*}})]} \\\\\n        & = \\textcolor{Crimson}{B(\\mathbf{x})} + c_{1} \\, \\textcolor{MidnightBlue}{V(\\mathbf{x})} + c_{2} \\, \\textcolor{Green}{N(\\mathbf{x})},\n    \\end{aligned}\n\\tag{1} where c_{1} and c_{2} are two scalars. For example, in MSE, c_{1} = c_{2} = 1.\nOf course, not all losses would satisfy the decomposition in Equation 1. However, as shown in (Domingos 2000 - Theorem 7), such decomposition can be used to bound the expected loss as long as the loss is metric. Nevertheless, in this post, we dicuss the composition on some common loss functions, such as 0-1 loss and Bregman divergence which includes MSE and Kullback-Leibler (KL) divergence."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#square-loss",
    "href": "posts/bias-variance-decomposition/index.html#square-loss",
    "title": "Bias - variance decomposition",
    "section": "3 Square loss",
    "text": "3 Square loss\nTo warm-up, we discuss a wellknown bias-variance decomposition in the literature. It is applied for MSE or square loss. Here, we use the notations of vectors instead of scalars as often seen in conventional analysis. We will derive a general decomposition for Bregman divergence in which MSE is a particular case in a later section.\n\nTheorem 1 When the loss is the square loss: \\ell(\\mathbf{y}_{1}, \\mathbf{y}_{2}) = || \\mathbf{y}_{1} - \\mathbf{y}_{2}||_{2}^{2}, then the expected loss on several training sets can be decomposed into: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\ell(\\mathbf{t}, \\mathbf{y}) & = \\textcolor{Crimson}{\\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m})} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [ \\ell(\\mathbf{y}_{m}, \\mathbf{y})]} + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ \\ell( \\mathbf{t}, \\mathbf{y}_{*} )]} \\\\\n        \\text{or: } \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y} ||_{2}^{2} & = \\underbrace{\\textcolor{Crimson}{|| \\mathbf{y}_{*} - \\mathbf{y}_{m} ||_{2}^{2}}}_{\\text{bias}} + \\underbrace{\\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} || \\mathbf{y}_{m} - \\mathbf{y} ||_{2}^{2}}}_{\\text{variance}} + \\underbrace{\\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y}_{*} ||_{2}^{2}}}_{\\text{noise}}.\n    \\end{aligned}\n\n\n\n\nPlease refer to the detailed proof here\n\n\nProof. Given the square loss, the optimal prediction can be determined as: \n    \\begin{aligned}\n        & \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y}^{\\prime} ||_{2}^{2} \\ge || \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\mathbf{t} \\right] - \\mathbf{y}^{\\prime} ||_{2}^{2} \\ge 0 \\quad \\text{(Jensen's inequality on L2-norm)}\\\\\n        \\implies & \\mathbf{y}_{*} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y}^{\\prime} ||_{2}^{2} = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}].\n    \\end{aligned}\n Similarly, the main model prediction can be obtained as: \\mathbf{y}_{m} = \\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}].\nThe expected loss can then be written as: \n    \\begin{aligned}\n        & \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y} ||_{2}^{2} \\\\\n        & = \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} (\\mathbf{t} - \\mathbf{y})^{\\top} (\\mathbf{t} - \\mathbf{y}) \\\\\n        & = \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left( (\\mathbf{t} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) + (\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}]) + (\\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}] - \\mathbf{y}) \\right)^{\\top} \\left( (\\mathbf{t} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) \\right. \\\\\n        & \\quad \\left. + (\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}]) + (\\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}] - \\mathbf{y}) \\right) \\\\\n        & = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] ||_{2}^{2} + || \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}] ||_{2}^{2} + \\mathbb{E}_{\\mathcal{D}} || \\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}] - \\mathbf{y} ||_{2}^{2} \\\\\n        & = \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y}_{*} ||_{2}^{2}} + \\textcolor{Crimson}{|| \\mathbf{y}_{*} - \\mathbf{y}_{m} ||_{2}^{2}} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} || \\mathbf{y}_{m} - \\mathbf{y} ||_{2}^{2}}.\n    \\end{aligned}"
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#loss",
    "href": "posts/bias-variance-decomposition/index.html#loss",
    "title": "Bias - variance decomposition",
    "section": "4 0-1 loss",
    "text": "4 0-1 loss\n\nDefinition 6 The 0-1 loss is defined as: \n    \\ell(\\mathbf{y}_{1}, \\mathbf{y}_{2}) = \\Bbb{1} (\\mathbf{y}_{1}, \\mathbf{y}_{2}) = \\begin{cases}\n        0 & \\text{if } \\mathbf{y}_{1} = \\mathbf{y}_{2},\\\\\n        1 & \\text{if } \\mathbf{y}_{1} \\neq \\mathbf{y}_{2}.\n    \\end{cases}\n\n\n\n4.1 Binary classification\n\nTheorem 2 ((Domingos 2000 - Theorem 2)) The expected 0-1 loss in a binary classification setting can be written as: \n    \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell(\\mathbf{t}, \\mathbf{y}) \\right] = \\textcolor{Crimson}{\\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m})} + \\textcolor{Brown}{c} \\, \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} \\left[ \\mathbf{y}, \\mathbf{y}_{m} \\right]} + \\left[ 2 p_{\\mathcal{D}}(\\mathbf{y} = \\mathbf{y}_{*}) - 1 \\right]  \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell(\\mathbf{t}, \\mathbf{y}_{*}) \\right]},\n where: \n    \\textcolor{Brown}{c} = \\begin{cases}\n        1 & \\text{if } \\mathbf{y}_{m} = \\mathbf{y}_{*}\\\\\n        -1 & \\text{otherwise}.\n    \\end{cases}\n\n\n\n\nThe proof is copied in (Domingos 2000 - Theorem 2) for a self-contained discussion.\n\n\nProof. To prove the theorem, we calculate \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] and \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{t}, \\mathbf{y})], then combine both of them to complete the proof.\nFirst, we proceed to prove the followings: \n    \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}) + c_{0} \\, \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y}_{*})],\n\\tag{2} with c_{0} = 1 if \\mathbf{y} = \\mathbf{y}_{*} and c_{0} = -1 if \\mathbf{y} \\neq \\mathbf{y}_{*}.\nIf \\mathbf{y} = \\mathbf{y}_{*}, then Equation 2 is trivially true with c_{0} = 1. We next prove Equation 2 when \\mathbf{y} \\neq \\mathbf{y}_{*}. Since there are only two classes, if \\mathbf{y} \\neq \\mathbf{y}_{*} and \\mathbf{t} \\neq \\mathbf{y}_{*}, then \\mathbf{y} = \\mathbf{t} and vice versa. And since two events are quivalent, p(\\mathbf{y} = \\mathbf{t}) = p(\\mathbf{t} \\neq \\mathbf{y}_{*}). The expected 0-1 loss w.r.t. \\mathbf{t} can be written as: \n    \\begin{aligned}\n        \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] & = p(\\mathbf{t} = \\mathbf{y})\\\\\n        & = 1 - p(\\mathbf{t} \\neq \\mathbf{y}) \\\\\n        & = 1 - p (\\mathbf{t} = \\mathbf{y}_{*}) \\\\\n        & = 1 - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ \\ell(\\mathbf{t}, \\mathbf{y}_{*}) ]\\\\\n        & = \\ell(\\mathbf{y}_{*}, \\mathbf{y}) - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ \\ell(\\mathbf{t}, \\mathbf{y}_{*}) ].\n    \\end{aligned}\n This proves Equation 2.\nNext, we show that: \n    \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}_{*}, \\mathbf{y})] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m}) + \\textcolor{Brown}{c} \\, \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}, \\mathbf{y}_{m})].\n\\tag{3}\nIf \\mathbf{y}_{m} = \\mathbf{y}_{*}, then Equation 3 is trivially true with \\textcolor{Brown}{c} = 1. If \\mathbf{y}_{m} \\neq \\mathbf{y}_{*}, then \\mathbf{y}_{m} \\neq \\mathbf{y} implies that \\mathbf{y} = \\mathbf{y}_{*} and vice-versa. Thus, the expected 0-1 loss w.r.t. different training set can be expressed as: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}_{*}, \\mathbf{y})] & = p(\\mathbf{y} \\neq \\mathbf{y}_{*}) = 1 - p(\\mathbf{y} = \\mathbf{y}_{*}) = 1 - p(\\mathbf{y}_{m} \\neq \\mathbf{y})\\\\\n        & = 1 - \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}_{m}, \\mathbf{y})] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m}) - \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}_{m}, \\mathbf{y})].\n    \\end{aligned}\n\nThus, it proves Equation 3.\nFinally, we can combine both results in Equation 2 and Equation 3 to prove the theorem. Taking the expectation w.r.t. \\mathcal{D} on both sides of Equation 2 gives: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell(\\mathbf{t}, \\mathbf{y}) \\right] & = \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{t}, \\mathbf{y})] + c_{0} \\, \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y}_{*})]\\\\\n        & = \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{t}, \\mathbf{y})] + c_{0} \\, \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y}_{*})].\n    \\end{aligned}\n\nAnd since: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} [c_{0}] & = p(\\mathbf{y} = \\mathbf{y}_{*}) - p (\\mathbf{y} \\neq \\mathbf{y}_{*} = 2 p(\\mathbf{y} = \\mathbf{y}_{*}) - 1,\n    \\end{aligned}\n we can then obtain the result of the theorem by using Equation 3.\n\n\n\n\n4.2 Multi-class classification\n\nTheorem 3 The expected loss for 0-1 loss in a multiclass classification can be decomposed into: \n    \\begin{aligned}\n        & \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell(\\mathbf{t}, \\mathbf{y}) \\right] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m}) + \\textcolor{Blue}{c} \\, \\mathbb{E}_{\\mathcal{D}} \\left[ \\mathbf{y}, \\mathbf{y}_{m} \\right] \\\\\n        & \\quad + [ 2 p_{\\mathcal{D}} (\\mathbf{y} = \\mathbf{y}_{*}) - p_{\\mathcal{D}} (\\mathbf{y} \\neq \\mathbf{y}_{*}) p_{\\mathbf{t}}(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} \\neq \\mathbf{t}) ]  \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ \\ell(\\mathbf{t}, \\mathbf{y}_{*}) ],\n    \\end{aligned}\n where: \n    c = \\begin{cases}\n        1 & \\text{if } \\mathbf{y}_{m} = \\mathbf{y}_{*}\\\\\n        -p_{\\mathcal{D}} (\\mathbf{y} = \\mathbf{y}_{*} | \\mathbf{y} \\neq \\mathbf{y}_{m}) & \\text{otherwise}.\n    \\end{cases}\n\n\n\n\nThe proof is copied in (Domingos 2000 - Theorem 3) for a self-contained discussion.\n\n\nProof. The proof is similar to the binary classification where we decompose \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] and \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{t}, \\mathbf{y})]. The key difference is that when \\mathbf{y} \\neq \\mathbf{y}_{*} and \\mathbf{t} \\neq \\mathbf{y}_{*} no longer imply that \\mathbf{y} = \\mathbf{t}. Similarly, \\mathbf{y}_{m} \\neq \\mathbf{y}_{*} and \\mathbf{y}_{m} \\neq \\mathbf{y} no longer imply \\mathbf{y} = \\mathbf{y}_{*}.\nNow, we want to prove the following decomposition: \n    \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}) + c_{0} \\, \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y}_{*})],\n\\tag{4} where: \n    c_{0} = \\begin{cases}\n        -p(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} \\neq \\mathbf{t}) & \\text{when } \\mathbf{y} \\neq \\mathbf{y}_{*}\\\\\n        1 & \\text{when } \\mathbf{y} = \\mathbf{y}_{*}.\n    \\end{cases}\n\nWhen \\mathbf{y} = \\mathbf{y}_{*}, Equation 4 is trivially true with c_{0} = 1.\nWhen \\mathbf{y} \\neq \\mathbf{y}_{*}, the following fact is true: p(\\mathbf{y} = \\mathbf{t}| \\mathbf{y}_{*} = \\mathbf{t}, \\mathbf{y} \\neq \\mathbf{y}_{*}) = 0. To simplify the notation, the condition \\mathbf{y} \\neq \\mathbf{y}_{*} is omitted. Thus, applying the sum rule on the probability of predicted label gives: \n    \\begin{aligned}\n        p(\\mathbf{y} = \\mathbf{t}) & = \\underbrace{p(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} = \\mathbf{t})}_{0} \\, p(\\mathbf{y}_{*} + \\mathbf{t}) + p(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} \\neq \\mathbf{t}) \\, p(\\mathbf{y}_{*} \\neq \\mathbf{t}) \\\\\n        & = p(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} \\neq \\mathbf{t}) \\, p(\\mathbf{y}_{*} \\neq \\mathbf{t}).\n    \\end{aligned}\n\nThe expected loss w.r.t. \\mathbf{t} can be written as: \n    \\begin{aligned}\n        \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] & = p(\\mathbf{y} \\neq \\mathbf{t}) = 1 - p(\\mathbf{y} = \\mathbf{t})\\\\\n        & = 1 \\underbrace{- p(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} \\neq \\mathbf{t})}_{c_{0}} \\, p(\\mathbf{y}_{*} \\neq \\mathbf{t})\\\\\n        & = \\ell(\\mathbf{y}_{*}, \\mathbf{y}) + c_{0} \\, \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y}_{*})].\n    \\end{aligned}\n This proves Equation 4.\nSimilarly, one can prove the decomposition for the expected loss w.r.t. \\mathcal{D}: \n    \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}_{*}, \\mathbf{y})] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m}) + \\textcolor{Brown}{c} \\, \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}, \\mathbf{y}_{m})].\n\\tag{5}\nCombining the results in Equation 4 and Equation 5 in a similar manner in the case of binary classification completes the proof."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#bregman-divergence",
    "href": "posts/bias-variance-decomposition/index.html#bregman-divergence",
    "title": "Bias - variance decomposition",
    "section": "5 Bregman divergence",
    "text": "5 Bregman divergence\nThe derivation and discussion in this section is extracted from (Pfau 2013) with some modification to make notations consistent.\n\nDefinition 7 If F: \\mathcal{Y} \\to \\mathbb{R} is a strictly convex differentiable function, then Bregman divergence derived from F is a function D_{F}: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}_{+} defined as: \n    D_{F} (\\mathbf{t}, \\mathbf{y}) = F(\\mathbf{t}) - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\, (\\mathbf{t} - \\mathbf{y}).\n\n\n\nRemark. Given the defintion, Bregman divergence is not symmetric. It does not satisfy the triangle inequality. Thus, it is not a metric.\n\nSome examples of Bregman divergence:\n\nSquared Euclidean distance or square loss: D_{F}(\\mathbf{t}, \\mathbf{y}) = || \\mathbf{t} - \\mathbf{y} ||_{2}^{2} which is derived from the convex function F(\\mathbf{y}) = || \\mathbf{y} ||_{2}^{2}\nThe squared Mahalanobis distance: \n  D_{F}(\\mathbf{t}, \\mathbf{y}) = \\frac{1}{2} (\\mathbf{t} - \\mathbf{y})^{\\top} \\mathbf{Q} (\\mathbf{t} - \\mathbf{y})\n which is generated from the convex function: F(\\mathbf{y}) = \\frac{1}{2} \\mathbf{y}^{\\top} \\mathbf{Q} \\mathbf{y}\nThe KL divergence: \n  D_{F}(\\mathbf{t}, \\mathbf{y}) = \\mathrm{KL} [p(\\mathbf{t} | \\mathbf{x}) || \\mathbf{y}] = \\sum_{c = 1}^{C} p(\\mathbf{t} = \\mathrm{one-hot}(c) | \\mathbf{x}) \\frac{p(\\mathbf{t} = \\mathrm{one-hot}(c) | \\mathbf{x})}{\\mathbf{y}_{c}}\n which is generated from the negative entropy: \n  F(\\mathbf{y}) = \\sum_{c = 1}^{C} \\mathbf{y}_{c} \\ln \\mathbf{y}_{c}.\n\n\n\n5.1 Some properties of Bregman divergence\nThis sub-section presents some properties of Bregman divergence, which can then be used in the bias-variance decomposition. Note that the notation \\mathbf{y}_{*}, \\mathbf{y} and \\mathbf{y}_{m} used in this section do not need to be label distribution, but can simply be the output of a model (without any normalization, e.g. no softmax). The case for label distributions will be considered in the subsequent section where the loss function is KL divergence.\n\nLemma 1 (Part 1 of Lemma 0.1 in (Pfau 2013)) The mean prediction for Bregman divergence with un-bounded support has the following property: \n    \\mathbf{y}_{m} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}^{\\prime}, \\mathbf{y})] \\Leftrightarrow \\nabla F(\\mathbf{y}_{m}) = \\mathbb{E}_{\\mathcal{D}} [ \\nabla F(\\mathbf{y}) ].\n\n\n\n\nDetailed proof\n\n\nProof.  \n\n5.1.1 Necessary\nWhen \\mathbf{y}_{m} is a minimizer of \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}^{\\prime}, \\mathbf{y})] w.r.t. \\mathbf{y}^{\\prime}, the necessary condition of such statement is that its gradient is zero: \n    \\begin{aligned}\n        \\nabla_{\\mathbf{y}_{m}} \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}_{m}, \\mathbf{y})] & = \\nabla_{\\mathbf{y}_{m}} \\mathbb{E}_{\\mathcal{D}} [ F(\\mathbf{y}_{m}) - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\, (\\mathbf{y}_{m} - \\mathbf{y}) ] \\\\\n        & = \\nabla_{\\mathbf{y}_{m}} F(\\mathbf{y}_{m}) - \\nabla_{\\mathbf{y}_{m}} \\mathbb{E}_{\\mathcal{D}} [ \\nabla^{\\top} F(\\mathbf{y}) \\, \\mathbf{y}_{m}]\\\\\n        & = \\nabla_{\\mathbf{y}_{m}} F(\\mathbf{y}_{m}) - \\mathbb{E}_{\\mathcal{D}} [ \\nabla F(\\mathbf{y}) ] = 0.\n    \\end{aligned}\n \n    \\implies \\nabla F(\\mathbf{y}_{m}) = \\mathbb{E}_{\\mathcal{D}} [ \\nabla F(\\mathbf{y}) ].\n\n\n\n5.1.2 Sufficient\nSimilar to the necessary condition, one can easily show that \\nabla_{\\mathbf{y}_{m}} F(\\mathbf{y}_{m}) = \\mathbb{E}_{\\mathcal{D}} [ \\nabla F(\\mathbf{y}) ] implies that \\nabla_{\\mathbf{y}_{m}} \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}_{m}, \\mathbf{y})] = 0 (assume that \\mathbf{y}_{m} is independent from \\mathcal{D}). And since D_{F} is convex in its first argument \\mathbf{y}_{m} (one property of Bregman divergence), \\mathbf{y}_{m} is unique and the minimizer of \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}^{\\prime}, \\mathbf{y})].\n\n\n5.1.3 Note\nThe lemma only holds for Bregman divergence with un-bounded support, e.g. F is MSE. Otherwise, the gradient of \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}_{m}, \\mathbf{y})] w.r.t. the first argument would not be zero, but the Lagrangean that consists of the additional constraints would. This will be presented in the subsequent section where the loss function is the KL divergence.\n\n\n\n\nLemma 2 (Part 2 of Lemma 0.1 in (Pfau 2013)) The optimal prediction of Bregman divergence can be expressed as: \n    \\mathbf{y}_{*} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F} (\\mathbf{t}, \\mathbf{y}^{\\prime})] = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}].\n\n\n\n\nDetailed proof\n\n\nProof. The proof is quite straight-forward. One can calculate the gradient and solve for the root of the gradient as follows: \n    \\begin{aligned}\n        \\nabla_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F} (\\mathbf{t}, \\mathbf{y}^{\\prime})] & = \\nabla_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ F(\\mathbf{t}) - F(\\mathbf{y}^{\\prime}) - \\nabla^{\\top} F(\\mathbf{y}^{\\prime}) \\, (\\mathbf{t} - \\mathbf{y}^{\\prime}) ]\\\\\n        & = - \\nabla_{\\mathbf{y}^{\\prime}} F(\\mathbf{y}^{\\prime}) - \\nabla^{2} F(\\mathbf{y}^{\\prime}) \\times \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] + \\nabla^{2} F(\\mathbf{y}^{\\prime}) \\times \\mathbf{y}^{\\prime} + \\nabla_{\\mathbf{y}^{\\prime}} F(\\mathbf{y}^{\\prime}) \\\\\n        & = \\nabla^{2} F(\\mathbf{y}^{\\prime}) (\\mathbf{y}^{\\prime} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) = 0\n    \\end{aligned}\n And since F(.) is strictly convex, its Hessian matrix \\nabla^{2} F(\\mathbf{y}^{\\prime}) is positive definite and invertible. Hence, one can imply that: \n    \\mathbf{y}^{\\prime} = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}].\n\n\n\n\nLemma 3 (Part 1 of Theorem 0.1 in (Pfau 2013)) The expected Bregman divergences w.r.t. the set of training sets \\mathcal{D} have the following exact decomposition: \n    \\mathbb{E}_{\\mathcal{D}} [ D_{F} (\\mathbf{y}^{\\prime}, \\mathbf{y})] = D_{F}(\\mathbf{y}^{\\prime}, \\mathbf{y}_{m}) + \\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}_{m}, \\mathbf{y})],\n where: \\mathbf{y}_{m} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}^{\\prime}, \\mathbf{y})] is the mean prediction of the model of interest, and \\mathbf{y}^{\\prime} is a (random) prediction that is independent from \\mathcal{D}.\n\n\n\nDetailed proof\n\n\nProof. The is quite straight-forward: \n    \\begin{aligned}\n        & D_{F}(\\mathbf{y}^{\\prime}, \\mathbf{y}_{m}) + \\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}_{m}, \\mathbf{y})] \\\\\n        & = F(\\mathbf{y}^{\\prime}) - F(\\mathbf{y}_{m}) - \\nabla^{\\top} F(\\mathbf{y}_{m}) \\times (\\mathbf{y}^{\\prime} - \\mathbf{y}_{m}) + \\mathbb{E}_{\\mathcal{D}} [F(\\mathbf{y}_{m}) - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbf{y}_{m} - \\mathbf{y})] \\\\\n        & = F(\\mathbf{y}^{\\prime}) - \\nabla^{\\top} F(\\mathbf{y}_{m}) \\times (\\mathbf{y}^{\\prime} - \\mathbf{y}_{m}) - \\mathbb{E}_{\\mathcal{D}} [ F(\\mathbf{y}) + \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbf{y}_{m} - \\mathbf{y})]\\\\\n        & = F(\\mathbf{y}^{\\prime}) - \\mathbb{E}_{\\mathcal{D}} [ \\nabla^{\\top} F(\\mathbf{y}) ] \\times (\\mathbf{y}^{\\prime} - \\mathbf{y}_{m}) - \\mathbb{E}_{\\mathcal{D}} [ F(\\mathbf{y}) + \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbf{y}_{m} - \\mathbf{y})] \\\\\n        & = \\mathbb{E}_{\\mathcal{D}} [ F(\\mathbf{y}^{\\prime}) - F(\\mathbf{y}) - \\mathbb{E}_{\\mathcal{D}} [ \\nabla^{\\top} F(\\mathbf{y}) ] \\times (\\mathbf{y}^{\\prime} - \\mathbf{y}_{m} + \\mathbf{y}_{m} - \\mathbf{y}) ]\\\\\n        & = \\mathbb{E}_{\\mathcal{D}} [ D_{F} (\\mathbf{y}^{\\prime}, \\mathbf{y})].\n    \\end{aligned}\n The third inequality is due to Lemma 1.\n\n\n\nLemma 4 (Part 2 of Theorem 0.1 in (Pfau 2013)) The expected Bregman divergences w.r.t. the underlying label distribution p(\\mathbf{t} | \\mathbf{x}) have the following exact decomposition: \n    \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ D_{F} (\\mathbf{t}, \\mathbf{y})] = D_{F}(\\mathbf{y}_{*}, \\mathbf{y}) + \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F}(\\mathbf{t}, \\mathbf{y}_{*})],\n where \\mathbf{y}_{*} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F} (\\mathbf{t}, \\mathbf{y}^{\\prime})] = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] is the optimal prediction in Lemma 2.\n\n\n\nDetailed proof\n\n\nProof. The proof is quite straight-forward: \n    \\begin{aligned}\n        & D_{F}(\\mathbf{y}_{*}, \\mathbf{y}) + \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F}(\\mathbf{t}, \\mathbf{y}_{*})] \\\\\n        & = F(\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbf{y}) \\\\\n        & \\quad + \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [F(\\mathbf{t}) - F(\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) - \\nabla^{\\top} F(\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) \\times (\\mathbf{t} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}])] \\\\\n        & = - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbf{y}) + \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [F(\\mathbf{t}) - \\nabla^{\\top} F(\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) \\times (\\mathbf{t} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}])] \\\\\n        & = - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbf{y}) + \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [F(\\mathbf{t})]\\\\\n        & = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ F(\\mathbf{t}) - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbf{t} - \\mathbf{y})] \\\\\n        & = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ D_{F} (\\mathbf{t}, \\mathbf{y})].\n    \\end{aligned}\n\n\n\n\n\n5.2 Decomposition for Bregman divergence\nThe main result of bias-variance decomposition can be shown in the following:\n\nTheorem 4 The expected Bregman divergence on a set of training set \\mathcal{D} can be decomposed into: \n\\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F} (\\mathbf{t}, \\mathbf{y})] = \\textcolor{Crimson}{D_{F} (\\mathbf{y}_{*}, \\mathbf{y}_{m})} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}_{m}, \\mathbf{y})]} + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ D_{F} (\\mathbf{t}, \\mathbf{y}_{*}) \\right]}.\n\n\n\n\nDetailed proof\n\n\nProof. The proof is a consequence of the previous lemma: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F} (\\mathbf{t}, \\mathbf{y})] & = \\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}_{*}, \\mathbf{y}) + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F}(\\mathbf{t}, \\mathbf{y}_{*})]} ] \\\\\n        & = \\mathbb{E}_{\\mathcal{D}}[ D_{F}(\\mathbf{y}_{*}, \\mathbf{y})] + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F}(\\mathbf{t}, \\mathbf{y}_{*})]}\\\\\n        & = \\textcolor{Crimson}{D_{F} (\\mathbf{y}_{*}, \\mathbf{y}_{m})} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}_{m}, \\mathbf{y})]} + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F}(\\mathbf{t}, \\mathbf{y}_{*})]}.\n    \\end{aligned}\n The first equality is due to Lemma 4 and the last equality of the above equation is due to Lemma 3.\n\n\n\n5.2.1 Square loss\nAs MSE or square loss is a special instance of Bregman divergence, one can apply Theorem 4 to obtain the result for MSE as shown in Theorem 1."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#kullback-leibler-divergence",
    "href": "posts/bias-variance-decomposition/index.html#kullback-leibler-divergence",
    "title": "Bias - variance decomposition",
    "section": "6 Kullback-Leibler divergence",
    "text": "6 Kullback-Leibler divergence\nKL divergence is a special case of Bregman divergence. However, the analysis done for the Bregman divergence presented in this post is considered on un-bounded support, where the support space for the KL divergence is the probability space. In addition, KL divergence is used to measure the difference between 2 distributions. Such differences result in a different in terms of bias-variance decomposition.\nIn this section, \\mathbf{y}_{*}, \\mathbf{y} and \\mathbf{y}_{m} are label distributions or probabilities. They will be replaced by p(\\mathbf{t} | \\mathbf{x}), \\hat{p}(\\mathbf{t} | \\mathbf{x}) and p_{m}(\\mathbf{t} | \\mathbf{x}), respectively, to make the formulation easier to understand.\n\nLemma 5 (Main model prediction - Eq. (2.3) in (Heskes 1998)) The main model prediction when the loss is the KL divergence has the following property: \n    p_{m}(\\mathbf{t} | \\mathbf{x}) = \\arg\\min_{q(\\mathbf{t} | \\mathbf{x})} \\mathbb{E}_{\\mathcal{D}} [\\mathrm{KL} [q(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x})]] \\Rightarrow p_{m}(\\mathbf{t} | \\mathbf{x}) = \\frac{1}{Z} \\exp \\left[ \\mathbb{E}_{\\mathcal{D}} [\\ln \\hat{p}(\\mathbf{t} | \\mathbf{x})] \\right],\n where Z is a normalization constant independent of model prediction \\hat{p}(\\mathbf{t} | \\mathbf{x}).\n\n\n\nDetailed proof\n\n\nProof. The proof is similar to Lemma 1, except the constraint \\sum_{\\mathbf{t}} p_{m}(\\mathbf{t} | \\mathbf{x}) = 1 is taken into account. More specifically, the Lagrangean can be written as: \n    \\mathsf{L} = \\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [ p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x})]] + \\lambda (\\pmb{1}^{\\top} p_{m}(\\mathbf{t} | \\mathbf{x}) - 1),\n where \\lambda is the Lagrange multiplier.\nAt the optimal point, the gradient of the Lagrangean is zero: \n    \\begin{aligned}\n        \\nabla_{p_{m}(\\mathbf{t} | \\mathbf{x})} \\mathsf{L} & = \\ln p_{m}(\\mathbf{t} | \\mathbf{x}) - \\mathbb{E}_{\\mathcal{D}} [ \\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] + \\lambda = 0\\\\\n        & \\Rightarrow \\ln p_{m}(\\mathbf{t} | \\mathbf{x}) = \\mathbb{E}_{\\mathcal{D}} [ \\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] - \\lambda\\\\\n        & \\Rightarrow p_{m}(\\mathbf{t} | \\mathbf{x}) = \\underbrace{\\frac{1}{\\exp(\\lambda)}}_{\\frac{1}{Z}} \\exp[\\mathbb{E}_{\\mathcal{D}} [ \\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ]].\n    \\end{aligned}\n Actually, the normalization constant Z is the negative variance: \n    \\ln Z \\times \\pmb{1} = \\mathbb{E}_{\\mathcal{D}} [ \\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] - \\ln p_{m}(\\mathbf{t} | \\mathbf{x})] = \\mathbb{E}_{\\mathcal{D}} \\left[ \\ln \\frac{\\hat{p}(\\mathbf{t} | \\mathbf{x})}{p_{m}(\\mathbf{t} | \\mathbf{x})} \\right].\n Note that: \n    \\ln Z = \\mathbb{E}_{p_{m}(\\mathbf{t} | \\mathbf{x})} [ \\ln Z \\times \\pmb{1}].\n Thus: \n        \\ln Z = \\mathbb{E}_{p_{m}(\\mathbf{t} | \\mathbf{x})} \\mathbb{E}_{\\mathcal{D}} \\left[ \\ln \\frac{\\hat{p}(\\mathbf{t} | \\mathbf{x})}{p_{m}(\\mathbf{t} | \\mathbf{x})} \\right] = - \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} \\left[ \\mathrm{KL} [p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x})] \\right]}.\n\n\n\n\nTheorem 5 (Decomposition for KL divergence) The bias-variance decomposition for KL divergence can be presented as: \n    \\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [p(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x})] ] = \\textcolor{Crimson}{\\mathrm{KL} [ p(\\mathbf{t} | \\mathbf{x}) || p_{m}(\\mathbf{t} | \\mathbf{x}) ]} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [ p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] ]}.\n\n\n\nProof. The proof is quite straight-forward from Lemma 5.\n\nThe result in Theorem 5 does not consist of an intrinsic noise since the loss defined by KL divergence is based on the true label distribution instead of each sample \\mathbf{t}. To obtain the wellknown form of bias-variance decomposition based on label \\mathbf{t}, the negative log likelihood -\\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) is used as the loss function. Note that p_{m}(\\mathbf{t} | \\mathbf{x}) is still defined with KL divergence as the loss function.\nFrom Lemma 5, one can obtain: \n    \\mathbb{E}_{\\mathcal{D}} [ -\\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] = -\\ln p_{m}(\\mathbf{t} | \\mathbf{x}) + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [ p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] ]}.\n\nThus, the negative log-likelihood can be written as: \n    \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ -\\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] = -\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ \\ln p_{m}(\\mathbf{t} | \\mathbf{x})] + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [ p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] ]}.\n\nOr: \n    \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ -\\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] = \\textcolor{Crimson}{\\mathrm{KL}[p(\\mathbf{t} | \\mathbf{x}) || p_{m}(\\mathbf{t} | \\mathbf{x})]} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [ p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] ]} + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})}[-\\ln p(\\mathbf{t} | \\mathbf{x})]}.\n\nThe bias-variance decomposition for negative log-likelihood in this case consists of an intrinsic noise term which equals to the Shannon entropy of the true label distribution p(\\mathbf{t} | \\mathbf{x})."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#conclusion",
    "href": "posts/bias-variance-decomposition/index.html#conclusion",
    "title": "Bias - variance decomposition",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nIn general, the bias - variance decomposition might not be always in the form of bias, variance and noise as commonly seen in MSE. Here, we show that different loss function might have a different decomposition. Nevertheless, the two most common loss functions, i.e., MSE and KL divergence, share a similar form. Note that, one needs to be careful when applying such bias - variance decomposition due to their difference in terms of main model prediction and optimal label."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#references",
    "href": "posts/bias-variance-decomposition/index.html#references",
    "title": "Bias - variance decomposition",
    "section": "8 References",
    "text": "8 References\n\n\nDomingos, Pedro. 2000. “A Unified Bias-Variance Decomposition.” In International Conference on Machine Learning, 231–38.\n\n\nHeskes, Tom. 1998. “Bias/Variance Decompositions for Likelihood-Based Estimators.” Neural Computation 10 (6): 1425–33.\n\n\nPfau, David. 2013. “A Generalized Bias-Variance Decomposition for Bregman Divergences.” Unpublished Manuscript."
  },
  {
    "objectID": "posts/PAC-Bayes-bounds/index.html",
    "href": "posts/PAC-Bayes-bounds/index.html",
    "title": "PAC-Bayes bounds for generalisation error",
    "section": "",
    "text": "Properly approaximately correct (PAC) learning is a part of statistical machine learning which has been a fundamental course for most of graduate programs in machine learning. Its main idea is to upper-bound the true risk (or generalisation error) by the empirical risk with certain confidence level. In other words, it is often written in the following form: \n\\Pr (\\text{true risk} \\le \\text{empirical risk} + r(m, \\delta)) \\ge 1 - \\delta\n where \\Pr(A) is the probability of event A, \\delta \\in (0, 1] is the confidence parameter, and r(m, \\delta) – a function of sample size m and the confidence parameter \\delta – is the regularization that is satisfied: \n\\lim_{m \\to +\\infty} r(m, \\delta) = 0.\n PAC-Bayes upper generalization bound is a kind of PAC learning. It was firstly proposed in 1999 McAllester (1999), and has attracted much of research interest. There has been many subsequent improvements made to tighten further this classic PAC-Bayes bound or to extend it to more general loss functions. However, the classic PAC-Bayes theorem is still the backbone. In this post, I will show how to prove this interesting theorem."
  },
  {
    "objectID": "posts/PAC-Bayes-bounds/index.html#auxillary-lemmas",
    "href": "posts/PAC-Bayes-bounds/index.html#auxillary-lemmas",
    "title": "PAC-Bayes bounds for generalisation error",
    "section": "1 Auxillary lemmas",
    "text": "1 Auxillary lemmas\nTo prove the classic PAC-Bayes theorem, we need two auxilliary lemmas shown below.\n\n1.1 Change of measure inequality for Kullback-Leibler divergence\n\nLemma 1 (Banerjee 2006 - Lemma 1) For any measurable function \\phi(h) on a set of predictor under consideration \\mathcal{H}, and any distributions P and Q on \\mathcal{H}, the following inequality holds: \n\\mathbb{E}_{Q} [\\phi(h)] \\le \\mathrm{KL} [Q \\Vert P] + \\ln \\mathbb{E}_{P} [\\exp(\\phi(h))].\n Further, \n\\sup_{\\phi} \\mathbb{E}_{Q} [\\phi(h)] - \\ln \\mathbb{E}_{P} [\\exp(\\phi(h))] = \\mathrm{KL} [Q \\Vert P].\n\n\n\nProof. For any measurable function \\phi(h), the following holds: \n\\begin{aligned}\n    \\mathbb{E}_{Q} [\\phi(h)] & = \\mathbb{E}_{Q} \\left[ \\ln \\left( \\exp(\\phi(h)) \\frac{Q(h)}{P(h)} \\frac{P(h)}{Q(h)} \\right) \\right] \\\\\n    & = \\mathrm{KL} [Q \\Vert P] + \\mathbb{E}_{Q} \\left[ \\ln \\left( \\exp(\\phi(h)) \\frac{P(h)}{Q(h)} \\right) \\right] \\\\\n    & \\le \\mathrm{KL} [Q \\Vert P] + \\ln \\mathbb{E}_{Q} \\left[ \\exp(\\phi(h)) \\frac{P(h)}{Q(h)} \\right] \\\\\n    & \\qquad \\text{(Jensen's inequality)}\\\\\n    & = \\mathrm{KL} [Q \\Vert P] + \\ln \\mathbb{E}_{P} \\left[ \\exp(\\phi(h)) \\right].\n\\end{aligned}\n\nFor the second part of the lemma, we need to examine the equality condition of the Jensen’s inequality. Since \\ln(x) is a strictly concave function for x &gt; 0, it follows that the equality holds when: \n\\begin{aligned}\n    \\exp \\left( \\phi(h) \\right) & \\frac{P(h)}{Q(h)} = 1 \\\\\n    \\iff \\phi(h) & = \\ln \\left[ \\frac{Q(h)}{P(h)} \\right].\n\\end{aligned}\n With this choice of \\phi(h), we can verify that the equality does hold.\nThis completes the proof.\n\n\n\n1.2 Concentration inequality\n\nLemma 2 (Shalev-Shwartz and Ben-David 2014 - Exercise 31.1) Let X be a random variable that satisfies: \\mathrm{Pr} (X \\ge \\epsilon) \\le e^{-2m \\epsilon^{2}}. Prove that \n\\mathbb{E} \\left[ e^{2(m - 1) X^{2}} \\right] \\le m.\n\n\n\nProof. Since the assumption is expressed in term of probability, while the conclusion is written in form of an expectation, what we need to do first is to try to present the expectation in terms of probability.\nFor simplicity, let Y = e^{2(m - 1) X^{2}}. Since X \\in [0, +\\infty), then Y \\in [1, +\\infty) and Y can be presented as: \nY = \\int_{1}^{+\\infty} \\pmb{1}(Y \\ge t) \\, \\mathrm{d}t + 1,\n where \\pmb{1}(A) is the indication function of event A. Note that the integral above is the area of a rectangle with height as 1 and the width Y - 1.\nOne important property of the indication function is that: \n\\mathbb{E} \\left[ \\pmb{1}(Y \\ge t) \\right] = \\mathrm{Pr}(Y \\ge t).\n This allows to express the expectation of interest as: \n\\begin{aligned}\n\\mathbb{E}[Y] & = \\mathbb{E} \\left[ \\int_{1}^{+\\infty} \\pmb{1}(Y \\ge t) \\, \\mathrm{d}t \\right] + 1 \\\\\n& = \\int_{1}^{+\\infty} \\mathbb{E} [\\pmb{1}(Y \\ge t)] \\, \\mathrm{d}t + 1 \\quad \\text{(Fubini's theorem)} \\\\\n& = \\int_{1}^{+\\infty} \\mathrm{Pr}(Y \\ge t) \\, \\mathrm{d}t + 1.\n\\end{aligned}\n Or: \n\\mathbb{E} \\left[ e^{2(m - 1) X^{2}} \\right] = \\int_{1}^{+\\infty} \\mathrm{Pr}( e^{2(m - 1) X^{2}} \\ge x) \\, \\mathrm{d}x + 1.\n\nWe then make a change of variable from x to \\epsilon to utilize the given inequality in the assumption. Let’s define: \nx = e^{2(m - 1) \\epsilon^{2}}.\n Since \\epsilon is assumed to be non-negative, we can express it as: \n\\epsilon = \\sqrt{\\frac{\\ln x}{2(m - 1)}},\n and: \n\\mathrm{d}x = 4(m - 1) \\epsilon \\, e^{2(m - 1) \\epsilon^{2}} \\, \\mathrm{d} \\epsilon.\n\nThe expectation of interest can, therefore, be written as: \n\\begin{aligned}\n    \\mathbb{E} \\left[ e^{2(m - 1) X^{2}} \\right] & = \\int_{0}^{+\\infty} \\mathrm{Pr} \\left( e^{2(m - 1) X^{2}} \\ge e^{2(m - 1) \\epsilon^{2}} \\right) 4(m - 1) \\epsilon \\, e^{2(m - 1) \\epsilon^{2}} \\, \\mathrm{d} \\epsilon  + 1\\\\\n    & = \\int_{0}^{+\\infty} \\mathrm{Pr} \\underbrace{\\left( X \\ge \\epsilon \\right)}_{\\le e^{-2m\\epsilon^{2}}} 4(m - 1) \\epsilon \\, e^{2(m - 1) \\epsilon^{2}} \\, \\mathrm{d} \\epsilon + 1\\\\\n    & \\le 4(m - 1) \\int_{0}^{+\\infty} \\epsilon \\, e^{-2 \\epsilon^{2}} \\, \\mathrm{d} \\epsilon + 1 = m.\n\\end{aligned}"
  },
  {
    "objectID": "posts/PAC-Bayes-bounds/index.html#pac-bayes-bound",
    "href": "posts/PAC-Bayes-bounds/index.html#pac-bayes-bound",
    "title": "PAC-Bayes bounds for generalisation error",
    "section": "2 PAC-Bayes bound",
    "text": "2 PAC-Bayes bound\n\nTheorem 1 Let D be an arbitrary distribution over an example domain Z. Let \\mathcal{H} be a hypothesis class, \\ell: \\mathcal{H} \\times Z \\to [0, 1] be a loss function, \\pi be a prior distribution over \\mathcal{H}, and \\delta \\in (0, 1]. If S = \\{z_j\\}_{j=1}^{m} is an i.i.d. training set sampled according to D, then for any “posterior” Q over \\mathcal{H}, the following holds: \n\\mathrm{Pr} \\left( \\mathbb{E}_{z_{j} \\sim D} \\mathbb{E}_{h \\sim Q} \\left[ \\ell(h, z_{j}) \\right] \\le \\mathbb{E}_{z_{j} \\sim S} \\mathbb{E}_{h \\sim Q} \\left[ \\ell(h, z_{j}) \\right] + \\sqrt{\\frac{\\mathrm{KL} [Q \\Vert \\pi] + \\frac{\\ln m}{\\delta}}{2(m - 1)}} \\right) \\ge 1 - \\delta.\n\n\n\nProof. We define some notations to ease the proving: - L = \\mathbb{E}_{z_{j} \\sim D} \\left[ \\ell(h, z_{j}) \\right] - \\hat{L} = \\mathbb{E}_{z_{j} \\sim S} \\left[ \\ell(h, z_{j}) \\right] = \\frac{1}{m} \\sum_{j=1}^{m} \\ell(h, z_{j}) - \\Delta L = L - \\hat{L}\nApplying Lemma 1 with P(h) = \\pi (h) and \\phi(h) = 2(m - 1) (\\Delta L)^{2} gives: \n2(m - 1) \\mathbb{E}_{Q} \\left[ (\\Delta L)^{2} \\right] - \\mathrm{KL} [Q \\Vert \\pi] \\le \\textcolor{purple}{\\ln \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right]}.\n\\tag{1}\nWe upper-bound the last term in the RHS (highlighted in purple color) by Lemma 2. To do that, we consider the empirical loss on each observable data point l(h, z_{j}) as a random variable in [0, 1] with true and empirical means L and \\hat{L}, respectively. Following the Hoeffding’s inequality gives: \n\\begin{aligned}\n\\mathrm{Pr} \\left( \\Delta L \\ge \\epsilon \\right) & = \\mathrm{Pr} \\left( L - \\hat{L} \\ge \\epsilon \\right)\\\\\n& \\le \\mathrm{Pr} \\left( | L - \\hat{L} | \\ge \\epsilon \\right)\\\\\n& \\le \\exp(-2m \\epsilon^{2}), \\quad \\epsilon \\ge 0.\n\\end{aligned}\n According to Lemma 2, this implies: \n\\mathbb{E}_{S} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right] \\le m.\n Taking the expectation w.r.t. h \\sim \\pi(h) on both sides and applying Fubini’s theorem (to interchange the 2 expectations) gives: \n\\begin{aligned}\n& \\mathbb{E}_{S} \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right] \\le \\mathbb{E}_{\\pi} \\left[ m \\right] = m\\\\\n& \\implies \\ln \\mathbb{E}_{S} \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right] \\le \\ln m\\\\\n& \\implies \\mathbb{E}_{S} \\textcolor{purple}{\\ln \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right]} \\le \\ln m.\n\\end{aligned}\n Note that the last implication is due to Jensen’s inequality.\nWe then apply Markov’s inequality for the term highlighted in purple: \n\\begin{aligned}\n\\mathrm{Pr} \\left( \\textcolor{purple}{\\ln \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right]} \\ge \\varepsilon \\right) & \\le \\frac{\\mathbb{E}_{S} \\textcolor{purple}{\\ln \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right]}}{\\varepsilon} \\\\\n& \\le \\frac{\\ln m}{\\varepsilon}.\n\\end{aligned}\n\nThis implies: \n\\mathrm{Pr} \\left( \\textcolor{purple}{\\ln \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right]} \\le \\varepsilon \\right) \\ge 1 - \\frac{\\ln m}{\\varepsilon}.\n\\tag{2}\nCombining the results in Equation 1 and Equation 2 gives: \n\\mathrm{Pr} \\left( 2(m - 1) \\mathbb{E}_{Q} \\left[ (\\Delta L)^{2} \\right] - \\mathrm{KL} [Q \\Vert \\pi] \\le \\varepsilon \\right) \\ge 1 - \\frac{\\ln m}{\\varepsilon}.\n\nThis is equivalent to: \n\\mathrm{Pr} \\left( \\mathbb{E}_{Q} \\left[ (\\Delta L)^{2} \\right] \\le \\frac{\\mathrm{KL} [Q \\Vert \\pi] + \\varepsilon}{2(m - 1)} \\right) \\ge 1 - \\frac{\\ln m}{\\varepsilon}.\n\\tag{3}\nNote that squared function is a strictly concave function, resulting in: \n\\mathbb{E}_{Q} \\left[ (\\Delta L)^{2} \\right] \\ge \\left( \\mathbb{E}_{Q} \\left[ \\Delta L \\right] \\right)^{2}.\n\nHence, Equation 3 can be written as: \n\\mathrm{Pr} \\left( \\mathbb{E}_{Q} \\left[ \\Delta L \\right] \\le \\sqrt{\\frac{\\mathrm{KL} [Q \\Vert \\pi] + \\varepsilon}{2(m - 1)}} \\right) \\ge 1 - \\frac{\\ln m}{\\varepsilon}.\n\nSeting \\delta = \\frac{\\ln m}{\\varepsilon}, and expanding \\Delta L according to its definition complete the proof."
  },
  {
    "objectID": "posts/PAC-Bayes-bounds/index.html#discussion",
    "href": "posts/PAC-Bayes-bounds/index.html#discussion",
    "title": "PAC-Bayes bounds for generalisation error",
    "section": "3 Discussion",
    "text": "3 Discussion\nAFAIK, the result in Theorem 1 is a seminal PAC-Bayes bound in the literature of PAC learning. Readers could refer subsequent derivations of tighter PAC-Bayes bounds developed later."
  },
  {
    "objectID": "posts/PAC-Bayes-bounds/index.html#references",
    "href": "posts/PAC-Bayes-bounds/index.html#references",
    "title": "PAC-Bayes bounds for generalisation error",
    "section": "4 References",
    "text": "4 References\n\n\nBanerjee, Arindam. 2006. “On Bayesian Bounds.” In International Conference on Machine Learning, 81–88.\n\n\nMcAllester, David A. 1999. “PAC-Bayesian Model Averaging.” In Conference on Computational Learning Theory, 164–70.\n\n\nShalev-Shwartz, Shai, and Shai Ben-David. 2014. Understanding Machine Learning: From Theory to Algorithms. Cambridge university press."
  },
  {
    "objectID": "posts/meta-learning/index.html",
    "href": "posts/meta-learning/index.html",
    "title": "From hyper-parameter optimisation to meta-learning",
    "section": "",
    "text": "Meta-learning, also known as learn-how-to-learning, has been being studied from 1980s (Schmidhuber 1987; Naik and Mammone 1992), and recently attracted much attention from the research community. Meta-learning is a technique in transfer learning — a learning paradigm that utilises knowledge gained from past experience to facilitate the learning in the future. Due to being defined implicitly, meta -learning is often confused with other transfer learning techniques, e.g. fine-tuning, multi-task learning, domain adaptation and continual learning. The purpose of this post is to formulate meta-learning explicitly via empirical Bayes, and in particular hyper-parameter optimisation, to differentiate meta-learning from those common transfer learning approaches.\nThis post is structured as follows: First, we define some terminologies used in general transfer learning and review hyper-parameter optimisation in single-task setting. We then formulate meta-learning as an extension of hyper-parameter optimisation in multi-task setting. Finally, we show the differences between meta-learning and other transfer-learning approaches."
  },
  {
    "objectID": "posts/meta-learning/index.html#background",
    "href": "posts/meta-learning/index.html#background",
    "title": "From hyper-parameter optimisation to meta-learning",
    "section": "1 Background",
    "text": "1 Background\n\n1.1 Data generation model of a task\nA data point of a task indexed by i \\in \\mathbb{N} consists of an input \\mathbf{x}_{ij} \\in \\mathcal{X} \\subseteq \\mathbb{R}^{d} and a corresponding label \\mathbf{y}_{ij} \\in \\mathcal{Y} with j \\in \\mathbb{N}. For simplicity, only two families of tasks – regression and classification – are considered in this thesis. As a result, the label is defined as \\mathcal{Y} \\subseteq \\mathbb{R} for regression and as \\mathcal{Y} = \\{0, 1, \\ldots, C - 1\\} for classification, where C is the number of classes.\nEach data point in a task can be generated in 2 steps:\n\ngenerate the input \\mathbf{x}_{ij} by sampling from some probability distribution \\mathcal{D}_{i},\ndetermine the label \\mathbf{y}_{ij} = f(\\mathbf{x}_{ij}), where f_{i}: \\mathcal{X} \\to \\mathcal{Y} is the correct labelling function.\n\nBoth the probability distribution \\mathcal{D}_{i} and the labelling function f_{i} are unknown to the learning agent during training, and the aim of the supervised learning is to use the generated data to infer such labelling function f.\nFor simplicity, we denote (\\mathbf{x}_{ij}, \\mathbf{y}_{ij}) \\sim (\\mathcal{D}_{i}, f_{i}) as the data generation model of task i-th.\n\n\n1.2 Task instance\n\nDefinition 1 (Hospedales et al. 2021)\nA task or a task instance \\mathcal{T}_{i} consists of an unknown associated data generation model (\\mathcal{D}_{i}, f_{i}), and a loss function \\ell_{i}, denoted as: \n\\mathcal{T}_{i} = \\{(\\mathcal{D}_{i}, f_{i}), \\ell_{i}\\}.\n\n\n\nRemark. The loss function \\ell_{i} is defined abstractly, and can be either:\n\nnegative log-likelihood (NLL): - \\ln p(y_{ij} | \\mathbf{x}_{ij}, \\mathbf{w}_{i}), corresponding to maximum likelihood estimation. This type of loss is quite common in practice, for example:\n\nmean squared error (MSE) in regression\ncross-entropy in classification\n\nvariational-free energy (negative evidence lower-bound) — corresponding to the objective function in variational inference.\n\n\nTo solve a task \\mathcal{T}_{i}, one needs to obtain an optimal task-specific model {h(.; \\mathbf{w}_{i}^{*}): \\mathcal{X} \\to \\mathcal{Y}}, parameterised by \\mathbf{w}^{*}_{i} \\in \\mathcal{W} \\subseteq \\mathbb{R}^{n}, which minimises a loss function \\ell_{i} on the data of that task: \n\\mathbf{w}_{i}^{*} = \\arg\\min_{\\mathbf{w}_{i}} \\mathbb{E}_{(\\mathbf{x}_{ij}, \\mathbf{y}_{ij}) \\sim (\\mathcal{D}_{i}, f_{i})} \\left[ \\ell_{i} (\\mathbf{x}_{ij}, \\mathbf{y}_{ij}; \\mathbf{w}_{i}) \\right].\n\nIn practice, since both \\mathcal{D}_{i} and f_{i} are unknown, the data generation model is replaced by a dataset consisting of a finite number of data-points generated according to the data generation model (\\mathcal{D}_{i}, f_{i}), denoted as S_{i} = \\{\\mathbf{x}_{ij}, \\mathbf{y}_{ij}\\}_{j=1}^{m_{i}}. The objective to solve that task is often known as empirical risk minimisation: \n\\mathbf{w}^{\\mathrm{ERM}}_{i} = \\arg\\min_{\\mathbf{w}_{i}} \\frac{1}{m_{i}} \\sum_{j = 1}^{m_{i}} \\left[ \\ell_{i} (\\mathbf{x}_{ij}, \\mathbf{y}_{ij}; \\mathbf{w}_{i}) \\right].\n\\tag{1}\nSince the loss function used is the same for each task family, e.g. \\ell is NLL or variational-free energy, the subscript on the loss function is, therefore, dropped, and the loss is denoted as \\ell throughout this chapter. Furthermore, given the commonality of the loss function across all tasks, a task can, therefore, be simply represented by either its data generation model (\\mathcal{D}_{i}, f_{i}) or the associated dataset S_{i}.\n\n\n1.3 Hyper-parameter optimisation\nIn single-task setting, the common way to tune or optimise a hyper-parameter is to split a given dataset S_{i} into two disjoint subsets: \n\\begin{aligned}\nS_{i}^{(t)} \\cup S_{i}^{(v)} & = S_{i}\\\\\nS_{i}^{(t)} \\cap S_{i}^{(v)} & = \\varnothing,\n\\end{aligned}\n where:\n\nS_{i}^{(t)} = \\left\\{ \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)} \\right) \\right\\}_{j=1}^{m_{i}^{(t)}} is the training (or support) subset,\nS_{i}^{(v)} = \\left\\{ \\left( \\mathbf{x}_{ij}^{(v)}, y_{ij}^{(v)} \\right) \\right\\}_{j=1}^{m_{i}^{(v)}} is the validation (or query) subset.\n\nNote that with this definition, m_{i}^{(t)} + m_{i}^{(v)} = m_{i}, and m_{i}^{(t)} and m_{i}^{(v)} are not necessarily identical.\nThe subset S_{i}^{(t)} is used to train the model parameter of interest \\mathbf{w}_{i}, while the subset S_{i}^{(v)} is used to validate the hyper-parameter, denoted by \\theta (we provide examples of the hyper-parameter in Section Formulation of meta-learning). Mathematically, hyper-parameter optimisation in the single-task setting can be written as the following bi-level optimisation: \n\\begin{aligned}\n& \\min_{\\theta} \\frac{1}{m_{i}^{(v)}} \\sum_{k = 1}^{m_{i}^{(v)}}  \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*} (\\theta) \\right)\\\\\n& \\text{s.t.: } \\mathbf{w}_{i}^{*} (\\theta) = \\arg\\min_{\\mathbf{w}_{i}} \\frac{1}{m_{i}^{(t)}} \\sum_{j = 1}^{m_{i}^{(t)}}  \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\mathbf{w}_{i} (\\theta) \\right).\n\\end{aligned}\n\nWe can extend the hyper-parameter optimisation from the two data subsets S_{i}^{(t)} and S_{i}^{(v)} to the general data generation model as the following: \n\\begin{aligned}\n& \\min_{\\theta} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[  \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*} (\\theta) \\right) \\right]\\\\\n& \\text{s.t.: } \\mathbf{w}_{i}^{*} (\\theta) = \\arg\\min_{\\mathbf{w}_{i}} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(t)}, y_{ik}^{(t)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(t)}, f_{i} \\right)} \\left[  \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\mathbf{w}_{i} (\\theta) \\right) \\right],\n\\end{aligned}\n where \\mathcal{D}_{i}^{(t)} and \\mathcal{D}_{i}^{(v)} are the probability distributions of training and validation input data, respectively, and they are not necessarily identical.\nFormulation of meta-learning\nThe setting of the meta-learning problem considered in this paper follows the task environment (Baxter 2000) that describes the unknown distribution p(\\mathcal{D}, f) over a family of tasks. Each task \\mathcal{T}_{i} is sampled from this task environment and can be represented as \\left( \\mathcal{D}_{i}^{(t)}, \\mathcal{D}_{i}^{(v)}, f_{i} \\right), where \\mathcal{D}_{i}^{(t)} and \\mathcal{D}_{i}^{(v)} are the probability of training and validation input data, respectively, and are not necessarily identical. The aim of meta-learning is to use T training tasks to train a meta-learning model that can be fine-tuned to perform well on an unseen task sampled from the same task environment.\nSuch meta-learning methods use meta-parameters to model the common latent structure of the task distribution p(\\mathcal{D}, f). In this thesis, we consider meta-learning as an extension of hyper-parameter optimisation in single-task learning, where the hyper-parameter of interest — often called meta-parameter — is shared across many tasks. Similar to hyper-parameter optimisation presented in subsection hyper-parameter-optimisation, the objective of meta-learning is also a bi-level optimisation: \n\\begin{aligned}\n& \\min_{\\theta}  \\textcolor{crimson}{\\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f_{i} \\right)}} \\mathbb{E}_{ \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right]\\\\\n& \\text{s.t.: } \\mathbf{w}^{*}_{i}(\\theta) = \\arg\\min_{\\mathbf{w}_{i}} \\mathbb{E}_{\\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(t)}, f_{i} \\right)} \\left[ \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\mathbf{w}_{i}(\\theta) \\right) \\right].\n\\end{aligned}\n\\tag{2}\nThe difference between meta-learning and hyper-parameter optimisation is that the meta-parameter (also known as hyper-parameter) \\theta is shared across all tasks sampled from the task environment p(\\mathcal{D}, f) as highlighted in red colour in Equation 2.\nIn practice, the meta-parameter (or shared hyper-parameter) \\theta can be chosen as one of the followings:\n\nlearning rate of gradient-based optimisation used to minimise the lower level objective function in Equation 2 to learn \\mathbf{w}_{i}^{*} \\left(\\theta\\right) (Z. Li et al. 2017),\ninitialisation of model parameter (Finn, Abbeel, and Levine 2017),\ndata representation or feature extractor (Vinyals et al. 2016; Snell, Swersky, and Zemel 2017),\noptimiser used to optimise the lower-level in Equation 2.\n\nIn this post, the meta-parameter \\theta is assumed to be the initialisation of model parameters. Formulation, derivation and analysis in the subsequent sections and chapters will, therefore, revolve around this assumption. Note that the analysis can be straight-forwardly extended to other types of meta-parameters with slight modifications.\nIn general, the objective function of meta-learning in Equation 2 can be solved by gradient-based optimisation, such as gradient descent. Due to the nature of the bi-level optimisation, the optimisation are often carried out in two steps. The first step is to adapt (or fine-tuned) the meta-parameter \\theta to the task-specific parameter \\mathbf{w}_{i}(\\theta). This corresponds to the optimisation in the lower-level, and can be written as: \n\\mathbf{w}_{i}^{*}(\\theta) = \\theta - \\alpha \\mathbb{E}_{\\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(t)}, f_{i} \\right)} \\left[ \\nabla_{\\theta} \\ell \\left( \\mathbf{x}_{ij}^{(t)}, \\mathbf{y}_{ij}^{(t)}; \\mathbf{w}_{i}(\\theta) \\right) \\right],\n\\tag{3} where \\alpha is a hyper-parameter denoting the learning rate for task \\mathcal{T}_{i}. For simplicity, the adaptation step in Equation 3} is carried out with only one gradient descent update.\nThe second step is to minimise the validation loss induced by the locally-optimal task-specific parameter \\mathbf{w}_{i}^{*}(\\theta) evaluated on the validation subset w.r.t. the meta-parameter \\theta. This corresponds to the upper-level optimisation, and can be expressed as: \n\\theta \\gets \\theta - \\gamma \\mathbb{E}_{\\mathcal{T}_{i} \\sim p(\\mathcal{D}, f)} \\mathbb{E}_{ \\left( \\mathbf{x}_{ik}^{(v)}, \\mathbf{y}_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\nabla_{\\theta} \\ell \\left( \\mathbf{x}_{ij}^{(v)}, \\mathbf{y}_{ij}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right],\n\\tag{4} where \\gamma is another hyper-parameter representing the learning rate to learn \\theta.\nThe general algorithm of meta-learning using gradient-based optimisation is shown in Algorithm 1.\n\n\n\\begin{algorithm} \\caption{Training procedure of meta-learning in general} \\begin{algorithmic} \\Procedure{Training}{task environment $p(\\mathcal{D}, f)$, learning rates $\\gamma$ and $\\alpha$} \\State initialise meta-parameter $\\theta$ \\While{$\\theta$ not converged} \\State sample a mini-batch of $T$ tasks from task environment $p\\left( \\mathcal{D}, f \\right)$ \\For{each task $\\mathcal{T}_{i}, i \\in \\{1, \\ldots, T\\}$} \\State sample two data subsets $S_{i}^{(t)}$ and $S_{i}^{(v)}$ from task $\\mathcal{T}_{i} = (\\mathcal{D}_{i}^{(t)}, \\mathcal{D}_{i}^{(v)}, f_{i})$ \\State adapt meta-parameter to task $\\mathcal{T}_{i}$: $\\mathbf{w}_{i}^{*} \\left( \\theta \\right) = \\theta - \\frac{\\alpha}{m_{i}^{(t)}} \\sum_{j = 1}^{m_{i}^{(t)}} \\nabla_{\\theta} \\left[ \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\theta \\right)\\right]$ \\EndFor \\State update meta-parameter: $\\theta \\gets \\theta - \\frac{\\gamma}{T} \\sum_{i=1}^{T} \\frac{1}{m_{i}^{(v)}} \\sum_{k=1}^{m_{i}^{(v)}} \\nabla_{\\theta} \\left[\\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*} \\left( \\theta \\right) \\right) \\right]$ \\EndWhile \\State \\textbf{return} the trained meta-parameter $\\theta$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n1.4 Second-order meta-learning\nAs shown in Equation 4, the optimisation for the meta-parameter \\theta requires the gradient of the validation loss averaged across T tasks. Given that each task-specific parameter \\mathbf{w}_{i}^{*} is a function of \\theta due to the lower-level optimisation in Equation 3, the gradient of interest can be expanded as: \n\\begin{aligned}\n& \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\nabla_{\\theta} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right]\\\\\n& = \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\nabla_{\\theta}^{\\top} \\mathbf{w}_{i}^{*} \\left( \\theta \\right) \\times \\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right]\\\\\n& = \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\left\\{ \\left[ \\mathbf{I} - \\alpha \\mathbb{E}_{ \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(t)}, f_{i} \\right)} \\left[  \\textcolor{crimson}{\\nabla_{\\theta}^{2} \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\theta \\right)} \\right] \\right] \\right.\\\\\n& \\quad \\times \\left. \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\textcolor{green}{\\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right)} \\right] \\right\\},\n\\end{aligned}\n where the first equality is due to chain rule, and the second equality is the result that differentiates the gradient update in Equation 3. Note that in the second equality, we remove the transpose notation since the corresponding matrix is symmetric.\nThus, naively implementing such gradient would require to calculate the Hessian matrix $ $, resulting in an intractable procedure for large models, such as deep neural networks. To obtain a more efficient implementation, one can utilise the Hessian-vector product (Pearlmutter 1994) between the gradient vector \\textcolor{green}{\\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, \\mathbf{y}_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right)} and the Hessian matrix $ $ to efficiently calculate the gradient of the validation loss w.r.t. \\theta.\nAnother way to calculate the gradient of the validation loss w.r.t. the meta-parameter \\theta is to use implicit differentiation (Domke 2012; Rajeswaran et al. 2019; Lorraine, Vicol, and Duvenaud 2020). This approach is more advantaged since it does not need to stores the computational graph and takes gradient via chain rule. Such implicit differentiation technique reduces the memory usage and therefore, allows to work with large-scale models. However, the trade-off is the increasing computational time to apply the chain rule to calculate the gradient of interest.\nNevertheless, the implementations that compute the exact gradient of the validation loss w.r.t. \\theta without approximation are often referred to as second-order meta-learning.\n\n\n1.5 First-order meta-learning\nIn practice, the Hessian matrix $ $ is often omitted from the calculation to simplify the update for the meta-parameter \\theta (Finn, Abbeel, and Levine 2017). The resulting gradient consists of only the gradient of validation loss \\textcolor{Green}{\\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ij}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right)}, which is more efficient to calculate with a single forward-pass if auto differentiation is used. This approximation is often referred as first-order meta-learning, and the gradient of interest can be presented as: \n\\begin{aligned}\n& \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left(\\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\nabla_{\\theta} \\ell \\left( \\mathbf{x}_{ij}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right] \\\\\n& \\approx \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\textcolor{Green}{\\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ij}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right)} \\right].\n\\end{aligned}\n\nREPTILE [Nichol, Achiam, and Schulman (2018)} — a variant first-order meta-learning — approximates further the gradient of validation loss \\textcolor{Green}{\\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ij}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right)} by the difference \\theta - \\mathbf{w}_{i}^{*}, resulting in a much simpler approximation: \n\\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, \\mathbf{y}_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\nabla_{\\theta} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right] = \\theta - \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\left[ \\mathbf{w}_{i}^{*}(\\theta) \\right]."
  },
  {
    "objectID": "posts/meta-learning/index.html#differentiation-from-other-transfer-learning-approaches",
    "href": "posts/meta-learning/index.html#differentiation-from-other-transfer-learning-approaches",
    "title": "From hyper-parameter optimisation to meta-learning",
    "section": "2 Differentiation from other transfer learning approaches",
    "text": "2 Differentiation from other transfer learning approaches\nIn this section, some popular transfer learning methods are described with their objective functions to purposely distinguish from meta-learning.\n\n2.1 Fine-tuning\nFine-tuning is the most common technique in neural network based transfer learning (Pratt et al. 1991; Yosinski et al. 2014) where the last or a couple of last layers in a neural network pre-trained on a source task are replaced and fine-tuned on a target task. Formally, if g(.; \\mathbf{w}_{0}) is denoted as the forward function of the shared layers with shared parameters \\mathbf{w}_{0}, where \\mathbf{w}_{s} and \\mathbf{w}_{t} are the parameters of the remaining layers h specifically trained on source and target tasks, respectively, then the objective of fine-tuning can be expressed as: \n\\begin{aligned}\n& \\min_{\\mathbf{w}_{t}} \\mathbb{E}_{(\\mathbf{x}_{t}, \\mathbf{y}_{t}) \\sim \\mathcal{T}_{t}} \\left[ \\ell \\left( h\\left( g\\left( \\mathbf{x}_{t}; \\mathbf{w}_{0}^{*} \\right); \\mathbf{w}_{t} \\right), \\mathbf{y}_{t} \\right) \\right] \\\\\n& \\text{s.t.: } \\mathbf{w}_{0}^{*}, \\mathbf{w}_{s}^{*} = \\arg\\min_{\\mathbf{w}_{0}, \\mathbf{w}_{s}} \\mathbb{E}_{(\\mathbf{x}_{s}, \\mathbf{y}_{s}) \\sim \\mathcal{T}_{s}} \\left[ \\ell \\left( h \\left( g\\left( \\mathbf{x}_{s}; \\mathbf{w}_{0} \\right); \\mathbf{w}_{s} \\right), \\mathbf{y}_{s} \\right) \\right],\n\\end{aligned}\n\\tag{5}\nwhere \\mathbf{x}_{s}, \\mathbf{y}_{s} and \\mathbf{x}_{t}, \\mathbf{y}_{t} are the data sampled from the source task \\mathcal{T}_{s} and target task \\mathcal{T}_{t}, respectively.\nAlthough the objective of fine-tuning shown in Equation 5 is still a bi-level optimisation, it is easier to solve than the one in meta-learning due to the following reasons:\n\nThe objective in fine-tuning has only one constrain corresponding to one source task, while meta-learning has several constrains corresponding to multiple training tasks.\nIn fine-tuning, \\mathbf{w}_{t} and \\mathbf{w}_{0} are inferred separately, while in meta-learning, the task-specific parameter is a function of the meta-parameter, resulting in a more complicated correlation.\n\nThe downside of fine-tuning is the requirement of a reasonable number of training examples on the target task to fine-tune \\mathbf{w}_{t}. In contrast, meta-learning leverages the knowledge extracted from several training tasks to quickly adapt to a new task with only a few training examples.\n\n\n2.2 Domain adaptation and generalisation\nDomain adaptation or domain-shift refers to the case when the joint data-label distribution on source and target are different, denoted as p_{s} \\left( \\mathcal{D}, f \\right) \\neq p_{t} \\left( \\mathcal{D}, f \\right), or simply p_{s}(\\mathbf{x}, \\mathbf{y}) \\neq p_{t}(\\mathbf{x}, \\mathbf{y}) (Heckman 1979; Shimodaira 2000; Japkowicz and Stephen 2002; Daume III and Marcu 2006; Ben-David et al. 2007). The aim of domain adaptation is to leverage the model trained on source domain to available data in the target domain, so that the model adapted to the target domain can perform reasonably well. In other words, domain adaptation relies on a data transformation g(., .; \\mathbf{w}_{0}): \\mathcal{X} \\times \\mathcal{Y} \\to \\mathcal{X}^{\\prime} \\times \\mathcal{Y}^{\\prime} that produces a domain-invariant latent space. Mathematically, the transformation g is obtained by minimising a divergence between the two transformed data distribution: \n\\begin{aligned}\n& \\min_{\\mathbf{w}_{0}} \\mathrm{Divergence} \\left[ p\\left( \\mathbf{x}_{s}^{\\prime}, \\mathbf{y}_{s}^{\\prime} \\right) || p\\left( \\mathbf{x}_{t}^{\\prime}, \\mathbf{y}_{t}^{\\prime} \\right) \\right]\\\\\n& \\text{s.t.: } \\left( \\mathbf{x}_{i}^{\\prime}, \\mathbf{y}_{i}^{\\prime} \\right) = g \\left( \\mathbf{x}_{i}, \\mathbf{y}_{i}; \\mathbf{w}_{0} \\right), i \\in \\{s, t\\}.\n\\end{aligned}\n\\tag{6}\nAfter obtaining the transformation g, one can simply train a model using the transformed data of the source domain, and then use that model to make predictions on the target domain.\nGiven the optimisation in Equation 6, domain adaptation is different from meta-learning due to the following reasons:\n\nDomain adaptation assumes a shift in the task environments that generate source and target tasks, while meta-learning is based on the assumption of same task generation.\nDomain adaptation utilises information of data from target domain, while meta-learning does not have such access.\n\nIn general, meta-learning learns a shared prior or hyper-parameters to generalise for unseen tasks, while domain adaptation produces a model to solve a particular task in a specified target domain. Recently, there is a variance of domain adaptation, named domain generalisation, where the aim is to learn a domain-invariant model without any information of target domain. In this view, domain generalisation is very similar to meta-learning, and there are some works that employ meta-learning algorithms for domain generalisation (D. Li et al. 2018; Y. Li et al. 2019).\n\n\n2.3 Multi-task learning\nMulti-task learning learns several related auxiliary tasks and a target task simultaneously to exploit the diversity of task representation to regularise and improve the performance on the target task (Caruana 1997). If the input \\mathbf{x} is assumed to be the same across T extra tasks and the target task \\mathcal{T}_{T + 1}, then the objective of multi-task learning can be expressed as: \n\\min_{\\mathbf{w}_{0}, \\{\\mathbf{w}_{i}\\}_{i = 1}^{T + 1}} \\frac{1}{T + 1} \\sum_{i = 1}^{T + 1} \\ell_{i} \\left( h_{i} \\left( g\\left( \\mathbf{x}; \\mathbf{w}_{0} \\right); \\mathbf{w}_{i} \\right), \\mathbf{y}_{i} \\right),\n\\tag{7} where \\mathbf{y}_{i}, \\ell_{i} and h_{i} are the label, loss function and the classifier for task \\mathcal{T}_{i}, respectively, and g(., \\mathbf{w}_{0}) is the shared feature extractor for T + 1 tasks.\nMulti-task learning is often confused with meta-learning due to their similar nature extracting information from many tasks. However, the objective function of multi-task learning in Equation 7 is a single-level optimisation for the shared parameter \\mathbf{w}_{0} and multiple task-specific classifier \\{\\mathbf{w}_{i}\\}_{i = 1}^{T + 1}. It is, therefore, not as complicated as a bi-level optimisation seen in meta-learning as shown in Equation 2. Furthermore, multi-task learning aims to solve a number of specific tasks known during training (referred to as target tasks), while meta-learning targets the generalisation for unseen tasks in the future.\n\n\n2.4 Continual learning\nContinual or life-long learning refers to a situation where a learning agent has access to a continuous stream of tasks available over time, and the number of tasks to be learnt is not pre-defined (Chen and Liu 2018; Parisi et al. 2019). The aim is to accommodate the knowledge extracted from one-time observed tasks to accelerate the learning of new tasks without catastrophically forgetting old tasks (French 1999). In this sense, continual learning is very similar to meta-learning. However, continual learning most likely focuses on systematic design to acquire new knowledge in such a way that prevents interfering to the existing one, while meta-learning is more about algorithmic design to learn the new knowledge more efficiently. Thus, we cannot mathematically distinguish their differences as done in sub-sections Fine-tuning, Domain adaptation and generalisation and Multi-task learning . Nevertheless, continual learning criteria, especially catastrophic forgetting, can be encoded into meta-learning objective to advance further continual learning performance (Al-Shedivat et al. 2018; Nagabandi et al. 2019)."
  },
  {
    "objectID": "posts/meta-learning/index.html#summary",
    "href": "posts/meta-learning/index.html#summary",
    "title": "From hyper-parameter optimisation to meta-learning",
    "section": "3 Summary",
    "text": "3 Summary\nIn general, meta-learning is an extension of hyper-parameter optimisation in multi-task setting. The objective function of meta-learning is, therefore, a bi-level optimisation, where the lower-level is to adapt the meta-parameter to a task, while the upper-level is to evaluate how well the meta-parameter performs across T tasks. Given such mathematical formulation, we can easily distinguish meta-learning from some common transfer learning approaches, such as fine-tuning, multi-task learning, domain adaptation and continual learning.\nHope that this post would give another perspective of meta-learning. I’ll see you in the next post about probabilistic methods in meta-learning."
  },
  {
    "objectID": "posts/meta-learning/index.html#references",
    "href": "posts/meta-learning/index.html#references",
    "title": "From hyper-parameter optimisation to meta-learning",
    "section": "4 References",
    "text": "4 References\n\n\nAl-Shedivat, Maruan, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. 2018. “Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments.” In International Conference on Learning Representation.\n\n\nBaxter, Jonathan. 2000. “A Model of Inductive Bias Learning.” Journal of Artificial Intelligence Research 12: 149–98.\n\n\nBen-David, Shai, John Blitzer, Koby Crammer, Fernando Pereira, et al. 2007. “Analysis of Representations for Domain Adaptation.” Advances in Neural Information Processing Systems 19: 137.\n\n\nCaruana, Rich. 1997. “Multitask Learning.” Machine Learning 28 (1): 41–75.\n\n\nChen, Zhiyuan, and Bing Liu. 2018. “Lifelong Machine Learning.” Synthesis Lectures on Artificial Intelligence and Machine Learning 12 (3): 1–207.\n\n\nDaume III, Hal, and Daniel Marcu. 2006. “Domain Adaptation for Statistical Classifiers.” Journal of Artificial Intelligence Research 26: 101–26.\n\n\nDomke, Justin. 2012. “Generic Methods for Optimization-Based Modeling.” In Artificial Intelligence and Statistics, 318–26. PMLR.\n\n\nFinn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.” In International Conference on Machine Learning, 1126–35.\n\n\nFrench, Robert M. 1999. “Catastrophic Forgetting in Connectionist Networks.” Trends in Cognitive Sciences 3 (4): 128–35.\n\n\nHeckman, James J. 1979. “Sample Selection Bias as a Specification Error.” Econometrica: Journal of the Econometric Society, 153–61.\n\n\nHospedales, Timothy M, Antreas Antoniou, Paul Micaelli, and Amos J Storkey. 2021. “Meta-Learning in Neural Networks: A Survey.” IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n\nJapkowicz, Nathalie, and Shaju Stephen. 2002. “The Class Imbalance Problem: A Systematic Study.” Intelligent Data Analysis 6 (5): 429–49.\n\n\nLi, Da, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. 2018. “Learning to Generalize: Meta-Learning for Domain Generalization.” In Thirty-Second AAAI Conference on Artificial Intelligence.\n\n\nLi, Yiying, Yongxin Yang, Wei Zhou, and Timothy Hospedales. 2019. “Feature-Critic Networks for Heterogeneous Domain Generalization.” In International Conference on Machine Learning, 3915–24. PMLR.\n\n\nLi, Zhenguo, Fengwei Zhou, Fei Chen, and Hang Li. 2017. “Meta-Sgd: Learning to Learn Quickly for Few-Shot Learning.” arXiv Preprint arXiv:1707.09835.\n\n\nLorraine, Jonathan, Paul Vicol, and David Duvenaud. 2020. “Optimizing Millions of Hyperparameters by Implicit Differentiation.” In International Conference on International Conference on Artificial Intelligence and Statistics, 1540–52. PMLR.\n\n\nNagabandi, Anusha, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. 2019. “Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning.” In International Conference on Learning Representation.\n\n\nNaik, Devang K, and RJ Mammone. 1992. “Meta-Neural Networks That Learn by Learning.” In International Joint Conference on Neural Networks, 1:437–42. IEEE.\n\n\nNichol, Alex, Joshua Achiam, and John Schulman. 2018. “On First-Order Meta-Learning Algorithms.” CoRR abs/1803.02999. http://arxiv.org/abs/1803.02999.\n\n\nParisi, German I, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. 2019. “Continual Lifelong Learning with Neural Networks: A Review.” Neural Networks 113: 54–71.\n\n\nPearlmutter, Barak A. 1994. “Fast Exact Multiplication by the Hessian.” Neural Computation 6: 147–60.\n\n\nPratt, Lorien Y, Jack Mostow, Candace A Kamm, and Ace A Kamm. 1991. “Direct Transfer of Learned Information Among Neural Networks.” In Aaai, 91:584–89.\n\n\nRajeswaran, Aravind, Chelsea Finn, Sham Kakade, and Sergey Levine. 2019. “Meta-Learning with Implicit Gradients.”\n\n\nSchmidhuber, Jürgen. 1987. “Evolutionary Principles in Self-Referential Learning (on Learning How to Learn: The Meta-Meta-... Hook).” Diploma thesis, Technische Universität München.\n\n\nShimodaira, Hidetoshi. 2000. “Improving Predictive Inference Under Covariate Shift by Weighting the Log-Likelihood Function.” Journal of Statistical Planning and Inference 90 (2): 227–44.\n\n\nSnell, Jake, Kevin Swersky, and Richard Zemel. 2017. “Prototypical Networks for Few-Shot Learning.” In Advances in Neural Information Processing Systems, 4077–87.\n\n\nVinyals, Oriol, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. 2016. “Matching Networks for One Shot Learning.” In Advances in Neural Information Processing Systems, 29:3630–38.\n\n\nYosinski, Jason, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. “How Transferable Are Features in Deep Neural Networks?” In Advances in Neural Information Processing Systems."
  },
  {
    "objectID": "probabilita-ml.html",
    "href": "probabilita-ml.html",
    "title": "Welcome to Probabilita ML",
    "section": "",
    "text": "This website is dedicated to introducing the foundational motivations and key concepts of various probabilistic machine learning techniques. The aim is to facilitate a comprehensive understanding of how learning algorithms are derived and formulated through some level of mathematics, particularly probability theory, linear algebra, and multivariate calculus. While this content sounds boring and may not encompass several recently developed advanced techniques, these foundational principles are expected to remain central to the field of machine learning, despite the rapid advancements of the field."
  },
  {
    "objectID": "probabilita-ml.html#what-is-machine-learning",
    "href": "probabilita-ml.html#what-is-machine-learning",
    "title": "Welcome to Probabilita ML",
    "section": "What is machine learning?",
    "text": "What is machine learning?\nMachine learning has been trendy, especially after the Image-Net challenge 2012 with AlexNet improving the benchmark significantly compared to non-deep learning approaches. There are several definitions of machine learning that could be found on the internet. Here, machine learning is defined in a simple term:\n\n  \\begin{aligned}\n    \\text{machine learning} & = \\text{solving inverse problems}.\n  \\end{aligned}\n\nWhat is an inverse problem then? In simple terms, it is to work out unknown parameters from observations of a system of interest. For example:\n\nForward problem: given f(x) = x^{2} + 1, one can easily calculate y_{i} = f(x_{i}) for different values of x_{i}.\nInverse problem: given a set of observations \\{(x_{i}, y_{i})\\}_{i = 1}^{N}, how to find the function f that satisfies: y_{i} = f(x_{i}).\n\nAlthough the inverse problem may easily be solved for the example above, the difficulty to find f increases with the complexity of f. In general, the inverse problem is far more difficult than the forward problem."
  },
  {
    "objectID": "probabilita-ml.html#what-is-probabilistic-machine-learning",
    "href": "probabilita-ml.html#what-is-probabilistic-machine-learning",
    "title": "Welcome to Probabilita ML",
    "section": "What is probabilistic machine learning?",
    "text": "What is probabilistic machine learning?\nProbabilistic machine learning is to model the data generation process, for example, through some graphical models, then use the observed data to infer the posterior of the model’s parameter."
  },
  {
    "objectID": "probabilita-ml.html#why-is-the-name-probabilita",
    "href": "probabilita-ml.html#why-is-the-name-probabilita",
    "title": "Welcome to Probabilita ML",
    "section": "Why is the name probabilita?",
    "text": "Why is the name probabilita?\nThis is derived from the Latin word probabilitas, meaning probability."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Cuong Nguyen",
    "section": "",
    "text": "Courses taught at The University of Adelaide\n\n\n\n\n\n\n\n\n\n\n\nYear\nSemester\nCourse\nLevel\nRole\n\n\n\n\n2023\n2\nGrand Challenges in Computer Science\nUndergraduate\nIntructor\n\n\n2022\n2\nGrand Challenges in Computer Science\nUndergraduate\nIntructor\n\n\n2021\n1\nFoundations on Computer Science\nMaster\nTutor\n\n\n2021\n2\nFoundations on Computer Science\nMaster\nTutor\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Presentations",
    "section": "",
    "text": "Learning with limited and uncertain data\n\n\nFew-shot meta-learning & noisy label learning\n\n\n\n\n\n\n\n\nCuong Nguyen\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "presentations/pecman/index.html#outline",
    "href": "presentations/pecman/index.html#outline",
    "title": "Learning with limited and uncertain data",
    "section": "Outline",
    "text": "Outline\n\n\nPast: Few-shot learning via meta-learning\n\nVariational meta-learning (WACV 2020)\nPAC-Bayes meta-learning (T-PAMI 2022)\nTask modelling (UAI 2021)\nTask weighting (TMLR 2023)\n\nOn-going: Noisy label learning\n\nIdentifiability\n\nFuture"
  },
  {
    "objectID": "presentations/pecman/index.html#meta-learning---background",
    "href": "presentations/pecman/index.html#meta-learning---background",
    "title": "Learning with limited and uncertain data",
    "section": "Meta-learning - Background",
    "text": "Meta-learning - Background\nMeta-learning is to learn a hyper-parameter (e.g., learning rate or initialisation) shared across many tasks (or datasets).\n\n\n\n\n\n\n\n\n\n\n\norigin\n\n\n\nmeta\n\nθ\n\n\n\norigin-&gt;meta\n\n\n\n\n\ntask1\n\nTask 1\n\n\n\nmeta:n-&gt;task1\n\n\n\n\n\ntask2\n\nTask 2\n\n\n\nmeta:e-&gt;task2:w\n\n\n\n\n\ntask3\n\nTask 3\n\n\n\nmeta:s-&gt;task3"
  },
  {
    "objectID": "presentations/pecman/index.html#variational-meta-learning",
    "href": "presentations/pecman/index.html#variational-meta-learning",
    "title": "Learning with limited and uncertain data",
    "section": "Variational meta-learning",
    "text": "Variational meta-learning\nExtend the deterministic meta-learning (MAML) to a Bayesian meta-learning (VAMPIRE)\n\n\n\n\n\n\n\n\n\n\n(a) MAML\n\n\n\n\n\n\n\n\n\n\n(b) VAMPIRE\n\n\n\nFigure 1: Deterministic and Bayesian meta-learning methods."
  },
  {
    "objectID": "presentations/pecman/index.html#pac-bayesian-meta-learning",
    "href": "presentations/pecman/index.html#pac-bayesian-meta-learning",
    "title": "Learning with limited and uncertain data",
    "section": "PAC-Bayesian meta-learning",
    "text": "PAC-Bayesian meta-learning\nExisting formulation relies on fixed training tasks.\n\nIt is generalised to even unseen tasks through PAC-Bayes framework:\n\n\n\n\n\n\n\n\nTheorem: PAC-Bayesian meta-learning on unseen tasks\n\n\nGiven \\(T\\) tasks sampled from \\(p(\\mathcal{D}, f)\\), where each task has an associated pair of datasets \\((S_{i}^{(t)}, S_{i}^{(v)})\\) with samples generated from the task-specific data generation model \\((\\mathcal{D}_{i}^{(t)}, \\mathcal{D}_{i}^{(v)}, f_{i})\\), then for a bounded loss function \\(\\ell: \\mathcal{W} \\times \\mathcal{Y} \\to [0, 1]\\) and any distributions \\(q(\\theta; \\psi)\\) of meta-parameter \\(\\theta\\) and \\(q(\\mathbf{w}_{i}; \\lambda_{i})\\) of task-specific parameter \\(\\mathbf{w}_{i}\\): \\[\n    \\begin{aligned}\n        & \\mathrm{Pr} \\Big( \\mathbb{E}_{q(\\theta; \\psi)} \\mathbb{E}_{p(\\mathcal{D}, f)} \\mathbb{E}_{q(\\mathbf{w}_{i}; \\lambda_{i})} \\mathbb{E}_{(\\mathcal{D}_{i}^{(v)}, f_{i})} \\left[ \\ell \\left( \\mathbf{x}_{ij}^{(v)}, y_{ij}^{(v)}; \\mathbf{w}_{i} \\right) \\right] \\le \\frac{1}{T} \\sum_{i = 1}^{T} \\frac{1}{m_{i}^{(v)}} \\sum_{k = 1}^{m_{i}^{(v)}} \\mathbb{E}_{q(\\theta; \\psi)} \\mathbb{E}_{q(\\mathbf{w}_{i}; \\lambda)} \\left[ \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i} \\right) \\right] \\\\\n        & \\qquad + \\sqrt{ \\frac{ \\mathbb{E}_{q(\\theta; \\psi)} \\left[ \\mathrm{KL} \\left[ q(\\mathbf{w}_{i}; \\lambda_{i}) || p(\\mathbf{w}_{i}) \\right] \\right] + \\frac{T^{2}}{(T - 1) \\varepsilon}\\ln m_{i}^{(v)} }{2 \\left( m_{i}^{(v)} - 1 \\right)} } + \\sqrt{ \\frac{ \\mathrm{KL}  \\left[ q(\\theta; \\psi) || p(\\theta) \\right] + \\frac{T \\ln T}{\\varepsilon} }{2 (T - 1)} } \\Big) \\ge 1 - \\varepsilon,\n    \\end{aligned}\n\\] where: \\(\\varepsilon \\in (0, 1]\\), \\(p(\\mathbf{w}_{i}), \\forall i \\in \\{1, \\ldots, T\\}\\) is the prior of task-specific parameter \\(\\mathbf{w}_{i}\\) and \\(p(\\theta)\\) is the prior of meta-parameter \\(\\theta\\)."
  },
  {
    "objectID": "presentations/pecman/index.html#probabilistic-task-modelling",
    "href": "presentations/pecman/index.html#probabilistic-task-modelling",
    "title": "Learning with limited and uncertain data",
    "section": "Probabilistic task modelling",
    "text": "Probabilistic task modelling\nMotivation: wide variation of task performance when evaluating the same meta-learning model.\n\n\nFigure 2: The histogram accuracy of 15,504 5-way 1-shot classification tasks."
  },
  {
    "objectID": "presentations/pecman/index.html#probabilistic-task-modelling-1",
    "href": "presentations/pecman/index.html#probabilistic-task-modelling-1",
    "title": "Learning with limited and uncertain data",
    "section": "Probabilistic task modelling",
    "text": "Probabilistic task modelling\nEmploy latent Dirichlet allocation in topic modelling to model tasks in meta-learning\n\n\n\n\n\n\n\nAnalogy between topic modelling and probabilistic task modelling\n\n\n\n\n\nTopic modelling\nProbabilistic task modelling\n\n\n\n\nTopic\nTask-theme\n\n\nDocument\nTask (dataset)\n\n\nWord\nData point (image)\n\n\n\n\n\n\n\n\n\\[\n\\text{A task } T_{i} \\gets \\sum_{k = 1}^{K} \\pi_{ik} \\mathcal{N}(\\mu_{k}, \\Sigma_{k}).\n\\]"
  },
  {
    "objectID": "presentations/pecman/index.html#probabilistic-task-modelling-2",
    "href": "presentations/pecman/index.html#probabilistic-task-modelling-2",
    "title": "Learning with limited and uncertain data",
    "section": "Probabilistic task modelling",
    "text": "Probabilistic task modelling\nA task is represented as a point \\((\\pi_{1}, \\pi_{2}, \\dots, \\pi_{K})\\) in the latent task-theme simplex.\n\n\nFigure 3: A task is a point in the latent task-theme simplex"
  },
  {
    "objectID": "presentations/pecman/index.html#task-weighting-for-meta-learning",
    "href": "presentations/pecman/index.html#task-weighting-for-meta-learning",
    "title": "Learning with limited and uncertain data",
    "section": "Task weighting for meta-learning",
    "text": "Task weighting for meta-learning\n\n\n\n\n\n\nMotivation\n\n\nTasks are un-evenly distributed \\(\\to\\) biased towards frequently seen tasks.\n\n\n\n\n\n\n\n\n\n\nMethod with linear quadratic regulator\n\n\n\n\n\nOptimal control\nTask weighting\n\n\n\n\nState\nMeta-parameter\n\n\nAction\nWeighting vector\n\n\nTransition dynamics\nGradient update\n\n\nCost\nLoss + regularisation\n\n\n\nTL;DR: Look ahead a few epochs and optimise for weighting vector"
  },
  {
    "objectID": "presentations/pecman/index.html#noisy-label-learning",
    "href": "presentations/pecman/index.html#noisy-label-learning",
    "title": "Learning with limited and uncertain data",
    "section": "Noisy label learning",
    "text": "Noisy label learning\nAn annotated data point is represented as \\((x, p(t | x))\\).\n\n\n\n\n\n\n\nClean label\n\n\n\\[\ny = \\operatorname*{argmax}_{c \\in \\{1, \\dots, C\\}} p(t = c | x)\n\\]\n\n\n\n\n\n\n\n\n\n\n\nNoisy label\n\n\n\\[\n\\hat{y} \\sim \\operatorname{Categorical} \\left(y; \\frac{p(t | x) + \\varepsilon}{Z} \\right),\n\\] where: \\(\\varepsilon\\) is label noise and \\(Z\\) is the normalised constant.\n\n\n\n\n\n\\(\\implies \\hat{y}\\) may or may not be the same as \\(y\\)."
  },
  {
    "objectID": "presentations/pecman/index.html#identifiability-in-noisy-label-learning",
    "href": "presentations/pecman/index.html#identifiability-in-noisy-label-learning",
    "title": "Learning with limited and uncertain data",
    "section": "Identifiability in noisy label learning",
    "text": "Identifiability in noisy label learning\n\n\n\n\n\n\nRelation between \\(\\hat{y}\\) and \\(y\\)\n\n\n\\[\np(\\hat{y} | x) = \\sum_{c = 1}^{C} p(\\hat{y} | x, y = c) \\, p(y = c | x).\n\\]\n\n\n\n\n\n\n\n\n\n\nNon-identifiability without additional assumptions\n\n\n\\[\n\\begin{aligned}\n    p(\\hat{y} | x) = \\begin{bmatrix}\n        0.25 \\\\\n        0.45 \\\\\n        0.3\n    \\end{bmatrix} & = \\underbrace{\\begin{bmatrix}\n        0.8 & 0.1 & 0.2 \\\\\n        0.15 & 0.6 & 0.3 \\\\\n        0.05 & 0.3 & 0.5\n    \\end{bmatrix}}_{p_{1} (\\hat{y} | x, y)} \\underbrace{\\begin{bmatrix}\n        \\frac{2}{11} \\\\\n        \\frac{13}{22} \\\\\n        \\frac{5}{22}\n    \\end{bmatrix}}_{p_{1} (y | x)} = \\underbrace{\\begin{bmatrix}\n        0.7 & 0.2 & 0.1 \\\\\n        0.1 & 0.6 & 0.1 \\\\\n        0.2 & 0.2 & 0.8\n    \\end{bmatrix}}_{p_{2} (\\hat{y} | x, y)} \\underbrace{\\begin{bmatrix}\n        \\frac{2}{15} \\\\\n        \\frac{7}{10} \\\\\n        \\frac{1}{6}\n    \\end{bmatrix}}_{p_{2} (y | x)}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentations/pecman/index.html#identifiability-in-noisy-label-learning-1",
    "href": "presentations/pecman/index.html#identifiability-in-noisy-label-learning-1",
    "title": "Learning with limited and uncertain data",
    "section": "Identifiability in noisy label learning",
    "text": "Identifiability in noisy label learning\nPropose: employ multiple (or multinomial) noisy labels: \\[\np \\left( \\hat{y} | x; \\pi, \\rho \\right) = \\sum_{c = 1}^{C} \\underbrace{\\pi_{c}}_{p(y = c | x)} \\, \\underbrace{\\operatorname{Mult}(\\hat{y}; N, \\rho_{c})}_{p(\\hat{y} = c | x, y)}.\n\\]\n\n\n\n\n\n\n\nClaim\n\n\nAny noisy label learning problem where the noisy label distribution is modelled as a multinomial mixture model is identifiable if and only if there are at least \\(2C - 1\\) samples of noisy label \\(\\hat{y}\\) for an instance \\(x\\) with \\(C\\) being the number of classes."
  },
  {
    "objectID": "presentations/pecman/index.html#identifiability-in-noisy-label-learning-2",
    "href": "presentations/pecman/index.html#identifiability-in-noisy-label-learning-2",
    "title": "Learning with limited and uncertain data",
    "section": "Identifiability in noisy label learning",
    "text": "Identifiability in noisy label learning\n\n\n\n\n\n\nGenerate additional noisy labels through K nearest neighbours\n\n\n\\[\np(\\hat{y}_{i} | x_{i}) \\gets \\mu p(\\hat{y}_{i} | x_{i}) + (1 - \\mu) \\sum_{k = 1}^{K} \\mathbf{A}_{ik} p(\\hat{y}_{k} | x_{k}),\n\\] where \\(\\mathbf{A}\\) is a matrix consisting of similarity coefficients between nearest neighbours.\nAdditional noisy labels can be sampled from the obtained \\(p(\\hat{y}_{i} | x_{i})\\).\n\n\n\nThe EM algorithm is then used to infer \\(\\pi\\) and \\(\\rho\\)."
  },
  {
    "objectID": "index.html#biography",
    "href": "index.html#biography",
    "title": "Cuong Nguyen",
    "section": "Biography",
    "text": "Biography\n\nI am a Research Fellow at the Centre for Vision, Speech and Signal Processing, University of Surrey.\nBefore that, I was a Research Associate at the School of Computer Science, The University of Adelaide from April 2022 to January 2024. I received my PhD in Computer Science from The University of Adelaide in March 2022, an MPhil in Electronic Engineering also from The University of Adelaide in January 2018, and a B.S. in Mechanical Engineering from Portland State University in June 2012."
  },
  {
    "objectID": "index.html#areas-of-study",
    "href": "index.html#areas-of-study",
    "title": "Cuong Nguyen",
    "section": "Areas of study",
    "text": "Areas of study\nMachine learning, graphical models and probabilistic programming"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Cuong Nguyen",
    "section": "Publications",
    "text": "Publications\n\n Show publications\n\n\n\n\nDisplay More"
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Cuong Nguyen",
    "section": "Teaching",
    "text": "Teaching\n\n\nCourses taught at The University of Adelaide\n\n\n\n\n\n\n\n\n\n\n\nYear\nSemester\nCourse\nLevel\nRole\n\n\n\n\n2023\n2\nGrand Challenges in Computer Science\nUndergraduate\nIntructor\n\n\n2022\n2\nGrand Challenges in Computer Science\nUndergraduate\nIntructor\n\n\n2021\n1\nFoundations on Computer Science\nMaster\nTutor\n\n\n2021\n2\nFoundations on Computer Science\nMaster\nTutor"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Cuong Nguyen",
    "section": "Experience",
    "text": "Experience\n\n\n\n\n\ngantt\n    %% title Timeline\n    dateFormat YYYY-MM-DD\n    %% tickInterval 1month\n    axisFormat %y-%m\n    section 1\n        Research Fellow - University of Surrey      :surrey, 2024-03-07, 2025-03-31\n    section 0\n        Research Associate - University of Adelaide :adelaide, 2022-04-08, 2024-01-31"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Cuong Nguyen",
    "section": "Education",
    "text": "Education\n\n\n\n\n\nPhD in Computer Science\n The University of Adelaide, Australia\n April 2018 - March 2022\n\n\n\n\n\n\n\nMPhil in Electronic Engineering\n The University of Adelaide, Australia\n August 2015 - January 2018\n\n\n\n\n\n\n\nB.S. in Mechanical Engineering\n Portland State University, USA\n July 2010 - June 2012"
  },
  {
    "objectID": "posts/mixture-models/index.html",
    "href": "posts/mixture-models/index.html",
    "title": "Expectation - Maximisation algorithm and its applications in finite mixture models",
    "section": "",
    "text": "Missing data and latent variables are frequently encountered in various machine learning and statistical inference applications. A common example is the finite mixture model, which includes Gaussian mixture and multinomial mixture models. Due to the inherent nature of missing data or latent variables, calculating the likelihood of these models requires marginalisation over the latent variable distribution. This, in turn, complicates the process of maximum likelihood estimation (MLE).\nThe expectation-maximisation (EM) algorithm, introduced in (Dempster, Laird, and Rubin 1977), offers a general technique for handling latent variable models. The fundamental concept behind the EM algorithm is to iterate between two steps: the E-step (expectation step) and the M-step (maximization step). In the E-step, the posterior distribution of the latent variables (or missing data) is estimated. This estimated information is then used in the M-step to compute the MLE as if the data were complete. It has been proven that this iterative process guarantees a non-decreasing likelihood function. In simpler terms, the EM algorithm converges to a saddle point.\nWhile the EM algorithm is a powerful tool, this explanation may not be as clear as desired. Consequently, this post aims to provide a more accessible explanation of the EM algorithm. Additionally, some readers may question the choice of EM over stochastic gradient descent (SGD), a prevalent optimisation method. This post will, therefore, explore the key differences between these two approaches. Finally, the applications of the EM algorithm in the context of finite mixture modeling, specifically focusing on the MLE problems in Gaussian mixture models and multinomial mixture models, are also demonstrated."
  },
  {
    "objectID": "posts/mixture-models/index.html#notations",
    "href": "posts/mixture-models/index.html#notations",
    "title": "Expectation - Maximisation algorithm and its applications in finite mixture models",
    "section": "1 Notations",
    "text": "1 Notations\nBefore diving into the explanation and formulation, it is important to define the notations used in this post as follows:\n\nNotations used in the formulation of the EM algorithm.\n\n\nNotation\nDescription\n\n\n\n\n\\mathbf{x} \\in \\mathbb{R}^{D}\nobservable data\n\n\n\\mathbf{z} \\in \\mathbb{R}^{K}\nlatent variable or missing data\n\n\n\\theta \\in \\Theta\nthe parameter of interest in MLE"
  },
  {
    "objectID": "posts/mixture-models/index.html#em-algorithm",
    "href": "posts/mixture-models/index.html#em-algorithm",
    "title": "Expectation - Maximisation algorithm and its applications in finite mixture models",
    "section": "2 EM algorithm",
    "text": "2 EM algorithm\nThe formulation presented in this post follows a probabilistic approach. In probabilistic modelling, there are two processes: data generation (also known as a forward problem) and parameter inference (also known as an inverse problem).\n\n2.1 Data generation\nThe data is generated as follows:\n\ndraw the parameter \\pi from its prior: \\pi \\sim \\Pr(\\pi),\ndraw the parameter \\theta from its prior: \\theta \\sim \\Pr(\\theta),\ndraw a hidden sample \\mathbf{z} from a prior distribution: \\mathbf{z} \\sim \\Pr(\\mathbf{z} | \\pi), and\ndraw an observable sample \\mathbf{x} given \\mathbf{z} as follows: \\mathbf{x} \\sim \\Pr(\\mathbf{x} | \\mathbf{z}, \\theta),\n\nwhere \\pi and \\theta are the parameter of the model of interest.\n\n\n\n\n\n\nParameter \\pi\n\n\n\nIn many tutorials of EM, the parameter \\pi of the prior of the latent variable \\mathbf{z} is often defined implicitly. In this post, it is defined explicitly to make the explanation easier to follow.\n\n\nSuch a data generation process is often visualised by the graphical model shown below\n\n\n\n\n\n%%{\n    init: {\n        'theme': 'base',\n        'themeVariables': {\n            'primaryColor': '#ffffff'\n        }\n    }\n}%%\nflowchart LR\n    subgraph data[\"data\"]\n        z((z)):::nonfilled--&gt;x((x)):::filled;\n    end\n    pi((π)):::nonfilled--&gt;z;\n    theta((θ)):::nonfilled--&gt;x;\n\n    linkStyle default stroke: black;\n    classDef nonfilled fill: none;\n    style data fill: none;\n\n\n\n\n\n\n\n\n2.2 Parameter inference\nGiven a set of observed i.i.d data \\mathcal{D} = \\{\\mathbf{x}_{i}\\}_{i = 1}^{N}, the general objective is to infer the posterior \\Pr(\\pi, \\theta | \\mathbf{x}). of the parameters \\pi and \\theta. Instead of inferring the exact posterior \\Pr(\\pi, \\theta | \\mathbf{x}), which may be difficult in many cases, one can perform point estimate, such as MLE or maximise a posterior (MAP), which can be written as follows:\n\n\\begin{aligned}\n    \\max_{\\pi, \\theta} \\ln \\Pr(\\pi, \\theta | \\{\\mathbf{x}_{i}\\}_{i = 1}^{N}) & = \\max_{\\pi. \\theta} \\sum_{i = 1}^{N} \\underbrace{\\ln \\Pr(\\mathbf{x}_{i} | \\pi, \\theta)}_{\\text{in-complete log-likelihood}} + \\ln \\Pr(\\pi) + \\ln \\Pr(\\theta) \\\\\n    & = \\max_{\\pi, \\theta} \\sum_{i = 1}^{N} \\ln \\left[ \\sum_{\\mathbf{z}_{i}} \\Pr(\\mathbf{x}_{i}, \\mathbf{z}_{i} | \\pi, \\theta) \\right] + \\ln \\Pr(\\pi) + \\ln \\Pr(\\theta).\n\\end{aligned}\n\\tag{1}\nDue to the presence of the sum over the latent variable \\mathbf{z}, the in-complete log-likelihood may not be evaluated directly on the joint distribution (especially when \\mathbf{z} is continuous), making the optimisation difficult.\nFortunately, according to the data generation presented in Section 2.1, the completed log-likelihood \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi, \\theta) can be evaluated easily:\n\n\\ln \\Pr(\\mathbf{x}, \\mathbf{z}| \\pi, \\theta) = \\ln \\Pr(\\mathbf{x} | \\mathbf{z}, \\theta) + \\ln \\Pr(\\mathbf{z} | \\pi).\n\nSuch an assumption allows EM to get around the difficulty when evaluating the expression in Equation 1.\n\n\n\n\n\n\nMain idea behind EM\n\n\n\n\nfind a lower bound of the objective function in Equation 1,\ntighten the lower bound, and\nmaximise the tightest lower bound.\n\n\n\nThe first two sub-steps combined are often known as the Expectation step (or E-step for short), while the last step is known as the Maximisation step (or M-step for short). These steps are then presented in the following sub-sub-sections.\n\n2.2.1 Evidence lower bound (ELBO)\nTo find a lower bound of the objective function in Equation 1, one can follow the variational inference approach to obtain the ELBO. In particular, let q(\\mathbf{z}) &gt; 0 be an arbitrary distribution of the latent variable \\mathbf{z}. The in-complete log-likelihood in Equation 1 can be re-written as follows: \n    \\begin{aligned}\n        \\ln \\Pr(\\mathbf{x} | \\pi, \\theta) & = \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\ln \\Pr(\\mathbf{x} | \\pi, \\theta) \\right] \\\\\n        & = \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\ln \\Pr(\\mathbf{x} | \\pi, \\theta) + \\ln \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta) - \\ln \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta) + \\ln q(\\mathbf{z}) - \\ln q(\\mathbf{z}) \\right] \\\\\n        & = \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\ln \\Pr(\\mathbf{x} | \\pi, \\theta) + \\ln \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta) - \\ln q(\\mathbf{z}) \\right] + \\mathbb{E}_{q(\\mathbf{z})}\\left[ \\ln q(\\mathbf{z}) - \\ln \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta) \\right] \\\\\n        & = \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi, \\theta) - \\ln q(\\mathbf{z}) \\right] + \\operatorname{KL} \\left[ q(\\mathbf{z}) \\| \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta) \\right],\n    \\end{aligned}\n where: \\operatorname{KL}[ q \\| p ] is the Kullback-Leibler divergence (KL divergence for short) between probability distributions q and p.\nSince \\operatorname{KL}[ q \\| p ] \\ge 0 and \\operatorname{KL}[ q \\| p ] = 0 iff q = p, the log-likelihood of interest can be lower-bounded as: \n    \\ln \\Pr(\\mathbf{x} | \\pi, \\theta) \\ge \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi, \\theta) - \\ln q(\\mathbf{z}) \\right],\n and the equality occurs iff q(\\mathbf{z}) = \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta), which is the posterior of the latent variable \\mathbf{z} after observing the data \\mathbf{x}.\n\n\n2.2.2 Tightening the ELBO\nTo obtain the tightest lower bound, one must perform the following optimisation:\n\nq^{*} = \\operatorname*{argmax}_{q} \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi, \\theta) - \\ln q(\\mathbf{z}) \\right].\n\\tag{2}\nAs mentioned above, the tightest bound is when q^{*}(\\mathbf{z}) = \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta), or the “variational” posterior approaches the true posterior of the latent variable \\mathbf{z}. Such a true posterior can be obtained in certain simple cases, but is intractable when the modelling becomes more complex. In those cases, only a local optima “variational” posterior q(\\mathbf{z}) is calculated (Bernardo et al. 2003).\n\n\n\n\n\n\nTrue posterior in the E-step\n\n\n\nSuch an observation explains why in the vanilla EM, it is often stated that the E-step is to calculate the true posterior of the latent variable \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)}). The superscript t denotes the parameters at the t-th iteration. This is to avoid taking them into account when maximising the completed-log-likelihood in the M-step. Instead of following that convention, q^{*} is used to avoid the confusion.\n\n\n\n\n2.2.3 Maximising the possibly-tightest lower bound\nFinally, the possibly-tightest lower bound is then maximised with respect to the parameters \\pi and \\theta as follows:\n\n\\pi^{(t + 1)}, \\theta^{(t + 1)} \\gets \\operatorname*{argmax}_{\\pi, \\theta} \\sum_{i = 1}^{N} \\mathbb{E}_{q^{*}(\\mathbf{z}_{i})} \\left[ \\ln \\Pr(\\mathbf{x}_{i}, \\mathbf{z}_{i} | \\pi, \\theta) - \\cancel{\\ln q^{*}(\\mathbf{z})} \\right] + \\ln \\Pr(\\pi) + \\ln \\Pr(\\theta).\n\\tag{3}\nIn summary, instead of maximising the difficult-to-calculate objective function in Equation 1, the EM algorithm is to execute the alternative optimisation written as follows:\n\n\\max_{\\pi, \\theta} \\max_{q_{i}} \\sum_{i = 1}^{N} \\mathbb{E}_{q(\\mathbf{z}_{i})} \\left[ \\ln \\Pr(\\mathbf{x}_{i}, \\mathbf{z}_{i} | \\pi, \\theta) - \\ln q(\\mathbf{z}) \\right] + \\ln \\Pr(\\pi) + \\ln \\Pr(\\theta).\n\nThe whole EM algorithm can be referred to Algorithm 1.\n\n\n\\begin{algorithm} \\caption{Expectation - Maximisation algorithm} \\begin{algorithmic} \\Procedure{EM}{$\\mathbf{x}$} \\State initialise mixture coefficient $\\pi$ \\State initialise $\\theta$ \\While{not converged} \\State calculate the ELBO: $Q \\gets \\operatorname{E-step}(\\mathbf{x}, \\pi, \\theta)$ \\State maximise the ELBO: $\\pi, \\theta \\gets \\operatorname{M-step}(Q, \\pi, \\theta)$ \\EndWhile \\State return $\\pi, \\theta$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n2.3 Convergence of the EM algorithm\nThe following theorem proves that the EM algorithm improves the lower-bound after every iteration. For simplicity, the priors \\Pr(\\pi) and \\Pr(\\theta) are ignored from the proof below, but extending to include these prior terms is trivial.\n\nTheorem 1 Assume that q^{*}(\\mathbf{z}) = \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta), then after each EM iteration, the log-likelihood \\ln \\Pr(\\mathbf{x} | \\pi, \\theta) is non-decreasing. Mathematically, it can be written as follows: \n    \\Pr(\\mathbf{x} | \\pi^{(t + 1)}, \\theta^{(t + 1)}) \\ge \\Pr(\\mathbf{x} | \\pi^{(t)}, \\theta^{(t)}),\n where the superscript denotes the result obtained after that iteration.\n\n\nProof. The log-likelihood of interest can be written as: \n    \\begin{aligned}\n        \\ln \\Pr(\\mathbf{x} | \\pi, \\theta) & = \\mathbb{E}_{\\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)})} \\left[ \\ln \\Pr(\\mathbf{x} | \\pi, \\theta) \\right] \\\\\n        & = \\mathbb{E}_{\\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi, \\theta) - \\ln \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta) \\right].\n    \\end{aligned}\n\\tag{4}\nSince it holds for any (\\pi, \\theta), substituting \\pi = \\pi^{(t)} and \\theta = \\theta^{(t)} gives: \n    \\ln \\Pr(\\mathbf{x} | \\pi^{(t)}, \\theta^{(t)}) = \\mathbb{E}_{\\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi^{(t)}, \\theta^{(t)}) - \\ln \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)}) \\right].\n\\tag{5}\nSubstracting side by side of Equation 4 and Equation 5 gives the following: \n    \\begin{aligned}\n        & \\ln \\Pr(\\mathbf{x} | \\pi, \\theta) - \\ln \\Pr(\\mathbf{x} | \\pi^{(t)}, \\theta^{(t)}) \\\\\n        & = \\mathbb{E}_{\\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi, \\theta) \\right.\\\\\n        & \\quad \\left. - \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi^{(t)}, \\theta^{(t)}) + \\ln \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)}) - \\ln \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta) \\right] \\\\\n        & = \\mathbb{E}_{\\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi, \\theta) \\right.\\\\\n        & \\quad \\left. - \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi^{(t)}, \\theta^{(t)}) \\right] + \\operatorname{KL} \\left[ \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)}) \\| \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\theta) \\right].\n    \\end{aligned}\n\nSince KL divergence is non-negative, one can imply that: \n    \\begin{aligned}\n        & \\ln \\Pr(\\mathbf{x} | \\pi, \\theta) - \\ln \\Pr(\\mathbf{x} | \\pi^{(t)}, \\theta^{(t)}) \\\\\n        & \\quad \\ge \\mathbb{E}_{\\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi, \\theta) - \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi^{(t)}, \\theta^{(t)}) \\right].\n    \\end{aligned}\n\\tag{6}\nIn the M-step, the parameters (\\pi^{(t + 1)}, \\theta^{(t + 1)}) are obtained by maximising the first term in the right hand side: \\mathbb{E}_{\\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi, \\theta) \\right] w.r.t. (\\pi, \\theta). Thus, according to the definition of the maximisation: \n    \\mathbb{E}_{\\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi^{(t + 1)}, \\theta^{(t + 1)}) \\right] \\ge \\mathbb{E}_{\\Pr(\\mathbf{z} | \\mathbf{x}, \\pi^{(t)}, \\theta^{(t)})} \\left[ \\ln \\Pr(\\mathbf{x}, \\mathbf{z} | \\pi^{(t)}, \\theta^{(t)}) \\right].\n\nHence, one can conclude that: \n    \\ln \\Pr(\\mathbf{x} | \\pi^{(t + 1)}, \\theta^{(t + 1)}) \\ge \\ln \\Pr(\\mathbf{x} | \\pi^{(t)}, \\theta^{(t)})."
  },
  {
    "objectID": "posts/mixture-models/index.html#applications-of-em-in-finite-mixture-models",
    "href": "posts/mixture-models/index.html#applications-of-em-in-finite-mixture-models",
    "title": "Expectation - Maximisation algorithm and its applications in finite mixture models",
    "section": "3 Applications of EM in finite mixture models",
    "text": "3 Applications of EM in finite mixture models\nOne of the typical applications of EM algorithm is to perform maximum likelihood for finite mixture models. This section is, therefore, dedicated to discuss the application of EM on Gaussian and multinomial mixture models.\n\n3.1 Gaussian mixture models\nThe Gaussian mixture distribution can be written as a convex combination of K Gaussian components: \n    \\Pr(\\mathbf{x} | \\pi, \\mu, \\Sigma) = \\sum_{k = 1}^{K} \\pi_{k} \\, \\mathcal{N}(\\mathbf{x}; \\mu_{k}, \\Sigma_{k}),\n where: \\pi_{k} \\in [0, 1] and \\pmb{\\pi}^{\\top} \\pmb{1} = 1.\n\n3.1.1 Data generation\nA data-point of the above Gaussian mixture distribution can be generated as follows:\n\nsample a probability \\pi from a Dirichlet prior: \\pi \\sim \\Pr(\\pi | \\alpha) = \\operatorname{Dir}(\\pi | \\alpha),\nsample K sets of parameters (\\mu_{k}, \\Sigma_{k}) from an normal-inverse-Wishart prior: (\\mu_{k}, \\Sigma_{k}) \\sim \\Pr(\\mu, \\Sigma | m, \\lambda, \\Psi, \\nu) = \\operatorname{NIW}(\\mu, \\Sigma | m, \\lambda, \\Psi, \\nu),\nsample the index of a Gaussian component: \\mathbf{z} \\sim \\Pr(\\mathbf{z} | \\pi) = \\operatorname{Categorical}(\\mathbf{z} | \\pmb{\\pi}), then\nsample a data-point from the corresponding Gaussian component: \\mathbf{x} \\sim \\Pr(\\mathbf{x} | \\mathbf{z}, \\mu, \\Sigma) = \\mathcal{N}(\\mathbf{x}| \\mu_{k}, \\Sigma_{k}), where z_{k} = 1.\n\nThe data generation process can also be visualised in the graphical model shown below.\n\n\n\n\n\n%%{\n    init: {\n        'theme': 'base',\n        'themeVariables': {\n            'primaryColor': '#ffffff'\n        }\n    }\n}%%\nflowchart LR\n    subgraph data[\"data\"]\n        direction LR\n        z((z)):::rv --&gt; x((x)):::rv\n    end\n    alpha((α)):::notfilled --&gt; pi((π)):::params --&gt; z\n    sigma((Σ)):::params --&gt; mu\n    psi((Ψ)):::notfilled --&gt; sigma\n    nu((ν)):::notfilled --&gt; sigma\n    sigma --&gt; x\n    mu0((m)):::notfilled --&gt; mu((μ)):::params --&gt; x\n    lambda((λ)):::notfilled --&gt; mu\n\n    style z fill: none\n    classDef params stroke: #000, fill: none\n    classDef rv stroke: #000\n    classDef notfilled fill: none\n    linkStyle default stroke: #000\n    style data fill: none\n\n\n\n\n\n\n\n\n3.1.2 Objective\nGiven set of data-points \\{\\mathbf{x}_{i}\\}_{i = 1}^{N} sampled from the Gaussian mixture distribution, the aim is to infer the point estimate, and in particular MAP, of (\\pi, \\mu, \\Sigma). Such an objective can be written as follows:\n\n\\begin{aligned}\n    & \\max_{\\pi, \\mu, \\Sigma} \\ln \\Pr(\\pi, \\mu, \\Sigma | \\{\\mathbf{x}_{i}\\}_{i = 1}^{N}, \\alpha, m, \\lambda, \\Psi, \\nu) \\\\\n    &= \\max_{\\pi, \\mu, \\Sigma} \\frac{1}{N} \\sum_{i = 1}^{N} \\ln \\Pr(\\mathbf{x}_{i} | \\pi, \\mu, \\Sigma) + \\ln \\operatorname{Dir}(\\pi | \\alpha) + \\ln \\operatorname{NIW}(\\mu, \\Sigma | m, \\lambda, \\Psi, \\nu).\n\\end{aligned}\n\n\n\n3.1.3 Parameter inference\nIn this case, one can simply follow the EM algorithm presented in Section Section 2.2. Note that the likelihood on N iid data-points can be written as:\n\n    \\prod_{i = 1}^{N} \\Pr(\\mathbf{x}_{i} | \\pi, \\theta) = \\prod_{i = 1}^{N} \\sum_{k = 1}^{K} \\Pr(\\mathbf{x}_{i} | \\mathbf{z}_{ik} = 1, \\theta) \\, \\Pr(z_{ik} = 1 | \\pi).\n\nE-step: optimises the lower bound with respect to the “variational” posterior. As shown in Section 2.2, q^{*} = \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\mu, \\Sigma) results in the tightest bound. Fortunately, in this case of Gaussian mixture models, the true posterior \\Pr(\\mathbf{z} | \\mathbf{x}, \\pi, \\mu, \\Sigma) can be calculated in closed-form as follows:\n\n    \\boxed{\n        \\begin{aligned}\n            q^{*}(\\mathbf{z}_{ik} = 1) & = \\Pr(\\mathbf{z}_{ik} = 1 | \\mathbf{x}_{i}, \\pi^{(t)}, \\mu^{(t)}, \\Sigma^{(t)}) \\\\\n            & = \\frac{\\Pr(\\mathbf{x}_{i} | \\mathbf{z}_{ik} = 1, \\mu^{(t)}, \\Sigma^{(t)}) \\, \\Pr(\\mathbf{z}_{ik} = 1 | \\pi^{(t)})}{\\sum_{j = 1}^{K} \\Pr(\\mathbf{x}_{i} | \\mathbf{z}_{ij} = 1, \\mu^{(t)}, \\Sigma^{(t)}) \\, \\Pr(\\mathbf{z}_{ij} = 1 | \\pi^{(t)})} \\\\\n            & \\quad (\\text{Bayes' rule}) \\\\\n            & = \\frac{\\pi_{k} \\, \\mathcal{N}(\\mathbf{x}_{i}; \\mu_{k}^{(t)}, \\Sigma_{k}^{(t)})}{\\sum_{j = 1}^{K} \\pi_{j} \\, \\mathcal{N}(\\mathbf{x}_{i}; \\mu_{j}^{(t)}, \\Sigma_{j}^{(t)})}.\n        \\end{aligned}\n    }\n\\tag{7}\nM-step: maximises the “tighest” lower-bound w.r.t. model parameter (\\pi, \\mu, \\Sigma): \n    \\begin{aligned}\n        & \\operatorname*{argmax}_{\\pi, \\mu, \\Sigma} \\sum_{i = 1}^{N} \\mathbb{E}_{q^{*}(\\mathbf{z}_{i})} [ \\ln \\Pr(\\mathbf{x}_{i} | \\mathbf{z}_{i}, \\mu, \\Sigma) + \\ln \\Pr(\\mathbf{z}_{i} | \\pi) ] + \\ln \\Pr(\\pi | \\alpha) + \\ln \\Pr(\\mu, \\Sigma | m, \\lambda, \\Psi, \\nu) \\\\\n        & = \\operatorname*{argmax}_{\\mu, \\Sigma} \\sum_{i = 1}^{N} \\sum_{k = 1}^{K} q^{*}(\\mathbf{z}_{ik} = 1) \\left[\\ln \\Pr(\\mathbf{x}_{i} | \\mathbf{z}_{ik} = 1, \\pi, \\mu, \\Sigma) + \\ln \\Pr(\\mathbf{z}_{ik} = 1| \\pi) \\right] \\\\\n        & \\quad + \\ln \\operatorname{Dir}(\\pi | \\alpha) + \\ln \\operatorname{NIW}(\\mu_{k}, \\Sigma_{k} | m, \\lambda, \\Psi, \\nu)\\\\\n        & = \\operatorname*{argmax}_{\\mu, \\Sigma} \\sum_{i = 1}^{N} \\sum_{k = 1}^{K} q^{*}(\\mathbf{z}_{ik} = 1) \\left[ \\ln \\mathcal{N}(\\mathbf{x}_{i}; \\mu_{k}, \\Sigma_{k}) + \\ln \\pi_{k} \\right] \\\\\n        & \\quad + (\\alpha_{k} - 1) \\ln \\pi_{k} + \\ln \\mathcal{N} \\left( \\mu_{k} \\left| m, \\frac{1}{\\lambda} \\Sigma_{k} \\right. \\right) + \\ln \\mathcal{W}^{-1} \\left( \\Sigma_{k} | \\Psi, \\nu \\right)\\\\\n        & = \\operatorname*{argmax}_{\\mu, \\Sigma} -\\frac{1}{2} \\sum_{i = 1}^{N} \\sum_{k = 1}^{K} q^{*}(\\mathbf{z}_{ik} = 1) \\left[ \\ln \\left| \\Sigma_{k} \\right| + (\\mathbf{x}_{i} - \\mu_{k})^{\\top} \\Sigma_{k}^{-1} (\\mathbf{x}_{i} - \\mu_{k}) + \\ln \\pi_{k} \\right] \\\\\n        & \\quad + (\\alpha_{k} - 1) \\ln \\pi_{k} - \\frac{\\nu + D + 2}{2} \\ln |\\Sigma_{k}| - \\frac{1}{2} \\operatorname{Tr} \\left( \\Psi \\Sigma_{k}^{-1} \\right) - \\frac{\\lambda}{2} (\\mu_{k} - m)^{\\top} \\Sigma_{k}^{-1} (\\mu_{k} - m).\n    \\end{aligned}\n\nTaking derivative with respect to \\mu_{k} and setting it to zero give:\n\n    \\begin{aligned}\n        & \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) \\Sigma_{k}^{-1} (\\mathbf{x}_{i} - \\mu_{k}) - \\lambda \\Sigma_{k}^{-1} (\\mu_{k} - m) = 0 \\\\\n        & \\Leftrightarrow \\left[ \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) + \\lambda \\right] \\mu_{k} = \\sum_{i = 1}^{N} \\gamma(\\mathbf{z}_{ik}) \\mathbf{x}_{i} + \\lambda m.\n    \\end{aligned}\n\nOr:\n\n    \\boxed{\n        \\mu_{k} = \\frac{\\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) \\mathbf{x}_{i} + \\lambda m}{\\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) + \\lambda}.\n    }\n\\tag{8}\nSimilarly for \\Sigma_{k}:\n\n    \\begin{aligned}\n        & -\\frac{1}{2} \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) \\left[ \\Sigma_{k}^{-1} - \\Sigma_{k}^{-1} (\\mathbf{x}_{i} - \\mu_{k}) (\\mathbf{x}_{i} - \\mu_{k})^{\\top} \\Sigma_{k}^{-1} \\right] \\\\\n        & \\quad + \\frac{1}{2} \\Sigma_{k}^{-1} \\Psi \\Sigma_{k}^{-1} - \\frac{\\nu + D + 2}{2} \\Sigma_{k}^{-1} + \\frac{\\lambda}{2} \\Sigma_{k}^{-1} (\\mu_{k} - m)^{\\top} (\\mu_{k} - m) \\Sigma_{k}^{-1} = 0.\n    \\end{aligned}\n\nTo solve for \\Sigma_{k}, the covariance matrix itself is used to left- and right-multiply to obtain:\n\n    \\begin{aligned}\n        & -\\frac{1}{2} \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) \\left[ \\Sigma_{k} - (\\mathbf{x}_{i} - \\mu_{k}) (\\mathbf{x}_{i} - \\mu_{k})^{\\top} \\right] \\\\\n        & \\quad + \\frac{1}{2} \\Psi - \\frac{\\nu + D + 2}{2} \\Sigma_{k} + \\frac{\\lambda}{2} (\\mu_{k} - m)^{\\top} (\\mu_{k} - m) = 0 \\\\\n        & \\Leftrightarrow \\left[ \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) + \\nu + D + 2 \\right] \\Sigma_{k} \\\\\n        & \\quad = \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) (\\mathbf{x}_{i} - \\mu_{k}) (\\mathbf{x}_{i} - \\mu_{k})^{\\top} + \\Psi + \\lambda (\\mu_{k} - m)^{\\top} (\\mu_{k} - m).\n    \\end{aligned}\n\nOr:\n\n\\boxed{\n    \\Sigma_{k} = \\frac{\\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) (\\mathbf{x}_{i} - \\mu_{k}) (\\mathbf{x}_{i} - \\mu_{k})^{\\top} + \\Psi + \\lambda (\\mu_{k} - m)^{\\top} (\\mu_{k} - m)}{\\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) + \\nu + D + 2}.\n}\n\\tag{9}\nOne can further substitute \\mu_{k} in Equation 8 into Equation 9 to obtain an expression for \\Sigma_{k} that only depends on observed data \\mathbf{x} and prior parameters.\nFinally, one can obtain the optimal value for the mixture coefficient \\pi_{k} in a similar way, except it is now a constrained optimisation. Such an optimisation can be written as follows:\n\n\\begin{aligned}\n    & \\max_{\\pi} \\sum_{k = 1}^{K} \\left[\\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) + \\alpha_{k} - 1 \\right] \\ln \\pi_{k} \\\\\n    & \\text{subject to: } \\sum_{k = 1}^{K} \\pi_{k} = 1.\n\\end{aligned}\n\nThe constrained optimisation above can simly be solved by Lagrange multiplier. The result for \\pi_{k} can then be expressed as:\n\n\\boxed{\n    \\pi_{k} = \\frac{\\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) + \\alpha_{k} - 1}{N - K + \\sum_{k = 1}^{K} \\alpha_{k}}.\n}\n\nOne can also refer to Chapter 10.2 in (Bishop 2006) for a similar derivation and result.\n\n\n\n3.2 Multinomial mixture models\nSimilar to the Gaussian mixture models, a multinomial mixture model can also be written as:\n\n    \\Pr(\\mathbf{x} | \\pi, m, \\rho) = \\sum_{\\mathbf{z}} \\Pr(\\mathbf{z} | \\pi) \\Pr(\\mathbf{x} | \\mathbf{z}, m, \\rho) = \\sum_{k = 1}^{K} \\pi_{k} \\mathrm{Mult}(\\mathbf{x}; m, \\rho_{k}).\n\n\n\n\n\n\n\nm is given\n\n\n\nOnly the case where all the multinomial components have the same parameter m (the number of trials) are considered. The reason is that optimising for an integer number m is beyond the scope of this post.\n\n\n\n3.2.1 Data generation\nA data-point of the multinomial mixture model can be generated as follows:\n\nsample a probability \\pi from a Dirichlet prior: \\pi \\sim \\Pr(\\pi | \\alpha) = \\operatorname{Dir}(\\pi | \\alpha),\nsample K probability vectors, \\{ \\rho_{k} \\}_{k = 1}^{K}), from a Dirichlet prior: \\rho_{k} \\sim \\Pr(\\rho | \\beta ) = \\operatorname{Dir}(\\rho | \\beta),\nsample the index of a multinomial component: \\mathbf{z} \\sim \\Pr(\\mathbf{z} | \\pi) = \\operatorname{Categorical}(\\mathbf{z} | \\pmb{\\pi}), then\nsample a data-point from the corresponding multinomial component: \\mathbf{x} \\sim \\Pr(\\mathbf{x} | \\mathbf{z}, \\rho) = \\operatorname{Multinomial}(\\mathbf{x}| \\rho_{k}), where z_{k} = 1.\n\nThe data generation process can also be visualised in the graphical model shown below.\n\n\n\n\n\n%%{\n    init: {\n        'theme': 'base',\n        'themeVariables': {\n            'primaryColor': '#ffffff'\n        }\n    }\n}%%\nflowchart LR\n    subgraph data[\"data\"]\n        direction LR\n        z((z)):::rv --&gt; x((x)):::rv\n    end\n    alpha((α)):::notfilled --&gt; pi((π)):::params --&gt; z;\n    beta((β)):::params --&gt; rho((ρ)):::params;\n    rho --&gt; x;\n\n    style z fill: none\n    classDef params stroke: #000, fill: none\n    classDef rv stroke: #000\n    classDef notfilled fill: none\n    linkStyle default stroke: #000\n    style data fill: none\n\n\n\n\n\n\n\n\n3.2.2 Objective\nGiven set of data-points \\{\\mathbf{x}_{i}\\}_{i = 1}^{N} sampled from a multinomial mixture distribution, the aim is to infer the point estimate, and in particular MAP, of (\\pi, \\rho) as follows:\n\n    \\max_{\\pi, \\rho} \\ln \\Pr(\\pi, \\rho | \\{\\mathbf{x}_{i}\\}_{i = 1}^{N}, \\alpha, m, \\beta) = \\max_{\\pi, \\rho} \\frac{1}{N} \\sum_{i = 1}^{N} \\ln \\Pr(\\mathbf{x}_{i} | \\pi, m, \\rho) + \\ln \\operatorname{Dir}(\\pi | \\alpha) + \\ln \\operatorname{Dir}(\\rho | \\beta).\n\n\n\n3.2.3 Parameter inference with EM\nE-step calculates the posterior of the latent variable \\mathbf{z}_{i} given the data \\mathbf{x}_{i}: \n    \\begin{aligned}\n        q^{*}(\\mathbf{z}_{ik} = 1) & = \\Pr(\\mathbf{z}_{ik} = 1 | \\mathbf{x}_{i}, \\pi^{(t)}, \\rho^{(t)}) \\\\\n        & = \\frac{\\Pr(\\mathbf{x}_{i} | \\mathbf{z}_{ik} = 1, \\rho^{(t)}) \\, \\Pr(\\mathbf{z}_{ik} = 1 | \\pi^{(t)})}{\\sum_{k = 1}^{K} \\Pr(\\mathbf{x}_{i} | \\mathbf{z}_{ik} = 1, \\rho^{(t)}) \\, \\Pr(\\mathbf{z}_{ik} = 1 | \\pi^{(t)})} \\\\\n        & = \\frac{\\pi_{k}^{(t)} \\, \\mathrm{Mult}(\\mathbf{x}_{i}; m, \\rho_{k}^{(t)})}{\\sum_{k = 1}^{K} \\pi_{k}^{(t)} \\, \\mathrm{Mult}(\\mathbf{x}_{i}; m, \\rho_{k}^{(t)})}.\n    \\end{aligned}\n\\tag{10}\nM-step In the M-step, we maximise the following expected completed log-likelihood w.r.t. \\pi and \\rho:\n\n    \\begin{aligned}\n        & \\operatorname*{argmax}_{\\pi, \\rho} \\sum_{i = 1}^{N} \\mathbb{E}_{q^{*}(\\mathbf{z}_{i})} [ \\ln \\Pr(\\mathbf{x}_{i} | \\mathbf{z}_{i}, m, \\rho) + \\ln \\Pr(\\mathbf{z}_{i} | \\pi) ] + \\ln \\Pr(\\pi | \\alpha) + \\ln \\Pr(\\rho | \\beta) \\\\\n        & = \\operatorname*{\\argmax}_{\\pi, \\rho} \\sum_{i = 1}^{N} \\mathbb{E}_{q^{*}(\\mathbf{z}_{i})} \\left[ \\sum_{k = 1}^{K} \\mathbf{z}_{ik} \\ln \\operatorname{Mult}(\\mathbf{x}_{i} | m, \\rho_{k}) + \\ln \\operatorname{Categorical}(\\mathbf{z}_{i} | \\pi) \\right] \\\\\n        & \\quad + \\ln \\operatorname{Dir}(\\pi | \\alpha) + \\ln \\operatorname{Dir}(\\rho | \\beta) \\\\& = \\operatorname*{\\argmax}_{\\pi, \\rho} \\sum_{i = 1}^{N} \\mathbb{E}_{q^{*}(\\mathbf{z}_{i})} \\left[ \\sum_{k = 1}^{K} \\mathbf{z}_{ik} \\left( \\sum_{d = 1}^{D} \\mathbf{x}_{id} \\ln \\rho_{kd} \\right) + \\mathbf{z}_{ik} \\ln \\pi_{k} \\right] \\\\\n        & \\quad + \\sum_{k = 1}^{K} (\\alpha - 1) \\ln \\pi_{k} + (\\beta - 1) \\ln \\rho_{k} \\\\\n        & = \\operatorname*{\\argmax}_{\\pi, \\rho} \\sum_{i = 1}^{N} \\sum_{k = 1}^{K} q^{*}(\\mathbf{z}_{ik} = 1) \\left[ \\ln \\pi_{k} + \\sum_{d = 1}^{D} \\mathbf{x}_{id} \\ln \\rho_{kd} \\right] + (\\alpha - 1) \\ln \\pi_{k} + (\\beta - 1) \\sum_{d = 1}^{D} \\ln \\rho_{kd}.\n    \\end{aligned}\n\n\n\n\n\n\n\nProbability constrains on \\pi and \\rho\n\n\n\nDue to the nature of a multinomial mixture model, both the parameters \\pi and \\rho are probability vectors.\n\n\nThe Lagrangian for \\pi can be written as: \n    \\mathsf{L}_{\\pi} = \\sum_{i = 1}^{N} \\sum_{k = 1}^{K} q^{*}(\\mathbf{z}_{ik} = 1) \\ln \\pi_{k} + (\\alpha - 1) \\ln \\pi_{k} - \\lambda \\left( \\sum_{k = 1}^{K} \\pi_{k} - 1 \\right),\n where \\lambda is the Lagrange multiplier.\nTaking derivative of the Lagrangian w.r.t. \\pi_{k} gives: \n    \\frac{\\partial \\mathsf{L}_{\\pi}}{\\partial \\pi_{k}} = \\frac{1}{\\pi_{k}} \\left[ \\alpha - 1 + \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) \\right] - \\lambda.\n\nSetting the derivative to zero and solving for \\pi_{k} gives: \n    \\pi_{k} = \\frac{1}{\\lambda} \\left[ \\alpha - 1 + \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) \\right].\n\nAnd since \\sum_{k = 1}^{K} \\pi_{k} = 1, one can substitute and find that \\lambda = N + K (\\alpha - 1). Thus: \n    \\boxed{\n        \\pi_{k}^{(t + 1)} = \\frac{\\alpha - 1 + \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1)}{N + K (\\alpha - 1)}.\n    }\n\nSimilarly, the Lagrangian of \\rho can be expressed as: \n    \\mathsf{L}_{\\rho} = \\sum_{i = 1}^{N} \\sum_{k = 1}^{K} q^{*}(\\mathbf{z}_{ik} = 1) \\sum_{d = 1}^{D} \\mathbf{x}_{id} \\ln \\rho_{kd} + (\\beta - 1) \\ln \\rho_{kd} - \\sum_{k = 1}^{K} \\eta_{k} \\left( \\sum_{d = 1}^{D} \\rho_{kd} - 1 \\right),\n where \\eta_{k} is the Lagrange multiplier. Taking derivative w.r.t. \\rho_{kd} gives: \n    \\frac{\\partial \\mathsf{L}_{\\rho}}{\\partial \\rho_{kd}} = \\frac{1}{\\rho_{kd}} \\left[ \\beta - 1 + \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) \\mathbf{x}_{id} \\right] - \\eta_{k}.\n Setting the derivative to zero and solving for \\rho_{kd} gives: \n    \\rho_{kd} = \\frac{1}{\\eta_{k}} \\left[ \\beta - 1 + \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) \\mathbf{x}_{id} \\right].\n The constraint on \\rho_{k} as a probability vector leads to \\eta_{k} = K (\\beta - 1) + m \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1). Thus: \n    \\boxed{\n        \\rho_{kd}^{(t + 1)} = \\frac{\\beta - 1 + \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1) \\mathbf{x}_{id}}{K (\\beta - 1) + m \\sum_{i = 1}^{N} q^{*}(\\mathbf{z}_{ik} = 1)}.\n    }\n\nOne can also refer to (Elmore and Wang 2003) for a similar derivation and result."
  },
  {
    "objectID": "posts/mixture-models/index.html#references",
    "href": "posts/mixture-models/index.html#references",
    "title": "Expectation - Maximisation algorithm and its applications in finite mixture models",
    "section": "4 References",
    "text": "4 References\n\n\nBernardo, JM, MJ Bayarri, JO Berger, AP Dawid, D Heckerman, AFM Smith, M West, et al. 2003. “The Variational Bayesian EM Algorithm for Incomplete Data: With Application to Scoring Graphical Model Structures.” Bayesian Statistics 7 (453-464): 210.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B (Methodological) 39 (1): 1–22.\n\n\nElmore, Ryan T, and Shaoli Wang. 2003. “Identifiability and Estimation in Finite Mixture Models with Multinomial Components.” Technical Report 03-04, Pennsylvania State University."
  },
  {
    "objectID": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html",
    "href": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html",
    "title": "Stochastic gradient and Hamiltonian Monte Carlo",
    "section": "",
    "text": "This post is to introduce the formulation of stochastic gradient descent as a Monte Carlo sampling to approximate the posterior of the variables of interest."
  },
  {
    "objectID": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#motivation-of-monte-carlo-sampling",
    "href": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#motivation-of-monte-carlo-sampling",
    "title": "Stochastic gradient and Hamiltonian Monte Carlo",
    "section": "1 Motivation of Monte Carlo sampling",
    "text": "1 Motivation of Monte Carlo sampling\nAccording to (MacKay 2003, chap. 29), Monte Carlo based methods make use of random numbers (or in particular, random variables) to solve one or both of the following problems.\n\n\n\n\n\n\nProblem 1 - generate samples\n\n\n\nGenerate samples \\{\\theta^{(r)}\\}_{r = 1}^{R} from a given probability distribution P(\\theta).\n\n\n\n\n\n\n\n\nProblem 2 - estimate an expected value\n\n\n\nEstimate the expectation of a given function \\ell(\\theta) under a given distribution P(\\theta): \n    \\overline{\\ell} = \\int \\ell(\\theta) \\, P(\\theta) \\, \\operatorname{d}^{N} \\theta,\n where \\theta is assumed to be an N-dimensional vector with real components \\theta_{n}.\n\n\nIt is assumed that P(\\theta) is sufficiently complex that we cannot either (i) sample from it by some conventional techniques, and (ii) evaluate those expectations by exact methods. That motivates us to study Monte Carlo approximation methods.\nMajority of studies in Monte Carlo methods focus on the first problem (sampling) because if we have solved the first problem, then we can solve the second problem by using the Monte Carlo approximation to give an estimation about the expectation: \n    \\hat{\\ell} = \\frac{1}{R} \\sum_{r = 1}^{R} \\ell(\\theta^{(r)}),\n where: \\{\\theta^{(r)}\\}_{r = 1}^{R} are generated from P(\\theta).\nUnder this approximation, \\hat{\\ell} is an un-biased estimator of the exact expectation \\overline{\\ell}.\n\n\n\n\n\n\nWhy is sampling from P(\\theta) hard?\n\n\n\nWe will assume that the density from which we wish to draw samples, P(\\theta), can be evaluated, at least to within a multiplicative constant. In other words, we can evaluate a function P^{*}(\\theta) such that: \n    P(\\theta) = \\frac{P^{*}(\\theta)}{Z},\n\\tag{1} where Z is the normalising constant (that we do not know): \n    Z = \\int P^{*}(\\theta) \\, \\operatorname{d}^{N}\\theta.\n\\tag{2} Thus, it is hard to draw samples from P(\\theta) since Z is often assumed to be unknown. Even if we know Z, drawing samples from P(\\theta) is still challenging problem, especially in high-dimensional spaces because there is no obvious way to sample from P(\\theta) without enumerating all of the possible states.\n\n\nThere are various sampling techniques to generate samples from a given distribution, such as important sampling, rejection sampling or Metropolis - Hastings method. Here, we focus on a specific method, known as Hamiltonian Monte Carlo, which belongs to the family of the Metropolis - Hastings method."
  },
  {
    "objectID": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#the-metropolis---hastings-method",
    "href": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#the-metropolis---hastings-method",
    "title": "Stochastic gradient and Hamiltonian Monte Carlo",
    "section": "2 The Metropolis - Hastings method",
    "text": "2 The Metropolis - Hastings method\nThe Metropolis - Hastings algorithm uses a proposal density Q(\\theta | \\theta^{(t)}) which depends on the current state \\theta^{(t)}. For example, Q(\\theta; \\theta^{(t)}) might be a simple Gaussian distribution centred on the current \\theta^{(t)}. The proposal density Q(\\theta; \\theta^{(t)}) can be any fixed probability distribution from which we can easily sample.\nAs before, it is assumed that the un-normalised probability P^{*}(\\theta) can be evaluated for any \\theta. One can generate the next state \\theta^{\\prime} from the proposal distribution Q(\\theta; \\theta^{(t)}). To decide whether to accept the new state, a quantity (also known as Metropolis - Hastings score) is calculated. Depending on the value of the score, the next state can be (i) accepted, or (ii) accepted with certain probability depending on the value of the score.\n\nIf the step is accepted, then \\theta^{(t + 1)} = \\theta^{\\prime}.\nOtherwise, the previous state is kept: \\theta^{(t + 1)} = \\theta^{(t)}.\n\nThe details of the Metropolis - Hastings algorithm can be seen in Algorithm 1.\n\n\n\\begin{algorithm} \\caption{The Metropolis - Hastings sampling method} \\begin{algorithmic} \\Procedure{Metropolis-Hastings}{$P^{*}(\\theta), Q(\\theta; \\theta^{(t)})$} \\State initialise $\\theta^{(0)}$ \\While{$t = 0, 1, \\dots, T, \\dots, T_{\\mathrm{end}}$} \\State $\\theta^{\\prime} \\gets$ \\Call{sample-from-proposal-distribution}{$Q(\\theta; \\theta^{(t)})$} \\Comment{generate a new state} \\State $a \\gets \\displaystyle \\frac{p^{*}(\\theta^{\\prime})}{p^{*}(\\theta^{(t)})} \\frac{q(\\theta^{(t)}; \\theta^{\\prime})}{q(\\theta^{\\prime}; \\theta^{(t)})}$ \\Comment{calculate Metropolis - Hastings score} \\If{$a \\ge 1$} \\State $\\theta^{(t + 1)} \\gets \\theta^{\\prime}$ \\Comment{accept the new state} \\Else \\State $\\theta^{(t + 1)} \\gets \\theta^{(t)}$ \\Comment{reject the new state} \\EndIf \\EndWhile \\State return $\\{\\theta^{(t)}\\}_{t = T}^{T_{\\mathrm{end}}}$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nDifferent from rejection sampling\n\n\n\nIn rejection sampling, rejected points are discarded and have no influence on the list of samples \\{\\theta^{(r)}\\} that are collected to represent the distribution P(\\theta). In Metropolis - Hastings method, although rejected points are also discarded, the difference is that a rejection causes the current state \\theta^{(t)} to be written again onto the list.\n\n\nConvergence of the Metropolis - Hastings method   It has been shown that for any positive proposal distribution, i.e., Q(\\theta; \\theta^{(t)}) &gt; 0, \\forall \\theta, \\theta^{(t)}, as t\\to+\\infin, the probability distribution of \\theta^{(t)} converges to its true distribution P(\\theta) defined in Equation 1.\n\n\n\n\n\n\nDependency of samples generated from the Metropolis - Hastings method\n\n\n\nThe Metropolis - Hastings method is an example of a Markov chain Monte Carlo method (abbreviated MCMC). In MCMC methods, a Markov process is employed to generate a sequence of states \\{\\theta\\}, where each sample \\theta^{(t)} has a probability distribution depend on the previous state, \\theta^{(t - 1)}. And because successive samples are dependent, the Markov chain may need to be run for a considerable amount of time to effectively generate independent samples from the hidden distribution P(\\theta)."
  },
  {
    "objectID": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#the-hamiltonian-monte-carlo-method",
    "href": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#the-hamiltonian-monte-carlo-method",
    "title": "Stochastic gradient and Hamiltonian Monte Carlo",
    "section": "3 The Hamiltonian Monte Carlo method",
    "text": "3 The Hamiltonian Monte Carlo method\nThe Hamiltonian Monte Carlo method is an instance of the Metropolis - Hastings method that is applicable to continuous domain. It makes use of gradient information to reduce random walk behaviour, potentially resulting in a more efficient MCMC method. In particular, it replaces the proposal distribution Q(\\theta; \\theta^{(t)}) by an implicit distribution in the form of a differential equation.\nSimilar to the Metropolis - Hastings method, we assume that the density P(\\theta) is known up to a normalised constant and written in the form of the potential energy U(\\theta) as follows: \n    P(\\theta) = \\frac{\\exp(-U(\\theta))}{Z}.\n\nThe potential energy, U(\\theta), is defined as: \n\\boxed{\n    U(\\theta) = - \\sum_{x \\in \\mathcal{D}} \\ln p(x | \\theta) - \\ln p(\\theta),\n}\n\\tag{3} where p(x | \\theta) is a likelihood function, and p(\\theta) is the prior distribution of \\theta.\nThe Hamiltonian Monte Carlo method augments the variable of interest, \\theta, by an N_{\\rho}-dimensional momentum variables vector \\rho. A common analogy is that \\theta is the position, while \\rho is the velocity of an object of interest. In that case, the kinetic energy K(\\rho) is defined as follows: \n\\boxed{\n    K(\\rho) = \\frac{1}{2} \\rho^{\\top} M^{-1} \\rho,\n}\n\\tag{4} where M \\in \\mathbb{R}^{N_{\\rho} \\times N_{\\rho}} is symmetric positive definite matrix known as mass matrix.\nThe Hamiltonian dynamics of the whole system can then be defined as: \n    H(\\theta, \\rho) = U(\\theta) + K(\\rho).\n\nOne can then define the joint probability density as: \n    p_{H}(\\theta, \\rho) = \\frac{\\exp(-H(\\theta, \\rho))}{Z_{H}} = \\frac{1}{Z_{H}} \\exp(-U(\\theta)) \\, \\exp(-K(\\rho)).\n\\tag{5}\nSince the probability distribution p_{H} is separable, the marginal distribution of \\theta is the desired distribution p(\\theta) = \\frac{\\exp(-U(\\theta))}{Z}. Thus, simply discarding the momentum variables \\rho would allow to obtain a sequence of samples \\{\\theta^{(t)}\\} that asymptotically come from P(\\theta).\nThe characteristics of a Hamiltonian dynamics can be written as: \n\\begin{dcases}\n    \\frac{\\operatorname{d}\\theta}{\\operatorname{d}t} & = \\frac{\\partial H(\\theta, \\rho)}{\\partial \\rho} = M^{-1} \\rho \\\\\n    & \\\\\n    \\frac{\\operatorname{d}\\rho}{\\operatorname{d}t} & = - \\frac{\\partial H(\\theta, \\rho)}{\\partial \\theta} = -\\nabla_{\\theta} U(\\theta).\n\\end{dcases}\n\\tag{6}\n\n\n\n\n\n\n2D analogy of the Hamiltonian dynamics (Chen, Fox, and Guestrin 2014)\n\n\n\nTo analogise the Hamiltonian dynamics, one can imagine a hockey puck sliding over a frictionless ice surface of varying height. The potential energy is proportional to the height of the surface at the current position, \\theta, of the puck, while the kinectic energy is proportional to the momentum, \\rho, and the mass, M, of the hockey puck.\nIf the surface is flat: \\nabla_{\\theta} U(\\theta) = 0, then the hockey puck will move at a constant speed.\nIf it is going uphill (positive slope: \\nabla_{\\theta} U(\\theta) &gt; 0), the kinetic energy decreases as the potential energy increases util the kinetic reaches 0 (equivalently, \\rho = 0). The hockey puck stops in an instant and begins to slide back down the hill, resulting in increasing the kinectic energy and decreasing the potential energy.\n\n\nEquation 6 defines the transformation of the two variables (\\theta, \\rho) from time t to time t + \\Delta t. This transformation is reversible. Moreover, the Hamiltonian is invariant (or the preservation of the Hamiltonian H(\\theta, \\rho)): \n    \\frac{\\operatorname{d} H}{\\operatorname{d} t} = \\sum_{i = 1}^{N} \\frac{\\operatorname{d} \\theta_{i}}{\\operatorname{d} t} \\frac{\\partial H}{\\partial \\theta_{i}} + \\frac{\\operatorname{d} \\rho_{i}}{\\operatorname{d} t} \\frac{\\partial H}{\\partial \\rho_{i}} = \\sum_{d = 1}^{N} \\frac{\\partial H}{\\partial \\rho_{i}} \\frac{\\partial H}{\\partial \\theta_{i}} -\\frac{\\partial H}{\\partial \\theta_{i}} \\frac{\\partial H}{\\partial \\rho_{i}} = 0.\n\nThis makes any proposal (\\theta, \\rho) obtained from such a perfect simulation always acceptable. If the simulation is imperfect, due to the finite step size when performing the integration for example, then some of the dynamical proposals will be rejected. The rejection rule makes use of the change in H(\\theta, \\rho), which is zero if the simulation is perfect. Please refer to Algorithm 2 for further details of the Hamiltonian Monte Carlo method.\n\n\n\\begin{algorithm} \\caption{Hamiltonian Monte Carlo method} \\begin{algorithmic} \\Procedure{Hamiltonian-MC}{$U(.), M, \\varepsilon$} \\State initialise $\\theta^{(1)}$ \\While{$t = 1, 2, \\dots, T, \\dots, T_{\\mathrm{end}}$} \\State sample momentum: $\\rho^{(t)} \\sim \\mathcal{N}(0, M^{-1})$ \\State evaluate total energy: $H \\gets U(\\theta^{(t)}) + K(\\rho^{(t)})$ \\State $\\theta^{(t, 1)} \\gets \\theta^{(t)}$ \\State $\\rho^{(t, 1)} \\gets \\rho^{(t)}$ \\For{$i = 1, 2, \\dots, \\tau$} \\Comment{Simulate for next state} \\State $\\rho^{(t, i + \\frac{1}{2})} \\gets \\rho^{(t, i)} - \\frac{1}{2} \\varepsilon \\nabla_{\\theta} U(\\theta^{(t, i)})$ \\Comment{make a half-step in $\\rho$} \\State $\\theta^{(t, i + 1)} \\gets \\theta^{(t, i)} + \\varepsilon M^{-1} \\rho^{(t, i + \\frac{1}{2})}$ \\Comment{make a step in $\\theta$} \\State $\\rho^{(t, i + 1)} \\gets \\rho^{(t, i + \\frac{1}{2})} - \\frac{1}{2} \\varepsilon \\nabla_{\\theta} U(\\theta^{(t, i)})$ \\Comment{make another half-step in $\\rho$} \\EndFor \\State $\\theta^{\\prime} \\gets \\theta^{(t, \\tau)}$ \\Comment{new state of $\\theta$} \\State $\\rho^{\\prime} \\gets \\rho^{(t, \\tau)}$ \\Comment{new state of momentum} \\State evaluate total energy with the new state: $H_{\\mathrm{new}} \\gets U(\\theta^{\\prime}) + K(\\rho^{\\prime})$ \\State calculate: $\\operatorname{d}H \\gets H_{\\mathrm{new}} - H$ \\State sample: $u \\sim \\mathrm{uniform}(0, 1)$ \\If{$u &lt; \\exp(-\\operatorname{d}H)$} \\Comment{Metropolis - Hastings step} \\State $\\theta^{(t + 1)} \\gets \\theta^{\\prime}$ \\Comment{accept the new state} \\Else \\State $\\theta^{(t + 1)} \\gets \\theta^{(t)}$ \\Comment{reject the new state} \\EndIf \\EndWhile \\State return $\\{\\theta^{(t)}\\}_{t = T}^{T_{\\mathrm{end}}}$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\nDespite its efficiency, the Hamiltonian Monte Carlo method still requires to run through the entire dataset to perform the integration for \\theta as well as the Metropolis - Hastings step to decide whether to accept or reject the new state generated from the Hamiltonian dynamics. Hence, in the lense of machine learning, it is, however, impractical, especially for large-scaled datasets. It, therefore, motivates further studies and development to make the method practical."
  },
  {
    "objectID": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#stochastic-gradient-hamiltonian-monte-carlo",
    "href": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#stochastic-gradient-hamiltonian-monte-carlo",
    "title": "Stochastic gradient and Hamiltonian Monte Carlo",
    "section": "4 Stochastic gradient Hamiltonian Monte Carlo",
    "text": "4 Stochastic gradient Hamiltonian Monte Carlo\nTo reduce the cost calculating \\nabla_{\\theta} U(\\theta) on the entire dataset \\mathcal{D}, stochastic versions of Hamiltonian Monte Carlo are proposed in (Welling and Teh 2011; Chen, Fox, and Guestrin 2014). In this case, the whole-batch gradient, \\nabla_{\\theta} U(\\theta), is estimated by a noisy estimator, \\nabla_{\\theta} \\tilde{U}(\\theta), which is based on a single mini-batch, \\tilde{\\mathcal{D}}, of data. Such a noisy estimator can be written as follows: \n    \\nabla_{\\theta} \\tilde{U}(\\theta) = - \\frac{|\\mathcal{D}|}{|\\tilde{\\mathcal{D}|}} \\sum_{x \\in \\tilde{\\mathcal{D}}} \\ln p(x | \\theta) - \\ln p(\\theta).\n\\tag{7}\nIf there are many mini-batches, we can apply the Central Limit Theorem to approximate the noisy gradient of the potential energy as follows: \n    \\nabla_{\\theta} \\tilde{U}(\\theta) \\approx \\nabla_{\\theta} U(\\theta) + \\sqrt{V(\\theta)} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I),\n where V(\\theta) is the covariance matrix of the stochastic gradient noise (Welling and Teh 2011, Eq. (6)): \n    V(\\theta) = \\mathbb{E}_{\\text{mini-batch of } x \\in \\tilde{\\mathcal{D}}} \\left[ \\nabla_{\\theta} \\tilde{U}(\\theta) \\, \\nabla_{\\theta}^{\\top} \\tilde{U}(\\theta) \\right] - \\nabla_{\\theta} U(\\theta) \\, \\nabla_{\\theta}^{\\top} U(\\theta),\n and \\sqrt{V}(\\theta) denotes the matrix such that \\sqrt{V(\\theta)} \\left( \\sqrt{V(\\theta)} \\right)^{\\top} = V(\\theta) (e.g., Cholesky decomposition).\n\n4.1 Naive stochastic gradient Hamiltonian Monte Carlo\nA naive way is to directly substitute the noisy estimator in Equation 7 into the Hamiltonian dynamics in Equation 6: \n\\boxed{\n    \\begin{dcases}\n        \\frac{\\operatorname{d} \\theta}{\\operatorname{d} t} & =  M^{-1} \\rho \\\\\n        & \\\\\n        \\frac{\\operatorname{d} \\rho}{\\operatorname{d} t} & = -\\nabla_{\\theta} \\tilde{U}(\\theta) = - \\nabla_{\\theta} U(\\theta) + \\sqrt{V(\\theta)} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I).\n    \\end{dcases}\n}\n\\tag{8}\nIn this case, the Hamiltonian is not guaranteed to be invariant: \n\\begin{aligned}\n    \\frac{\\operatorname{d} H}{\\operatorname{d} t} & = \\sum_{i = 1}^{N} \\frac{\\operatorname{d}\\theta_{i}}{\\operatorname{d} t} \\frac{\\partial H}{\\partial \\theta_{i}} + \\frac{\\operatorname{d} \\rho_{i}}{\\operatorname{d} t} \\frac{\\partial H}{\\partial \\rho_{i}} \\\\\n    & = \\sum_{i = 1}^{N} (M^{-1} \\rho)_{i} \\, \\frac{\\partial U(\\theta)}{\\partial \\theta_{i}} - \\left( \\frac{\\partial U(\\theta)}{\\partial \\theta_{i}} + \\left( \\sqrt{V(\\theta)} \\epsilon \\right)_{i} \\right) \\, (M^{-1} \\rho)_{i}, \\\\\n    & = \\left[ \\sqrt{V(\\theta)} \\epsilon \\right]^{\\top} M^{-1} \\rho.\n\\end{aligned}\n\nWhen using a larger mini-batch size: \\tilde{\\mathcal{D}} \\to \\mathcal{D}, the variance V(\\theta) is smaller: V(\\theta) \\to 0, resulting in \\frac{\\operatorname{d} H}{\\operatorname{d} t} \\to 0. At the limit, the total energy H(\\theta, \\rho) is preserved, which is the full-batch Hamiltonian Monte Carlo mentioned above.\nWhen using a much smaller mini-batch size: |\\tilde{\\mathcal{D}}| \\ll |\\mathcal{D}|, the noise induced by the mini-batch, V(\\theta), is large (e.g., in terms of matrix norm), resulting in \\frac{\\operatorname{d} H}{\\operatorname{d} t} \\neq 0. Consequently, the Hamiltonian is no longer invariant.\nTo correct the error due to the effect of mini-batches, one needs to perform one Metropolis - Hastings step to either reject or accept the new state. Either running a short or long simulation (corresponding to a small or large \\tau in Algorithm Hamiltonian Monte Carlo), the cost of a Metropolis - Hastings step is still extremely large and wasteful if the sample is rejected. One workaround solution is to run a Metropolis - Hastings step on a subset of data instead of the entire dataset (Korattikara, Chen, and Welling 2014; Bardenet, Doucet, and Holmes 2014). There are, of course, some tradeoffs using such approaches.\n\n\n\n\n\n\nHockey puck on ice surface with random wind\n\n\n\nTo continue with the same analogy of a hockey puck, the environment is now different with random wind blowing over the ice surface. That random wind may push the hockey puck further away in some random direction.\n\n\nIndeed, the joint distribution p_{H}(\\theta, \\rho) can be determined to be stationary or not by analysing the corresponding Fokker - Planck equation as shown in the Appendix about the stationary of stochastic gradient due to mini-batches. In this case, p_{H}(\\theta, \\rho) is proved to be non-stationary.\nIn (Chaudhari and Soatto 2018), the joint distribution p_{H}(\\theta, \\rho) \\propto \\exp(-H(\\theta, \\rho)) in Equation 5 is assumed to be stationary under the stochastic dynamics in Equation 8. This is equivalent to proving that the left hand side term in the Fokker - Planck equation is zero: \\frac{\\partial p_{H}(\\theta, \\rho)}{\\partial t} = 0. The authors then analyse and show that the stationary distribution does not converge to the desired posterior distribution in general (Chaudhari and Soatto 2018). This is, however, only true if the stationary distribution exists. And in this case, we prove that it does not (the distribution is non-stationary as shown in Section stationary of stochastic gradient due to mini-batches).\n\n\n4.2 Stochastic gradient Hamiltonian Monte Carlo with “friction”\nOne way to overcome the stochastic estimation for the gradient of the potential energy, \\nabla_{\\theta} \\tilde{U}(\\theta), is to introduce a “friction” term to the momentum update: \n\\begin{dcases}\n    \\frac{\\operatorname{d} \\theta}{\\operatorname{d} t} & =  M^{-1} \\rho \\\\\n    & \\\\\n    \\frac{\\operatorname{d} \\rho}{\\operatorname{d} t} & = - \\nabla_{\\theta} U(\\theta) \\textcolor{Crimson}{- F M^{-1} \\rho} + \\sqrt{V(\\theta)} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I),\n\\end{dcases}\n where: F \\in \\mathbb{R}^{N_{\\rho} \\times N_{\\rho}} denotes friction coefficient matrix. One requirement for F is that: F \\succeq \\sqrt{V} (see the section on stationary SGD with injected noise for further details).\n\n\n\n\n\n\nHockey puck on a friction surface with random wind\n\n\n\nTo continue with the same analogy, the hockey puck is now sliding not on a frictionless ice surface, but a street surface which induces friction from the asphalt. There is still a random wind blowing. However, the friction of the surface prevents the hockey puck from moving too far away than the position it is expected.\n\n\nIn this case, one can prove that the joint distribution p_{H}(\\theta, \\rho) is stationary.\nTo link this sampling to the stochastic gradient descent, one can sample \\rho(t) \\sim \\mathcal{N}(0, M) and apply one leapfrog step as follows: \n\\begin{dcases}\n    \\rho\\left( t + \\frac{1}{2} \\right) & = \\rho(t) + \\frac{\\alpha}{2} \\left[ - \\nabla_{\\theta} U(\\theta) \\textcolor{Crimson}{- F M^{-1} \\rho} + \\sqrt{V(\\theta)} \\epsilon \\right] \\\\\n    \\theta (t + 1) & = \\theta(t) + \\alpha M^{-1} \\rho\\left( t + \\frac{1}{2} \\right).\n\\end{dcases}\n\nIt can be simplified by substituting \\rho(t + \\frac{\\alpha}{2}) into the expression of \\theta to obtain: \n\\boxed{\n    \\theta(t + 1) = \\theta(t) + \\frac{\\alpha^{2}}{2} M^{-1} \\left[ - \\nabla_{\\theta} U(\\theta) \\textcolor{Crimson}{- F M^{-1} \\rho} + \\sqrt{V(\\theta)} \\epsilon \\right] + \\alpha M^{-1} \\rho(t),\n}\n which has a similar form as the Stochastic Gradient Langevin Dynamics (Welling and Teh 2011)."
  },
  {
    "objectID": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#conclusion",
    "href": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#conclusion",
    "title": "Stochastic gradient and Hamiltonian Monte Carlo",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis post reviews some seminar studies in stochastic gradient and Monte Carlo sampling. There have been many successive studies that explored and extended further. Of course, they have mostly developed on top of these studies and achieved better performance. However, it is important to understand the basic before moving to advance. Hopefully, this post would be found useful in one or another way."
  },
  {
    "objectID": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#appendices",
    "href": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#appendices",
    "title": "Stochastic gradient and Hamiltonian Monte Carlo",
    "section": "Appendices",
    "text": "Appendices"
  },
  {
    "objectID": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#fokker---planck-equation",
    "href": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#fokker---planck-equation",
    "title": "Stochastic gradient and Hamiltonian Monte Carlo",
    "section": "6 Fokker - Planck equation",
    "text": "6 Fokker - Planck equation\n\nThe Fokker - Planck equation is used to analyse the evolution of the distribution of the variables in stochastic differential equation: \n    \\operatorname{d} x(t) = - \\nabla f(x) \\operatorname{d} t + \\sqrt{2 \\tau V(x)} \\operatorname{d} W(t),\n\\tag{9} where f(x) is some function (e.g., loss function), V(x) is a diffusion matrix and W(t) is the Brownian motion, and \\tau is a temperature.\n\nLemma 1 The distribution p(x) \\propto \\exp\\left( -H(x) \\right) of the variable x in Equation 9 evolves following the Fokker - Planck equation: \n    \\frac{\\partial p(x)}{\\partial t} = \\nabla \\cdot \\left[ \\nabla f(x) p(x) + \\tau \\nabla \\cdot \\left[ V(x) p(x) \\right] \\right],\n\\tag{10} where: \\nabla \\cdot denotes the divergence, and the divergence operator is applied column-wise to matrices.\n\nThus, one can prove that the distribution of the solution in the stochastic equation Equation 9 is invariant by simply proving that \\partial p(x)/\\partial t = 0."
  },
  {
    "objectID": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#stationary-distribution-of-parameters-obtained-from-sgd",
    "href": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#stationary-distribution-of-parameters-obtained-from-sgd",
    "title": "Stochastic gradient and Hamiltonian Monte Carlo",
    "section": "7 Stationary distribution of parameters obtained from SGD",
    "text": "7 Stationary distribution of parameters obtained from SGD\n\nThe main focus of this section is to investigate the stationary distribution p(\\theta, \\rho) obtained through the stochastic gradient Hamiltonian Monte Carlo. Two types of noises are considered: (i) noise due to mini-batch effect and (ii) injected noise as in (Welling and Teh 2011). The main tool is the Fokker - Planck equation presented in the section about the Fokker - Planck equation. To use the Fokker - Planck equation, the two variables of interest are coupled into a single vector: \n    z = \\begin{bmatrix}\n        \\theta & \\rho\n    \\end{bmatrix}^{\\top}."
  },
  {
    "objectID": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#references",
    "href": "posts/stochastic_grad_hamiltonian_monte_carlo/index.html#references",
    "title": "Stochastic gradient and Hamiltonian Monte Carlo",
    "section": "References",
    "text": "References\n\n\n\nBardenet, Rémi, Arnaud Doucet, and Chris Holmes. 2014. “Towards Scaling up Markov Chain Monte Carlo: An Adaptive Subsampling Approach.” In International Conference on Machine Learning, 405–13. PMLR.\n\n\nChaudhari, Pratik, and Stefano Soatto. 2018. “Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks.” In International Conference on Learning Representations.\n\n\nChen, Tianqi, Emily Fox, and Carlos Guestrin. 2014. “Stochastic Gradient Hamiltonian Monte Carlo.” In International Conference on Machine Learning, 1683–91. PMLR.\n\n\nKorattikara, Anoop, Yutian Chen, and Max Welling. 2014. “Austerity in MCMC Land: Cutting the Metropolis - Hastings Budget.” In International Conference on Machine Learning, 181–89. PMLR.\n\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press.\n\n\nWelling, Max, and Yee W Teh. 2011. “Bayesian Learning via Stochastic Gradient Langevin Dynamics.” In International Conference on Machine Learning, 681–88."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html",
    "href": "posts/vae-normalising-constant-matters/index.html",
    "title": "VAE: normalising constant matters",
    "section": "",
    "text": "Variational auto-encoder (VAE) is one of the most popular generative models in machine learning nowadays. However, the rapid development of the field has made many machine learning practitioners (or, maybe only me) focus too much on deep learning without paying much attention to some fundamentals, such as linear regression. That causes much confusion due to the discrepancy between the derivation and the practical implementation, in which the regularization of the loss, or specifically the Kullback-Leibler (KL) divergence, is weighted by some factor \\beta. I myself did experience and struggle at the beginning of my research. Even though weighting the KL divergence term by a factor $ $ could temporarily resolve the issue, I has been questioning why the balancing between reconstruction and KL divergence is necessary. Eventually, the answer is quite simple: the normalizing constant in the reconstruction loss (or negative log-likelihood) that has been often ignored. This ignorance is the main cause of the imbalance between the two losses."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html#variational-auto-encoder",
    "href": "posts/vae-normalising-constant-matters/index.html#variational-auto-encoder",
    "title": "VAE: normalising constant matters",
    "section": "1 Variational auto-encoder",
    "text": "1 Variational auto-encoder\nGiven data points \\mathbf{x} = \\{x_{i}\\}_{n=1}^{N}, the model of a VAE assumes that there is a corresponding latent variable \\mathbf{z} = \\{ z_{n} \\}_{n=1}^{N} that generates data \\mathbf{x}. In short, the objective function of a VAE is to minimize the variational-free energy (VFE) given as: \n    \\min_{q} \\underbrace{\\mathbb{E}_{q(\\mathbf{z})} \\left[ - \\ln p(\\mathbf{x} | \\mathbf{z}) \\right]}_{\\text{reconstruction loss}} + \\textcolor{red}{\\beta} \\mathrm{KL} \\left[ q(\\mathbf{z}) \\Vert p(\\mathbf{x}) \\right], \\tag{vfe}\n where q(\\mathbf{z}) is the variational distribution of the latent variable, and \\textcolor{red}{\\beta} = 1 is the weighting factor.\nIn practice, people often “specify” the reconstruction loss as mean squared error (MSE) or binary cross-entropy loss and use gradient descent to minimize VFE. With \\beta = 1 as in (vfe), the reconstruction of different images seem to be the same image (see Figure 1 (top)), whereas setting $ $ results in much better reconstructed images (see Figure 1 (bottom)).\n\n \n\nFigure 1. The reconstructed images from VAE with β = 1 (top) and β ≪ 1 (bottom). Source: stats.stackexchange.com\n\n\nThis does not make me satisfied, although some justifications for setting \\beta to some small value are made. For example: - Setting \\beta \\ll 1 leads to even a “further lower-bound”. Hence, maximizing this “further lower-bound” is still mathematically reasonable. However, this bound is very loose. Can we do something better? - One can cast the problem to a constrained optimization as in β-VAE paper. However, β in that case is the Lagrange multiplier, and should be obtained through the optimization. Is it mathematically correct if considering β as a hyper-parameter? I doubt that.\nLater on, I figure out that the main reason of the imbalance between the two losses is due to the “specification” of the reconstruction loss. Simply specifying the type of the loss -\\ln p(\\mathbf{x} \\vert \\mathbf{z}) as MSE or binary cross-entropy would ignore the normalizing constant, resulting in an incorrect reconstruction loss. The correct way is to specify the modeling assumption of the likelihood p(\\mathbf{x} \\vert \\mathbf{z}), which, in the case of VAE, goes back to linear regression.\nIn the following sections, f(\\mathbf{z}; \\theta) denotes the output of the decoder parameterized by a neural network with weight \\theta. Usually, f(\\mathbf{z}; \\theta) is assumed to be the reconstructed images, but this might not always true depending on the assumption used."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html#reconstruction-likelihood-with-gaussian-assumption",
    "href": "posts/vae-normalising-constant-matters/index.html#reconstruction-likelihood-with-gaussian-assumption",
    "title": "VAE: normalising constant matters",
    "section": "2 Reconstruction likelihood with Gaussian assumption",
    "text": "2 Reconstruction likelihood with Gaussian assumption\nThis corresponds to linear regression with Gaussian noise assumption.\nThe variable of interest \\mathbf{x} is assumed to be a deterministic function f(\\mathbf{z}; \\theta) with additional Gaussian noise, so that: \n    \\mathbf{x} = f(\\mathbf{z}; \\theta) + \\epsilon,\n where: \\epsilon \\sim \\mathcal{N}\\left( \\epsilon; 0, \\Lambda^{-1} \\right). Thus, the reconstruction likelihood can be written as: \n    p(\\mathbf{x} \\vert \\mathbf{z}, \\theta, \\Lambda) = \\mathcal{N}(\\mathbf{x}; f(\\mathbf{z}; \\theta), \\Lambda^{-1}) = \\prod_{n=1}^{N} \\mathcal{N}(x_{n}; f(z_{n}; \\theta), \\Lambda^{-1}).\n Hence, the negative log-likelihood, or the reconstruction loss in the VAE, can be expressed as: \n    -\\ln p(\\mathbf{x} \\vert \\mathbf{z}, \\theta, \\Lambda) = - \\frac{N}{2} \\ln \\frac{\\Lambda}{2 \\pi} + \\Lambda \\times \\frac{1}{2} \\underbrace{\\sum_{n=1}^{N} \\left[ x_{n} - f(z_{n}; \\theta) \\right]^{2}}_{N \\times \\text{MSE}}. \\tag{nll-G}\n\n\nNote that current practice uses only MSE, which ignores the first term and the scaling factor relating to the noise precision \\Lambda.\n\nUnder this modeling approach, the decoder would consist of 2 networks: one for mean \\bar{x} = f(z; \\theta) and the other for noise precision \\Lambda = g(z; \\phi). Of course, one can consider \\Lambda as a hyper-parameter to simplify further the implementation.\nThe “full” loss function of a VAE is, therefore, presented as: \n    \\boxed{\n    \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\frac{N}{2} \\ln(2\\pi) - \\frac{N}{2} \\ln \\Lambda + \\frac{\\Lambda}{2} \\sum_{n=1}^{N} \\left[ x_{n} - f(z_{n}; \\theta) \\right]^{2} \\right] + \\mathrm{KL} \\left[ q(\\mathbf{z}) \\Vert p(\\mathbf{x}) \\right]. \\tag{vfe-G}\n    }\n\nAfter training, one can pass an image to the encoder h(.; \\phi) and decoder to get the predicted mean and precision. The reconstructed images can then be obtained as: \n    \\hat{x} \\sim \\mathcal{N}(x; f(z; \\theta), \\Lambda), \\text{where } z = h(x; \\phi).\n Although this approach is easy to understand, one drawback is the unbounded support of the Gaussian distribution, resulting in reconstructed pixel intensity values out of the desired range [0, 1]. Consequently, when visualizing, the pixels that are out of that range will be truncated to 0 or 1, potentially making the reconstructed images blurrier."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html#reconstruction-likelihood-with-continuous-bernoulli-assumption",
    "href": "posts/vae-normalising-constant-matters/index.html#reconstruction-likelihood-with-continuous-bernoulli-assumption",
    "title": "VAE: normalising constant matters",
    "section": "3 Reconstruction likelihood with continuous Bernoulli assumption",
    "text": "3 Reconstruction likelihood with continuous Bernoulli assumption\nThis corresponding to linear regression in [0, 1] (not $ {0, 1 } $ as in logistic regression), and hence, the words “continuous Bernoulli”.\nThis modeling approach is not as intuitive as the one with Gaussian assumption, but please bear with me for a moment.\nThe likelihood of interest, p(\\mathbf{x} \\vert \\mathbf{z}), is assumed to be a continuous Bernoulli distribution: \n    p(\\mathbf{x} \\vert \\mathbf{z}) = \\mathcal{CB}(\\mathbf{x}; f(\\mathbf{z}; \\theta)) = \\prod_{n=1}^{N} \\underbrace{C \\left( f(z_{n}; \\theta) \\right)}_{\\text{normalizing const.}}  \\underbrace{\\left[ f(z_{n}; \\theta) \\right]^{x_{n}} \\left[ 1 - f(z_{n}; \\theta) \\right]^{1 - x_{n}}}_{\\text{Bernoulli pdf}},\n and $f(z_{n}; )) , n {1, , N } $.\nNote that: - the usage of continuous Bernoulli distribution is due to the fact that VAE tries to regress the pixel intensity x_{n} which falls in [0, 1], not $ {0, 1 } $ as in classification, - the pdf of a continuous Bernoulli distribution differs from a Bernoulli distribution at the normalizing constant term, - the output of the decoder now is not the mean of the reconstructed pixel intensity as in the case of Gaussian distribution, - due to the assumption of the continuous Bernoulli distribution, the last layer of the decoder must be activated by sigmoid function to ensure the output falling in $[0, 1] $.\nThe negative log-likelihood, or reconstruction loss, can be easily derived as: \n    - \\ln p(\\mathbf{x} \\vert \\mathbf{z}) = \\sum_{n=1}^{N} \\underbrace{ - \\left[ x_{n} \\ln f(z_{n}; \\theta) + (1 - x_{n}) \\ln \\left[1 - f(z_{n}; \\theta) \\right] \\right]}_{\\text{binary cross-entropy}} - \\underbrace{\\ln C \\left( f(z_{n}; \\theta) \\right)}_{\\text{log normalizing const.}}. \\tag{nll-CB}\n\n\nCurrent practice uses binary cross-entropy loss only, corresponding to Bernoulli distribution. To me, that practice is not correct, since the learning is to infer the parameter of the Bernoulli distribution, which is the probability when the outcome is 1. In that case, the pixel intensity is in $ {0, 1 } $, not $[0, 1] $. This explains why VAE using binary cross-entropy loss often works well for gray-scale, but not color, images.\n\nSubstituting (nll-CB) into (vfe) gives the “full” objective function for VAE: \n    \\boxed{\n        \\begin{aligned}\n        & - \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\sum_{n=1}^{N} x_{n} \\ln f(z_{n}; \\theta) + (1 - x_{n}) \\ln \\left[1 - f(z_{n}; \\theta) \\right] \\right. \\\\\n        & \\quad \\left. + \\ln C \\left( f(z_{n}; \\theta) \\right) \\right] + \\mathrm{KL} \\left[ q(\\mathbf{z}) \\Vert p(\\mathbf{x}) \\right].\n        \\end{aligned}\n        \\tag{vfe-CB}\n    }\n\nNote that after training, direct plotting f(z; \\theta) as the pixel intensity might result in an incorrect reconstructed image, since the mean of the continuous Bernoulli distribution is not equal to its parameter. To reconstruct an image x, one needs to pass that image through the encoder and decoder, and then: \n    \\hat{x} \\sim \\mathcal{CB}\\left(x; f(z; \\theta) \\right),\n and plot \\hat{x} to visualize the reconstructed image."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html#conclusion",
    "href": "posts/vae-normalising-constant-matters/index.html#conclusion",
    "title": "VAE: normalising constant matters",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nVAE is often considered as a basic generative model. However, most machine learning practitioners often learn by memorization about the “type” of reconstruction loss. This leads to the weighting trick in the implementation. Understanding the nature of the reconstruction loss as the log-likelihood in linear regression allows one to obtain the “full” objective function without applying any weighting tricks. Hopefully, this post would be useful to save time for ones who start to practise machine learning."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html#references",
    "href": "posts/vae-normalising-constant-matters/index.html#references",
    "title": "VAE: normalising constant matters",
    "section": "5 References",
    "text": "5 References\n\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S. and Lerchner, A., 2016. β-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representation.\nLoaiza-Ganem, G. and Cunningham, J.P., 2019. The continuous Bernoulli: fixing a pervasive error in variational autoencoders. In Advances in Neural Information Processing Systems (pp. 13287-13297)."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Stochastic gradient and Hamiltonian Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nCuong Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nExpectation - Maximisation algorithm and its applications in finite mixture models\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2022\n\n\nCuong Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nBias - variance decomposition\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2022\n\n\nCuong Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nFrom hyper-parameter optimisation to meta-learning\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2021\n\n\nCuong Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nOuter product approximation of Hessian matrix\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2021\n\n\nCuong Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nPAC-Bayes bounds for generalisation error\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2020\n\n\nCuong Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nVAE: normalising constant matters\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2020\n\n\nCuong Nguyen\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]