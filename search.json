[
  {
    "objectID": "posts/meta-learning/index.html",
    "href": "posts/meta-learning/index.html",
    "title": "From hyper-parameter optimisation to meta-learning",
    "section": "",
    "text": "Meta-learning, also known as learn-how-to-learning, has been being studied from 1980s (Schmidhuber 1987; Naik and Mammone 1992), and recently attracted much attention from the research community. Meta-learning is a technique in transfer learning — a learning paradigm that utilises knowledge gained from past experience to facilitate the learning in the future. Due to being defined implicitly, meta -learning is often confused with other transfer learning techniques, e.g. fine-tuning, multi-task learning, domain adaptation and continual learning. The purpose of this post is to formulate meta-learning explicitly via empirical Bayes, and in particular hyper-parameter optimisation, to differentiate meta-learning from those common transfer learning approaches.\nThis post is structured as follows: First, we define some terminologies used in general transfer learning and review hyper-parameter optimisation in single-task setting. We then formulate meta-learning as an extension of hyper-parameter optimisation in multi-task setting. Finally, we show the differences between meta-learning and other transfer-learning approaches."
  },
  {
    "objectID": "posts/meta-learning/index.html#background",
    "href": "posts/meta-learning/index.html#background",
    "title": "From hyper-parameter optimisation to meta-learning",
    "section": "1 Background",
    "text": "1 Background\n\n1.1 Data generation model of a task\nA data point of a task indexed by i \\in \\mathbb{N} consists of an input \\mathbf{x}_{ij} \\in \\mathcal{X} \\subseteq \\mathbb{R}^{d} and a corresponding label \\mathbf{y}_{ij} \\in \\mathcal{Y} with j \\in \\mathbb{N}. For simplicity, only two families of tasks – regression and classification – are considered in this thesis. As a result, the label is defined as \\mathcal{Y} \\subseteq \\mathbb{R} for regression and as \\mathcal{Y} = \\{0, 1, \\ldots, C - 1\\} for classification, where C is the number of classes.\nEach data point in a task can be generated in 2 steps:\n\ngenerate the input \\mathbf{x}_{ij} by sampling from some probability distribution \\mathcal{D}_{i},\ndetermine the label \\mathbf{y}_{ij} = f(\\mathbf{x}_{ij}), where f_{i}: \\mathcal{X} \\to \\mathcal{Y} is the correct labelling function.\n\nBoth the probability distribution \\mathcal{D}_{i} and the labelling function f_{i} are unknown to the learning agent during training, and the aim of the supervised learning is to use the generated data to infer such labelling function f.\nFor simplicity, we denote (\\mathbf{x}_{ij}, \\mathbf{y}_{ij}) \\sim (\\mathcal{D}_{i}, f_{i}) as the data generation model of task i-th.\n\n\n1.2 Task instance\n\nDefinition 1 (Hospedales et al. 2021)\nA task or a task instance \\mathcal{T}_{i} consists of an unknown associated data generation model (\\mathcal{D}_{i}, f_{i}), and a loss function \\ell_{i}, denoted as: \n\\mathcal{T}_{i} = \\{(\\mathcal{D}_{i}, f_{i}), \\ell_{i}\\}.\n\n\n\nRemark. The loss function \\ell_{i} is defined abstractly, and can be either:\n\nnegative log-likelihood (NLL): - \\ln p(y_{ij} | \\mathbf{x}_{ij}, \\mathbf{w}_{i}), corresponding to maximum likelihood estimation. This type of loss is quite common in practice, for example:\n\nmean squared error (MSE) in regression\ncross-entropy in classification\n\nvariational-free energy (negative evidence lower-bound) — corresponding to the objective function in variational inference.\n\n\nTo solve a task \\mathcal{T}_{i}, one needs to obtain an optimal task-specific model {h(.; \\mathbf{w}_{i}^{*}): \\mathcal{X} \\to \\mathcal{Y}}, parameterised by \\mathbf{w}^{*}_{i} \\in \\mathcal{W} \\subseteq \\mathbb{R}^{n}, which minimises a loss function \\ell_{i} on the data of that task: \n\\mathbf{w}_{i}^{*} = \\arg\\min_{\\mathbf{w}_{i}} \\mathbb{E}_{(\\mathbf{x}_{ij}, \\mathbf{y}_{ij}) \\sim (\\mathcal{D}_{i}, f_{i})} \\left[ \\ell_{i} (\\mathbf{x}_{ij}, \\mathbf{y}_{ij}; \\mathbf{w}_{i}) \\right].\n\nIn practice, since both \\mathcal{D}_{i} and f_{i} are unknown, the data generation model is replaced by a dataset consisting of a finite number of data-points generated according to the data generation model (\\mathcal{D}_{i}, f_{i}), denoted as S_{i} = \\{\\mathbf{x}_{ij}, \\mathbf{y}_{ij}\\}_{j=1}^{m_{i}}. The objective to solve that task is often known as empirical risk minimisation: \n\\mathbf{w}^{\\mathrm{ERM}}_{i} = \\arg\\min_{\\mathbf{w}_{i}} \\frac{1}{m_{i}} \\sum_{j = 1}^{m_{i}} \\left[ \\ell_{i} (\\mathbf{x}_{ij}, \\mathbf{y}_{ij}; \\mathbf{w}_{i}) \\right].\n\\tag{1}\nSince the loss function used is the same for each task family, e.g. \\ell is NLL or variational-free energy, the subscript on the loss function is, therefore, dropped, and the loss is denoted as \\ell throughout this chapter. Furthermore, given the commonality of the loss function across all tasks, a task can, therefore, be simply represented by either its data generation model (\\mathcal{D}_{i}, f_{i}) or the associated dataset S_{i}.\n\n\n1.3 Hyper-parameter optimisation\nIn single-task setting, the common way to tune or optimise a hyper-parameter is to split a given dataset S_{i} into two disjoint subsets: \n\\begin{aligned}\nS_{i}^{(t)} \\cup S_{i}^{(v)} & = S_{i}\\\\\nS_{i}^{(t)} \\cap S_{i}^{(v)} & = \\varnothing,\n\\end{aligned}\n where:\n\nS_{i}^{(t)} = \\left\\{ \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)} \\right) \\right\\}_{j=1}^{m_{i}^{(t)}} is the training (or support) subset,\nS_{i}^{(v)} = \\left\\{ \\left( \\mathbf{x}_{ij}^{(v)}, y_{ij}^{(v)} \\right) \\right\\}_{j=1}^{m_{i}^{(v)}} is the validation (or query) subset.\n\nNote that with this definition, m_{i}^{(t)} + m_{i}^{(v)} = m_{i}, and m_{i}^{(t)} and m_{i}^{(v)} are not necessarily identical.\nThe subset S_{i}^{(t)} is used to train the model parameter of interest \\mathbf{w}_{i}, while the subset S_{i}^{(v)} is used to validate the hyper-parameter, denoted by \\theta (we provide examples of the hyper-parameter in Section Formulation of meta-learning). Mathematically, hyper-parameter optimisation in the single-task setting can be written as the following bi-level optimisation: \n\\begin{aligned}\n& \\min_{\\theta} \\frac{1}{m_{i}^{(v)}} \\sum_{k = 1}^{m_{i}^{(v)}}  \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*} (\\theta) \\right)\\\\\n& \\text{s.t.: } \\mathbf{w}_{i}^{*} (\\theta) = \\arg\\min_{\\mathbf{w}_{i}} \\frac{1}{m_{i}^{(t)}} \\sum_{j = 1}^{m_{i}^{(t)}}  \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\mathbf{w}_{i} (\\theta) \\right).\n\\end{aligned}\n\nWe can extend the hyper-parameter optimisation from the two data subsets S_{i}^{(t)} and S_{i}^{(v)} to the general data generation model as the following: \n\\begin{aligned}\n& \\min_{\\theta} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[  \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*} (\\theta) \\right) \\right]\\\\\n& \\text{s.t.: } \\mathbf{w}_{i}^{*} (\\theta) = \\arg\\min_{\\mathbf{w}_{i}} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(t)}, y_{ik}^{(t)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(t)}, f_{i} \\right)} \\left[  \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\mathbf{w}_{i} (\\theta) \\right) \\right],\n\\end{aligned}\n where \\mathcal{D}_{i}^{(t)} and \\mathcal{D}_{i}^{(v)} are the probability distributions of training and validation input data, respectively, and they are not necessarily identical.\nFormulation of meta-learning\nThe setting of the meta-learning problem considered in this paper follows the task environment (Baxter 2000) that describes the unknown distribution p(\\mathcal{D}, f) over a family of tasks. Each task \\mathcal{T}_{i} is sampled from this task environment and can be represented as \\left( \\mathcal{D}_{i}^{(t)}, \\mathcal{D}_{i}^{(v)}, f_{i} \\right), where \\mathcal{D}_{i}^{(t)} and \\mathcal{D}_{i}^{(v)} are the probability of training and validation input data, respectively, and are not necessarily identical. The aim of meta-learning is to use T training tasks to train a meta-learning model that can be fine-tuned to perform well on an unseen task sampled from the same task environment.\nSuch meta-learning methods use meta-parameters to model the common latent structure of the task distribution p(\\mathcal{D}, f). In this thesis, we consider meta-learning as an extension of hyper-parameter optimisation in single-task learning, where the hyper-parameter of interest — often called meta-parameter — is shared across many tasks. Similar to hyper-parameter optimisation presented in subsection hyper-parameter-optimisation, the objective of meta-learning is also a bi-level optimisation: \n\\begin{aligned}\n& \\min_{\\theta} \\textcolor{red}{\\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f_{i} \\right)}} \\mathbb{E}_{ \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right]\\\\\n& \\text{s.t.: } \\mathbf{w}^{*}_{i}(\\theta) = \\arg\\min_{\\mathbf{w}_{i}} \\mathbb{E}_{\\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(t)}, f_{i} \\right)} \\left[ \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\mathbf{w}_{i}(\\theta) \\right) \\right].\n\\end{aligned}\n\\tag{2}\nThe difference between meta-learning and hyper-parameter optimisation is that the meta-parameter (also known as hyper-parameter) \\theta is shared across all tasks sampled from the task environment p(\\mathcal{D}, f) as highlighted in red colour in Equation 2.\nIn practice, the meta-parameter (or shared hyper-parameter) \\theta can be chosen as one of the followings:\n\nlearning rate of gradient-based optimisation used to minimise the lower level objective function in Equation 2 to learn \\mathbf{w}_{i}^{*} \\left(\\theta\\right) (Z. Li et al. 2017),\ninitialisation of model parameter (Finn, Abbeel, and Levine 2017),\ndata representation or feature extractor (Vinyals et al. 2016; Snell, Swersky, and Zemel 2017),\noptimiser used to optimise the lower-level in Equation 2.\n\nIn this post, the meta-parameter \\theta is assumed to be the initialisation of model parameters. Formulation, derivation and analysis in the subsequent sections and chapters will, therefore, revolve around this assumption. Note that the analysis can be straight-forwardly extended to other types of meta-parameters with slight modifications.\nIn general, the objective function of meta-learning in Equation 2 can be solved by gradient-based optimisation, such as gradient descent. Due to the nature of the bi-level optimisation, the optimisation are often carried out in two steps. The first step is to adapt (or fine-tuned) the meta-parameter \\theta to the task-specific parameter \\mathbf{w}_{i}(\\theta). This corresponds to the optimisation in the lower-level, and can be written as: \n\\mathbf{w}_{i}^{*}(\\theta) = \\theta - \\alpha \\mathbb{E}_{\\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(t)}, f_{i} \\right)} \\left[ \\nabla_{\\theta} \\ell \\left( \\mathbf{x}_{ij}^{(t)}, \\mathbf{y}_{ij}^{(t)}; \\mathbf{w}_{i}(\\theta) \\right) \\right],\n\\tag{3} where \\alpha is a hyper-parameter denoting the learning rate for task \\mathcal{T}_{i}. For simplicity, the adaptation step in Equation 3} is carried out with only one gradient descent update.\nThe second step is to minimise the validation loss induced by the locally-optimal task-specific parameter \\mathbf{w}_{i}^{*}(\\theta) evaluated on the validation subset w.r.t. the meta-parameter \\theta. This corresponds to the upper-level optimisation, and can be expressed as: \n\\theta \\gets \\theta - \\gamma \\mathbb{E}_{\\mathcal{T}_{i} \\sim p(\\mathcal{D}, f)} \\mathbb{E}_{ \\left( \\mathbf{x}_{ik}^{(v)}, \\mathbf{y}_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\nabla_{\\theta} \\ell \\left( \\mathbf{x}_{ij}^{(v)}, \\mathbf{y}_{ij}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right],\n\\tag{4} where \\gamma is another hyper-parameter representing the learning rate to learn \\theta.\nThe general algorithm of meta-learning using gradient-based optimisation is shown in Algorithm 1.\n\n\\begin{algorithm}\n\\caption{Training procedure of meta-learning in general}\n\\begin{algorithmic}\n\\PROCEDURE{Training}{task environment $p(\\mathcal{D}, f)$, learning rates $\\gamma$ and $\\alpha$}\n\\STATE initialise meta-parameter $\\theta$\n\\WHILE{$\\theta$ not converged}\n\\STATE sample a mini-batch of $T$ tasks from task environment $p\\left( \\mathcal{D}, f \\right)$\n\\FOR{each task $\\mathcal{T}_{i}, i \\in \\{1, \\ldots, T\\}$}\n\\STATE sample two data subsets $S_{i}^{(t)}$ and $S_{i}^{(v)}$ from task $\\mathcal{T}_{i} = (\\mathcal{D}_{i}^{(t)}, \\mathcal{D}_{i}^{(v)}, f_{i})$\n\\STATE adapt meta-parameter to task $\\mathcal{T}_{i}$: $\\mathbf{w}_{i}^{*} \\left( \\theta \\right) = \\theta - \\frac{\\alpha}{m_{i}^{(t)}} \\sum_{j = 1}^{m_{i}^{(t)}} \\nabla_{\\theta} \\left[ \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)};  \\theta \\right)\\right]$ \\COMMENT{Eq. @eq-task_adaptation_sgd}}\n\\ENDFOR\n\\STATE update meta-parameter: $\\theta \\gets \\theta - \\frac{\\gamma}{T} \\sum_{i=1}^{T} \\frac{1}{m_{i}^{(v)}} \\sum_{k=1}^{m_{i}^{(v)}} \\nabla_{\\theta} \\left[\\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*} \\left( \\theta \\right) \\right) \\right]$ \\COMMENT{Eq. @eq-meta_parameter_update_sgd}}\n\\ENDWHILE\n\\STATE \\textbf{return} the trained meta-parameter $\\theta$\n\\ENDPROCEDURE\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n1.4 Second-order meta-learning\nAs shown in Equation 4, the optimisation for the meta-parameter \\theta requires the gradient of the validation loss averaged across T tasks. Given that each task-specific parameter \\mathbf{w}_{i}^{*} is a function of \\theta due to the lower-level optimisation in Equation 3, the gradient of interest can be expanded as: \n\\begin{aligned}\n& \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\nabla_{\\theta} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right]\\\\\n& = \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\nabla_{\\theta}^{\\top} \\mathbf{w}_{i}^{*} \\left( \\theta \\right) \\times \\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right]\\\\\n& = \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\left\\{ \\left[ \\mathbf{I} - \\alpha \\mathbb{E}_{ \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(t)}, f_{i} \\right)} \\left[ \\textcolor{red}{\\nabla_{\\theta}^{2} \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\theta \\right)} \\right] \\right] \\right.\\\\\n& \\quad \\times \\left. \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\textcolor{green}{\\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right)} \\right] \\right\\},\n\\end{aligned}\n where the first equality is due to chain rule, and the second equality is the result that differentiates the gradient update in Equation 3. Note that in the second equality, we remove the transpose notation since the corresponding matrix is symmetric.\nThus, naively implementing such gradient would require to calculate the Hessian matrix \\textcolor{Red}{\\nabla_{\\theta}^{2} \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\theta \\right)}, resulting in an intractable procedure for large models, such as deep neural networks. To obtain a more efficient implementation, one can utilise the Hessian-vector product (Pearlmutter 1994) between the gradient vector \\textcolor{green}{\\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, \\mathbf{y}_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right)} and the Hessian matrix \\textcolor{red}{\\nabla_{\\theta}^{2} \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\theta \\right)} to efficiently calculate the gradient of the validation loss w.r.t. \\theta.\nAnother way to calculate the gradient of the validation loss w.r.t. the meta-parameter \\theta is to use implicit differentiation (Domke 2012; Rajeswaran et al. 2019; Lorraine, Vicol, and Duvenaud 2020). This approach is more advantaged since it does not need to stores the computational graph and takes gradient via chain rule. Such implicit differentiation technique reduces the memory usage and therefore, allows to work with large-scale models. However, the trade-off is the increasing computational time to apply the chain rule to calculate the gradient of interest.\nNevertheless, the implementations that compute the exact gradient of the validation loss w.r.t. \\theta without approximation are often referred to as second-order meta-learning.\n\n\n1.5 First-order meta-learning\nIn practice, the Hessian matrix \\textcolor{Red}{\\nabla_{\\theta}^{2} \\ell \\left( \\mathbf{x}_{ij}^{(t)}, y_{ij}^{(t)}; \\theta \\right)} is often omitted from the calculation to simplify the update for the meta-parameter \\theta (Finn, Abbeel, and Levine 2017). The resulting gradient consists of only the gradient of validation loss \\textcolor{Green}{\\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ij}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right)}, which is more efficient to calculate with a single forward-pass if auto differentiation is used. This approximation is often referred as first-order meta-learning, and the gradient of interest can be presented as: \n\\begin{aligned}\n& \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left(\\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\nabla_{\\theta} \\ell \\left( \\mathbf{x}_{ij}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right] \\\\\n& \\approx \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\textcolor{Green}{\\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ij}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right)} \\right].\n\\end{aligned}\n\nREPTILE [Nichol, Achiam, and Schulman (2018)} — a variant first-order meta-learning — approximates further the gradient of validation loss \\textcolor{Green}{\\nabla_{\\mathbf{w}_{i}^{*}(\\theta)} \\ell \\left( \\mathbf{x}_{ij}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right)} by the difference \\theta - \\mathbf{w}_{i}^{*}, resulting in a much simpler approximation: \n\\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\mathbb{E}_{\\left( \\mathbf{x}_{ik}^{(v)}, \\mathbf{y}_{ik}^{(v)} \\right) \\sim \\left( \\mathcal{D}_{i}^{(v)}, f_{i} \\right)} \\left[ \\nabla_{\\theta} \\ell \\left( \\mathbf{x}_{ik}^{(v)}, y_{ik}^{(v)}; \\mathbf{w}_{i}^{*}(\\theta) \\right) \\right] = \\theta - \\mathbb{E}_{\\mathcal{T}_{i} \\sim p \\left( \\mathcal{D}, f \\right)} \\left[ \\mathbf{w}_{i}^{*}(\\theta) \\right]."
  },
  {
    "objectID": "posts/meta-learning/index.html#differentiation-from-other-transfer-learning-approaches",
    "href": "posts/meta-learning/index.html#differentiation-from-other-transfer-learning-approaches",
    "title": "From hyper-parameter optimisation to meta-learning",
    "section": "2 Differentiation from other transfer learning approaches",
    "text": "2 Differentiation from other transfer learning approaches\nIn this section, some popular transfer learning methods are described with their objective functions to purposely distinguish from meta-learning.\n\n2.1 Fine-tuning\nFine-tuning is the most common technique in neural network based transfer learning (Pratt et al. 1991; Yosinski et al. 2014) where the last or a couple of last layers in a neural network pre-trained on a source task are replaced and fine-tuned on a target task. Formally, if g(.; \\mathbf{w}_{0}) is denoted as the forward function of the shared layers with shared parameters \\mathbf{w}_{0}, where \\mathbf{w}_{s} and \\mathbf{w}_{t} are the parameters of the remaining layers h specifically trained on source and target tasks, respectively, then the objective of fine-tuning can be expressed as: \n\\begin{aligned}\n& \\min_{\\mathbf{w}_{t}} \\mathbb{E}_{(\\mathbf{x}_{t}, \\mathbf{y}_{t}) \\sim \\mathcal{T}_{t}} \\left[ \\ell \\left( h\\left( g\\left( \\mathbf{x}_{t}; \\mathbf{w}_{0}^{*} \\right); \\mathbf{w}_{t} \\right), \\mathbf{y}_{t} \\right) \\right] \\\\\n& \\text{s.t.: } \\mathbf{w}_{0}^{*}, \\mathbf{w}_{s}^{*} = \\arg\\min_{\\mathbf{w}_{0}, \\mathbf{w}_{s}} \\mathbb{E}_{(\\mathbf{x}_{s}, \\mathbf{y}_{s}) \\sim \\mathcal{T}_{s}} \\left[ \\ell \\left( h \\left( g\\left( \\mathbf{x}_{s}; \\mathbf{w}_{0} \\right); \\mathbf{w}_{s} \\right), \\mathbf{y}_{s} \\right) \\right],\n\\end{aligned}\n\\tag{5}\nwhere \\mathbf{x}_{s}, \\mathbf{y}_{s} and \\mathbf{x}_{t}, \\mathbf{y}_{t} are the data sampled from the source task \\mathcal{T}_{s} and target task \\mathcal{T}_{t}, respectively.\nAlthough the objective of fine-tuning shown in Equation 5 is still a bi-level optimisation, it is easier to solve than the one in meta-learning due to the following reasons:\n\nThe objective in fine-tuning has only one constrain corresponding to one source task, while meta-learning has several constrains corresponding to multiple training tasks.\nIn fine-tuning, \\mathbf{w}_{t} and \\mathbf{w}_{0} are inferred separately, while in meta-learning, the task-specific parameter is a function of the meta-parameter, resulting in a more complicated correlation.\n\nThe downside of fine-tuning is the requirement of a reasonable number of training examples on the target task to fine-tune \\mathbf{w}_{t}. In contrast, meta-learning leverages the knowledge extracted from several training tasks to quickly adapt to a new task with only a few training examples.\n\n\n2.2 Domain adaptation and generalisation\nDomain adaptation or domain-shift refers to the case when the joint data-label distribution on source and target are different, denoted as p_{s} \\left( \\mathcal{D}, f \\right) \\neq p_{t} \\left( \\mathcal{D}, f \\right), or simply p_{s}(\\mathbf{x}, \\mathbf{y}) \\neq p_{t}(\\mathbf{x}, \\mathbf{y})~(Heckman 1979; Shimodaira 2000; Japkowicz and Stephen 2002; Daume III and Marcu 2006; Ben-David et al. 2007). The aim of domain adaptation is to leverage the model trained on source domain to available data in the target domain, so that the model adapted to the target domain can perform reasonably well. In other words, domain adaptation relies on a data transformation g(., .; \\mathbf{w}_{0}): \\mathcal{X} \\times \\mathcal{Y} \\to \\mathcal{X}^{\\prime} \\times \\mathcal{Y}^{\\prime} that produces a domain-invariant latent space. Mathematically, the transformation g is obtained by minimising a divergence between the two transformed data distribution: \n\\begin{aligned}\n& \\min_{\\mathbf{w}_{0}} \\mathrm{Divergence} \\left[ p\\left( \\mathbf{x}_{s}^{\\prime}, \\mathbf{y}_{s}^{\\prime} \\right) || p\\left( \\mathbf{x}_{t}^{\\prime}, \\mathbf{y}_{t}^{\\prime} \\right) \\right]\\\\\n& \\text{s.t.: } \\left( \\mathbf{x}_{i}^{\\prime}, \\mathbf{y}_{i}^{\\prime} \\right) = g \\left( \\mathbf{x}_{i}, \\mathbf{y}_{i}; \\mathbf{w}_{0} \\right), i \\in \\{s, t\\}.\n\\end{aligned}\n\\tag{6}\nAfter obtaining the transformation g, one can simply train a model using the transformed data of the source domain, and then use that model to make predictions on the target domain.\nGiven the optimisation in~Equation 6, domain adaptation is different from meta-learning due to the following reasons:\n\nDomain adaptation assumes a shift in the task environments that generate source and target tasks, while meta-learning is based on the assumption of same task generation.\nDomain adaptation utilises information of data from target domain, while meta-learning does not have such access.\n\nIn general, meta-learning learns a shared prior or hyper-parameters to generalise for unseen tasks, while domain adaptation produces a model to solve a particular task in a specified target domain. Recently, there is a variance of domain adaptation, named domain generalisation, where the aim is to learn a domain-invariant model without any information of target domain. In this view, domain generalisation is very similar to meta-learning, and there are some works that employ meta-learning algorithms for domain generalisation (D. Li et al. 2018; Y. Li et al. 2019).\n\n\n2.3 Multi-task learning\nMulti-task learning learns several related auxiliary tasks and a target task simultaneously to exploit the diversity of task representation to regularise and improve the performance on the target task~(Caruana 1997). If the input \\mathbf{x} is assumed to be the same across T extra tasks and the target task \\mathcal{T}_{T + 1}, then the objective of multi-task learning can be expressed as: \n\\min_{\\mathbf{w}_{0}, \\{\\mathbf{w}_{i}\\}_{i = 1}^{T + 1}} \\frac{1}{T + 1} \\sum_{i = 1}^{T + 1} \\ell_{i} \\left( h_{i} \\left( g\\left( \\mathbf{x}; \\mathbf{w}_{0} \\right); \\mathbf{w}_{i} \\right), \\mathbf{y}_{i} \\right),\n\\tag{7} where \\mathbf{y}_{i}, \\ell_{i} and h_{i} are the label, loss function and the classifier for task \\mathcal{T}_{i}, respectively, and g(., \\mathbf{w}_{0}) is the shared feature extractor for T + 1 tasks.\nMulti-task learning is often confused with meta-learning due to their similar nature extracting information from many tasks. However, the objective function of multi-task learning in Equation 7 is a single-level optimisation for the shared parameter \\mathbf{w}_{0} and multiple task-specific classifier \\{\\mathbf{w}_{i}\\}_{i = 1}^{T + 1}. It is, therefore, not as complicated as a bi-level optimisation seen in meta-learning as shown in Equation 2. Furthermore, multi-task learning aims to solve a number of specific tasks known during training (referred to as target tasks), while meta-learning targets the generalisation for unseen tasks in the future.\n\n\n2.4 Continual learning\nContinual or life-long learning refers to a situation where a learning agent has access to a continuous stream of tasks available over time, and the number of tasks to be learnt is not pre-defined~(Chen and Liu 2018; Parisi et al. 2019). The aim is to accommodate the knowledge extracted from one-time observed tasks to accelerate the learning of new tasks without catastrophically forgetting old tasks~(French 1999). In this sense, continual learning is very similar to meta-learning. However, continual learning most likely focuses on systematic design to acquire new knowledge in such a way that prevents interfering to the existing one, while meta-learning is more about algorithmic design to learn the new knowledge more efficiently. Thus, we cannot mathematically distinguish their differences as done in sub-sections Fine-tuning, Domain adaptation and generalisation and Multi-task learning . Nevertheless, continual learning criteria, especially catastrophic forgetting, can be encoded into meta-learning objective to advance further continual learning performance~(Al-Shedivat et al. 2018; Nagabandi et al. 2019)."
  },
  {
    "objectID": "posts/meta-learning/index.html#summary",
    "href": "posts/meta-learning/index.html#summary",
    "title": "From hyper-parameter optimisation to meta-learning",
    "section": "3 Summary",
    "text": "3 Summary\nIn general, meta-learning is an extension of hyper-parameter optimisation in multi-task setting. The objective function of meta-learning is, therefore, a bi-level optimisation, where the lower-level is to adapt the meta-parameter to a task, while the upper-level is to evaluate how well the meta-parameter performs across T tasks. Given such mathematical formulation, we can easily distinguish meta-learning from some common transfer learning approaches, such as fine-tuning, multi-task learning, domain adaptation and continual learning.\nHope that this post would give another perspective of meta-learning. I’ll see you in the next post about probabilistic methods in meta-learning."
  },
  {
    "objectID": "posts/meta-learning/index.html#references",
    "href": "posts/meta-learning/index.html#references",
    "title": "From hyper-parameter optimisation to meta-learning",
    "section": "4 References",
    "text": "4 References\n\n\nAl-Shedivat, Maruan, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. 2018. “Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments.” In International Conference on Learning Representation.\n\n\nBaxter, Jonathan. 2000. “A Model of Inductive Bias Learning.” Journal of Artificial Intelligence Research 12: 149–98.\n\n\nBen-David, Shai, John Blitzer, Koby Crammer, Fernando Pereira, et al. 2007. “Analysis of Representations for Domain Adaptation.” Advances in Neural Information Processing Systems 19: 137.\n\n\nCaruana, Rich. 1997. “Multitask Learning.” Machine Learning 28 (1): 41–75.\n\n\nChen, Zhiyuan, and Bing Liu. 2018. “Lifelong Machine Learning.” Synthesis Lectures on Artificial Intelligence and Machine Learning 12 (3): 1–207.\n\n\nDaume III, Hal, and Daniel Marcu. 2006. “Domain Adaptation for Statistical Classifiers.” Journal of Artificial Intelligence Research 26: 101–26.\n\n\nDomke, Justin. 2012. “Generic Methods for Optimization-Based Modeling.” In Artificial Intelligence and Statistics, 318–26. PMLR.\n\n\nFinn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.” In International Conference on Machine Learning, 1126–35.\n\n\nFrench, Robert M. 1999. “Catastrophic Forgetting in Connectionist Networks.” Trends in Cognitive Sciences 3 (4): 128–35.\n\n\nHeckman, James J. 1979. “Sample Selection Bias as a Specification Error.” Econometrica: Journal of the Econometric Society, 153–61.\n\n\nHospedales, Timothy M, Antreas Antoniou, Paul Micaelli, and Amos J Storkey. 2021. “Meta-Learning in Neural Networks: A Survey.” IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n\nJapkowicz, Nathalie, and Shaju Stephen. 2002. “The Class Imbalance Problem: A Systematic Study.” Intelligent Data Analysis 6 (5): 429–49.\n\n\nLi, Da, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. 2018. “Learning to Generalize: Meta-Learning for Domain Generalization.” In Thirty-Second AAAI Conference on Artificial Intelligence.\n\n\nLi, Yiying, Yongxin Yang, Wei Zhou, and Timothy Hospedales. 2019. “Feature-Critic Networks for Heterogeneous Domain Generalization.” In International Conference on Machine Learning, 3915–24. PMLR.\n\n\nLi, Zhenguo, Fengwei Zhou, Fei Chen, and Hang Li. 2017. “Meta-Sgd: Learning to Learn Quickly for Few-Shot Learning.” arXiv Preprint arXiv:1707.09835.\n\n\nLorraine, Jonathan, Paul Vicol, and David Duvenaud. 2020. “Optimizing Millions of Hyperparameters by Implicit Differentiation.” In International Conference on International Conference on Artificial Intelligence and Statistics, 1540–52. PMLR.\n\n\nNagabandi, Anusha, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. 2019. “Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning.” In International Conference on Learning Representation.\n\n\nNaik, Devang K, and RJ Mammone. 1992. “Meta-Neural Networks That Learn by Learning.” In International Joint Conference on Neural Networks, 1:437–42. IEEE.\n\n\nNichol, Alex, Joshua Achiam, and John Schulman. 2018. “On First-Order Meta-Learning Algorithms.” CoRR abs/1803.02999. http://arxiv.org/abs/1803.02999.\n\n\nParisi, German I, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. 2019. “Continual Lifelong Learning with Neural Networks: A Review.” Neural Networks 113: 54–71.\n\n\nPearlmutter, Barak A. 1994. “Fast Exact Multiplication by the Hessian.” Neural Computation 6: 147–60.\n\n\nPratt, Lorien Y, Jack Mostow, Candace A Kamm, and Ace A Kamm. 1991. “Direct Transfer of Learned Information Among Neural Networks.” In Aaai, 91:584–89.\n\n\nRajeswaran, Aravind, Chelsea Finn, Sham Kakade, and Sergey Levine. 2019. “Meta-Learning with Implicit Gradients.”\n\n\nSchmidhuber, Jürgen. 1987. “Evolutionary Principles in Self-Referential Learning (on Learning How to Learn: The Meta-Meta-... Hook).” Diploma thesis, Technische Universität München.\n\n\nShimodaira, Hidetoshi. 2000. “Improving Predictive Inference Under Covariate Shift by Weighting the Log-Likelihood Function.” Journal of Statistical Planning and Inference 90 (2): 227–44.\n\n\nSnell, Jake, Kevin Swersky, and Richard Zemel. 2017. “Prototypical Networks for Few-Shot Learning.” In Advances in Neural Information Processing Systems, 4077–87.\n\n\nVinyals, Oriol, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. 2016. “Matching Networks for One Shot Learning.” In Advances in Neural Information Processing Systems, 29:3630–38.\n\n\nYosinski, Jason, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. “How Transferable Are Features in Deep Neural Networks?” In Advances in Neural Information Processing Systems."
  },
  {
    "objectID": "posts/mixture-models/index.html",
    "href": "posts/mixture-models/index.html",
    "title": "Expectation - Maximisation algorithm and its applications for mixture models",
    "section": "",
    "text": "In machine learning or statistical inference, we often encounter problems relating to missing data or hidden variables. One typical example of such latent variable models is finite mixture models, e.g. Gaussian mixture or multinomial mixture models. Due to the nature of missing data or latent variables, calculating the likelihood of those models requires the marginalization over the distribution of the latent variables, and hence, complicates the maximum likelihood estimation (MLE). A general technique dealing with latent variable models is the expectation - maximization (EM)~(Dempster, Laird, and Rubin 1977). The basic idea of EM algorithm is to alternate between estimating the posterior of the latent variables (or missing data) in the E-step (expectation step), then using the completed data to calculate the MLE in the M-step (maximization step). It has been proved that by iterating the process, the likelihood of interest is non-decreasing. In other words, EM algorithm guarantees to converge to a saddle point.\nIn this post, we re-formulate a simpler form of the EM algorithm. We then demonstrate the application of the EM algorithm on two common MLE problems: Gaussian mixture models and multinomial mixture models. Readers could also refer to Chapter 9 in (Bishop and Nasrabadi 2006) (note that there are some typos which are corrected in erratum)."
  },
  {
    "objectID": "posts/mixture-models/index.html#notations",
    "href": "posts/mixture-models/index.html#notations",
    "title": "Expectation - Maximisation algorithm and its applications for mixture models",
    "section": "1 Notations",
    "text": "1 Notations\nBefore diving into the formulation and examples, we define the notations used in the following table.\n\nNotations used in the formulation of the EM algorithm.\n\n\nNotation\nDescription\n\n\n\n\n\\mathbf{x} \\in \\mathbb{R}^{D}\nobservable data\n\n\n\\mathbf{z} \\in \\mathbb{R}^{K}\nlatent variable or missing data\n\n\n\\theta \\in \\Theta\nthe parameter of interest in MLE"
  },
  {
    "objectID": "posts/mixture-models/index.html#em-algorithm",
    "href": "posts/mixture-models/index.html#em-algorithm",
    "title": "Expectation - Maximisation algorithm and its applications for mixture models",
    "section": "2 EM algorithm",
    "text": "2 EM algorithm\n\n2.1 Derivation\nThe aim of EM is to maximize the log-likelihood of the observed data: \n    \\max_{\\theta} \\ln p(\\mathbf{x} | \\theta) = \\max_{\\theta} \\ln \\left[ \\sum_{\\mathbf{z}} p(\\mathbf{x}, \\mathbf{z} | \\theta) \\right].\n\\tag{1}\nDue to the presence of the sum over the latent variable \\mathbf{z}, the logarithm cannot be evaluated directly on the joint distribution, resulting in complicated expressions for the maximum likelihood solution.\nTo solve the MLE in Equation 1, we shall now assume that the completed log-likelihood p(\\mathbf{x}, \\mathbf{z} | \\theta) can be evaluated and maximized easily. Such assumption allows EM to get around the MLE in Equation 1 as follows. Let q(\\mathbf{z}) &gt; 0 be an arbitrary distribution of the latent variable \\mathbf{z}. The observed data log-likelihood in Equation 1 can be written as: \n    \\begin{aligned}\n        \\ln p(\\mathbf{x} | \\theta) & = \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\ln p(\\mathbf{x} | \\theta) \\right] \\\\\n        & = \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\ln p(\\mathbf{x} | \\theta) + \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta) - \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta) + \\ln q(\\mathbf{z}) - \\ln q(\\mathbf{z}) \\right] \\\\\n        & = \\mathbb{E}_{q(\\mathbf{z})} \\left\\{ \\left[ \\ln p(\\mathbf{x} | \\theta) + \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta) - \\ln q(\\mathbf{z}) \\right] + \\left[ \\ln q(\\mathbf{z}) - \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta) \\right] \\right\\} \\\\\n        & = \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\ln p(\\mathbf{x} | \\theta) + \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta) - \\ln q(\\mathbf{z}) \\right] + \\mathrm{KL} \\left[ q(\\mathbf{z}) \\| p(\\mathbf{z} | \\mathbf{x}, \\theta) \\right],\n    \\end{aligned}\n where: \\mathrm{KL}[ q \\| p ] is the Kullback-Leibler divergence (KL divergence for short) between probability distributions q and p.\nSince \\mathrm{KL}[ q \\| p ] \\ge 0 and \\mathrm{KL}[ q \\| p ] = 0 iff q = p, the log-likelihood of interest can be lower-bounded as: \n    \\ln p(\\mathbf{x} | \\theta) \\ge \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\ln p(\\mathbf{x} | \\theta) + \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta) - \\ln q(\\mathbf{z}) \\right],\n and the equality occurs iff q(\\mathbf{z}) = p(\\mathbf{z} | \\mathbf{x}, \\theta). The tightest bound can then be written as: \n    \\begin{aligned}\n        \\mathsf{L}(\\theta, \\theta^{\\mathrm{old}}) & = \\mathbb{E}_{p(\\mathbf{z} | \\mathbf{x}, \\theta^{\\mathrm{old}})} [ \\ln p(\\mathbf{x} | \\theta) + \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta) - \\underbrace{\\ln p(\\mathbf{z} | \\mathbf{x}, \\theta^{\\mathrm{old}})}_{\\text{const. w.r.t. } \\theta} ] \\\\\n        & = \\mathbb{E}_{p(\\mathbf{z} | \\mathbf{x}, \\theta^{\\mathrm{old}})} [ \\ln p(\\mathbf{x}, \\mathbf{z} | \\theta) ] + \\mathrm{const.}\n    \\end{aligned}\n\\tag{2}\nNote that we denote the posterior of the latent variable as p(\\mathbf{z} | \\mathbf{x}, \\theta^{\\mathrm{old}}) to introduce the Expectation Maximization algorithm in the following. The reason why such \\theta^{\\mathrm{old}} is introduced is that we need \\theta to calculate the posterior of \\mathbf{z}, and we need the posterior to evaluate the lower-bound to optimize for \\theta. That leads to an iterative approach, known as EM, to maximize the log-likelihood of interest.\nHence, instead of maximizing the incomplete log-likelihood in Equation 1, we first tighten the lower-bound (E-step) and then maximize it (M-step). This allows an interative algorithm to perform the MLE on incomplete data, which is described as follows:\n\n\nInitialization: initialize the parameter of interest: \\theta^{\\mathrm{old}} \\gets \\theta\n\n\nE-step: calculate the posterior of the latent variable p(\\mathbf{z} | \\mathbf{x}, \\theta^{\\mathrm{old}})\n\n\nM-step: maximize the tightest lower-bound: \\theta \\gets \\arg\\max_{\\theta} \\mathsf{L}(\\theta, \\theta^{\\mathrm{old}}).\n\n\nThe whole algorithm can be referred to Algorithm 1.\n\\begin{algorithm}\n    \\caption{MLE via expectation - maximization algorithm}\n    \\begin{algorithmic}\n        \\PROCEDURE{Maximum-likelihood-estimation}{observed data $\\mathbf{x}$}\n            \\STATE initialise parameter $\\theta^{\\mathrm{old}}$\n            \\WHILE{$\\mathsf{L}(\\theta, \\theta^{\\mathrm{old}})$ not converged} \\Comment{$\\mathsf{L}(\\theta, \\theta^{\\mathrm{old}})$ is defined in Eq. (2)}\n                \\STATE calculate the posterior $p(\\mathbf{z} | \\mathbf{x}, \\theta^{\\mathrm{old}})$ \\Comment{E-step}\n                \\STATE maximize the lower-bound: $\\theta^{\\mathrm{old}} \\gets \\arg\\max_{\\theta} \\mathsf{L}(\\theta, \\theta^{\\mathrm{old}})$ \\COMMENT{M-step}\n            \\ENDWHILE\n            \\RETURN $\\theta$\n        \\ENDPROCEDURE\n    \\end{algorithmic}\n\\end{algorithm}\n\n\nRemark. One can also apply MAP instead of MLE by adding the log prior in the optimization objective. In this case, the difference is at the M-step, while the E-step is still the same.\n\n\n\n2.2 Convergence\n\nTheorem 1 After each EM iteration, the log-likelihood \\ln p(\\mathbf{x} | \\theta) is non-decreasing. Mathematically, it can be written as follows: \n    p(\\mathbf{x} | \\theta^{(n + 1)}) \\ge p(\\mathbf{x} | \\theta^{(n)}),\n where the superscript denotes the result obtained after that iteration.\n\n\nProof. Note that the EM algorithm improves the lower-bound \\mathsf{L}(\\theta, \\theta^{(n)}) after every iteration. Thus, we need to connect the lower-bound to the likelihood of interest to prove the theorem. The log-likelihood of interest can be written as: \n    \\begin{aligned}\n        \\ln p(\\mathbf{x} | \\theta) & = \\mathbb{E}_{p(\\mathbf{z} | \\mathbf{x}, \\theta^{(n)})} \\left[ \\ln p(\\mathbf{x} | \\theta) \\right] \\\\\n        & = \\mathbb{E}_{p(\\mathbf{z} | \\mathbf{x}, \\theta^{(n)})} \\left[ \\ln p(\\mathbf{x}, \\mathbf{z} | \\theta) - \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta) \\right] \\\\\n        & = \\mathsf{L}(\\theta, \\theta^{(n)}) - \\mathbb{E}_{p(\\mathbf{z} | \\mathbf{x}, \\theta^{(n)})} \\left[ \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta) \\right].\n    \\end{aligned}\n\\tag{3}\nSince it holds for any \\theta, we can substitute \\theta = \\theta^{(n)}) to obtain the likelihood after iteration n-th: \n    \\ln p(\\mathbf{x} | \\theta^{(n)}) = \\mathsf{L}(\\theta^{(n)}, \\theta^{(n)}) - \\mathbb{E}_{p(\\mathbf{z} | \\mathbf{x}, \\theta^{(n)})} \\left[ \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta^{(n)}) \\right].\n\\tag{4}\nSubstracting side by side of Equation 3 and Equation 4 gives the following: \n    \\begin{aligned}\n        \\ln p(\\mathbf{x} | \\theta) - \\ln p(\\mathbf{x} | \\theta^{(n)}) & = \\mathsf{L}(\\theta, \\theta^{(n)}) - \\mathsf{L}(\\theta^{(n)}, \\theta^{(n)}) + \\mathbb{E}_{p(\\mathbf{z} | \\mathbf{x}, \\theta^{(n)})} \\left[ \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta^{(n)}) - \\ln p(\\mathbf{z} | \\mathbf{x}, \\theta) \\right] \\\\\n        & = \\mathsf{L}(\\theta, \\theta^{(n)}) - \\mathsf{L}(\\theta^{(n)}, \\theta^{(n)}) + \\mathrm{KL} \\left[ p(\\mathbf{z} | \\mathbf{x}, \\theta^{(n)}) \\| p(\\mathbf{z} | \\mathbf{x}, \\theta) \\right].\n    \\end{aligned}\n\nSince KL divergence is non-negative, one can imply that: \n    \\ln p(\\mathbf{x} | \\theta) - \\ln p(\\mathbf{x} | \\theta^{(n)}) \\ge \\mathsf{L}(\\theta, \\theta^{(n)}) - \\mathsf{L}(\\theta^{(n)}, \\theta^{(n)}).\n\\tag{5}\nIn the M-step, we obtain \\theta^{(n + 1)} by maximizing \\mathsf{L}(\\theta, \\theta^{(n)}) w.r.t. \\theta. Thus, according to the definition of the maximization: \n    \\mathsf{L}(\\theta^{(n + 1)}, \\theta^{(n)}) \\ge \\mathsf{L}(\\theta^{(n)}, \\theta^{(n)}).\n\nHence, one can conclude that: \n    \\ln p(\\mathbf{x} | \\theta^{(n + 1)}) \\ge \\ln p(\\mathbf{x} | \\theta^{(n)})."
  },
  {
    "objectID": "posts/mixture-models/index.html#applications-of-em",
    "href": "posts/mixture-models/index.html#applications-of-em",
    "title": "Expectation - Maximisation algorithm and its applications for mixture models",
    "section": "3 Applications of EM",
    "text": "3 Applications of EM\nOne of the typical applications of EM algorithm is to perform maximum likelihood for finite mixture models. This section is, therefore, dedicated to discuss the application of EM on Gaussian and multinomial mixture models.\n\n3.1 Gaussian mixture models\nThe Gaussian mixture distribution can be written as a convex combination of K Gaussian components: \n    p(\\mathbf{x}) = \\sum_{k = 1}^{K} \\pi_{k} \\, \\mathcal{N}(\\mathbf{x}; \\pmb{\\mu}_{k}, \\pmb{\\Sigma}_{k}),\n where: \\pi_{k} \\in [0, 1] and \\pmb{\\pi}^{\\top} \\pmb{1} = 1.\nA data-point of the above Gaussian mixture distribution can be generated as follows:\n\n\nsample a K-dimensional categorical (one-hot) vector from the distribution of mixture coefficient: \\mathbf{z} \\sim \\mathrm{Categorical}(\\mathbf{z}; \\pmb{\\pi})\n\n\nsample a data-point from the corresponding Gaussian component: \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}; \\pmb{\\mu}_{k}, \\pmb{\\Sigma}_{k}), where z_{k} = 1.\n\n\nIn other words, the Gaussian mixture distribution can be written in the form of latent variable models as:\n\n    p(\\mathbf{x}) = \\sum_{\\mathbf{z}} p(\\mathbf{z}) \\, p(\\mathbf{x} | \\mathbf{z}) = \\sum_{k = 1}^{K} \\pi_{k} \\, \\mathcal{N}(\\mathbf{x}; \\pmb{\\mu}_{k}, \\pmb{\\Sigma}_{k}),\n\nwhere: \\mathbf{z} is the latent random variable.\nIf the objective is to use MLE to find the Gaussian components from a given set of data-points \\mathbf{X} = \\{\\mathbf{x}_{n}\\}_{n = 1}^{N} sampled from the Gaussian mixture distribution, the parameter of interest will be: \\theta = \\{(\\pmb{\\mu}_{k}, \\pmb{\\Sigma}_{k})\\}_{k = 1}^{K}. In this case, one can simply follow the EM algorithm presented in Section 2. Note that the likelihood on N iid data-points can be written as:\n\n    p(\\mathbf{X} | \\theta) = \\prod_{n = 1}^{N} p(\\mathbf{x}_{n} | \\theta) = \\prod_{n = 1}^{N} \\sum_{k = 1}^{K} p(\\mathbf{x}_{n} | z_{nk} = 1, \\theta) \\, p(z_{nk} = 1).\n\nE-step: calculate the posterior of the latent variable \\mathbf{z}_{n} given the observed data \\mathbf{x}_{n} and the model parameter \\{(\\pmb{\\mu}_{k}^{\\mathrm{old}}, \\pmb{\\Sigma}_{k}^{\\mathrm{old}})\\}_{k = 1}^{K}\n\n    \\begin{aligned}\n        \\gamma(z_{nk}) = p\\left(z_{nk} = 1 | \\mathbf{x}_{n}, \\theta^{\\mathrm{old}} \\right) & = \\frac{p(\\mathbf{x}_{n} | z_{nk} = 1, \\theta^{\\mathrm{old}}) \\, p(z_{nk} = 1)}{\\sum_{j = 1}^{K} p(\\mathbf{x}_{n} | z_{nj} = 1, \\theta^{\\mathrm{old}}) \\, p(z_{nj} = 1)} \\\\\n        & = \\frac{\\pi_{k} \\, \\mathcal{N}(\\mathbf{x}_{n}; \\pmb{\\mu}_{k}^{\\mathrm{old}}, \\pmb{\\Sigma}_{k}^{\\mathrm{old}})}{\\sum_{j = 1}^{K} \\pi_{j} \\, \\mathcal{N}(\\mathbf{x}_{n}; \\pmb{\\mu}_{j}^{\\mathrm{old}}, \\pmb{\\Sigma}_{j}^{\\mathrm{old}})}.\n    \\end{aligned}\n\\tag{6}\nM-step: maximize the lower-bound w.r.t. model parameter \\theta where the lower-bound can be expressed as: \n    \\begin{aligned}\n        \\mathsf{L}\\left(\\theta, \\theta^{\\mathrm{old}} \\right) & = \\sum_{n = 1}^{N} \\mathbb{E}_{p(\\mathbf{z}_{n} | \\mathbf{x}_{n}, \\theta^{\\mathrm{old}})} [ \\ln p(\\mathbf{x}_{n} | \\mathbf{z}_{n}, \\theta) + \\ln p(\\mathbf{z}_{n}) ] \\\\\n        & = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} p(z_{nk} = 1 | \\mathbf{x}_{n}, \\theta) \\ln p(\\mathbf{x}_{n} | z_{nk} = 1, \\theta) + \\mathrm{const.} \\\\\n        & = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} \\gamma(z_{nk}) \\ln \\mathcal{N}(\\mathbf{x}_{n}; \\pmb{\\mu}_{k}, \\pmb{\\Sigma}_{k}) + \\mathrm{const.} \\\\\n        & = -\\frac{1}{2} \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} \\gamma(z_{nk}) \\left[ \\ln \\left| \\pmb{\\Sigma}_{k} \\right| + (\\mathbf{x}_{n} - \\pmb{\\mu}_{k})^{\\top} \\pmb{\\Sigma}_{k}^{-1} (\\mathbf{x}_{n} - \\pmb{\\mu}_{k}) \\right] + \\mathrm{const.}\n    \\end{aligned}\n\nTaking derivative w.r.t. \\pmb{\\mu}_{k} and setting it to zero give:\n\n    \\begin{aligned}\n        & \\Delta_{\\pmb{\\mu}_{k}} \\mathsf{L} = \\sum_{n = 1}^{N} \\gamma(z_{nk}) \\pmb{\\Sigma}_{k}^{-1} (\\mathbf{x}_{n} - \\pmb{\\mu}_{k}) = 0 \\\\\n        & \\Rightarrow \\left[ \\sum_{n = 1}^{N} \\gamma(z_{nk}) \\right] \\pmb{\\mu}_{k} = \\sum_{n = 1}^{N} \\gamma(z_{nk}) \\mathbf{x}_{n}.\n    \\end{aligned}\n\nOr:\n\n    \\boxed{\n        \\pmb{\\mu}_{k} = \\frac{\\sum_{n = 1}^{N} \\gamma(z_{nk}) \\mathbf{x}_{n}}{\\sum_{n = 1}^{N} \\gamma(z_{nk})}.\n    }\n\nSimilarly for \\pmb{\\Sigma}_{k}:\n\n    \\begin{aligned}\n        \\Delta_{\\pmb{\\Sigma}_{k}} & = -\\frac{1}{2} \\sum_{n = 1}^{N} \\gamma(z_{nk}) \\left[ \\pmb{\\Sigma}_{k}^{-1} - \\pmb{\\Sigma}_{k}^{-1} (\\mathbf{x}_{n} - \\pmb{\\mu}_{k}) (\\mathbf{x}_{n} - \\pmb{\\mu}_{k})^{\\top} \\pmb{\\Sigma}_{k}^{-1} \\right] = 0 \\\\\n        \\Rightarrow & \\boxed{\n            \\pmb{\\Sigma}_{k} = \\frac{1}{\\sum_{n = 1}^{N} \\gamma(z_{nk})} \\sum_{n = 1}^{N} \\gamma(z_{nk}) (\\mathbf{x}_{n} - \\pmb{\\mu}_{k}) (\\mathbf{x}_{n} - \\pmb{\\mu}_{k})^{\\top}.\n        }\n    \\end{aligned}\n\n\n\n3.2 Multinomial mixture models\nSimilar to the Gaussian mixture models, a multinomial mixture model can also be written as:\n\n    p(\\mathbf{x}) = \\sum_{\\mathbf{z}} p(\\mathbf{z}) p(\\mathbf{x} | \\mathbf{z}) = \\sum_{k = 1}^{K} \\pi_{k} \\mathrm{Mult}(\\mathbf{x}; m, \\rho_{k}).\n\nNote that we only consider the case where all the multinomial components have the same parameter m (the number of trials).\nE-step This step is to calculate the posterior of the latent variable \\mathbf{z}_{n} given the data \\mathbf{x}_{n}: \n    \\begin{aligned}\n        \\gamma_{nk} & = p(\\mathbf{z}_{nk} = 1 | \\mathbf{x}_{n}, \\pi^{(t)}, \\rho^{(t)}) \\\\\n        & = \\frac{p(\\mathbf{x}_{n} | \\mathbf{z}_{nk} = 1, \\rho^{(t)}) \\, p(\\mathbf{z}_{nk} = 1 | \\pi^{(t)})}{\\sum_{k = 1}^{K} p(\\mathbf{x}_{n} | \\mathbf{z}_{nk} = 1, \\rho^{(t)}) \\, p(\\mathbf{z}_{nk} = 1 | \\pi^{(t)})} \\\\\n        & = \\frac{\\pi_{k}^{(t)} \\, \\mathrm{Mult}(\\mathbf{x}_{n}; m, \\rho_{k}^{(t)})}{\\sum_{k = 1}^{K} \\pi_{k}^{(t)} \\, \\mathrm{Mult}(\\mathbf{x}_{n}; m, \\rho_{k}^{(t)})}.\n    \\end{aligned}\n\\tag{7}\nM-step In the M-step, we maximise the following expected completed log-likelihood w.r.t. \\pi and \\rho:\n\n    \\begin{aligned}\n        \\mathsf{L} = & \\sum_{n = 1}^{N} \\mathbb{E}_{p(\\mathbf{z}_{n} | \\mathbf{x}_{n}, \\pi^{(t)}, \\rho^{(t)})} \\left[ \\ln p(\\mathbf{x}_{n}, \\mathbf{z}_{n} | \\pi, \\rho) \\right] \\\\\n        & = \\sum_{n = 1}^{N} \\mathbb{E}_{p(\\mathbf{z}_{n} | \\mathbf{x}_{n}, \\pi^{(t)}, \\rho^{(t)})} \\left[ \\ln p(\\mathbf{z}_{n} | \\pi) + \\ln p(\\mathbf{x}_{n} | \\mathbf{z}_{n}, \\rho) \\right] \\\\\n        & = \\sum_{n = 1}^{N} \\mathbb{E}_{p(\\mathbf{z}_{n} | \\mathbf{x}_{n}, \\pi^{(t)}, \\rho^{(t)})} \\left[ \\sum_{k = 1}^{K} \\mathbf{z}_{nk} \\ln \\pi_{k} + \\mathbf{z}_{nk} \\ln \\mathrm{Mult} (\\mathbf{x}_{n}; m, \\rho_{k}) \\right] \\\\\n        & = \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} \\gamma_{nk} \\left[ \\ln \\pi_{k} + \\sum_{d = 1}^{D} \\mathbf{x}_{nd} \\ln \\rho_{kd} + \\mathrm{const.} \\right]\n    \\end{aligned}\n\nThe Lagrangian for \\pi can be written as: \n    \\mathsf{L}_{\\pi} = \\mathsf{L} - \\lambda \\left( \\sum_{k = 1}^{K} \\pi_{k} - 1 \\right),\n where \\lambda is the Lagrange multiplier.\nTaking derivative of the Lagrangian w.r.t. \\pi_{k} gives: \n    \\frac{\\partial \\mathsf{L}_{\\pi}}{\\partial \\pi_{k}} = \\frac{1}{\\pi_{k}} \\sum_{n = 1}^{N} \\gamma_{nk} - \\lambda.\n\nSetting the derivative to zero and solving for \\pi_{k} gives: \n    \\pi_{k} = \\frac{1}{\\lambda} \\sum_{n = 1}^{N} \\gamma_{nk}.\n\nAnd since \\sum_{k = 1}^{K} \\pi_{k} = 1, one can substitute and find that \\lambda = N. Thus: \n    \\boxed{\n        \\pi_{k}^{(t + 1)} = \\frac{1}{N} \\sum_{n = 1}^{N} \\gamma_{nk}.\n    }\n\nSimilarly, the Lagrangian of \\rho can be expressed as: \n    \\mathsf{L}_{\\rho} = \\mathsf{L} - \\sum_{k = 1}^{K} \\eta_{k} \\left( \\sum_{d = 1}^{D} \\rho_{kd} - 1 \\right),\n where \\eta_{k} is the Lagrange multiplier. Taking derivative w.r.t. \\rho_{kd} gives: \n    \\frac{\\partial \\mathsf{L}_{\\rho}}{\\partial \\rho_{kd}} = \\frac{1}{\\rho_{kd}} \\sum_{n = 1}^{N} \\gamma_{nk} \\mathbf{x}_{nd} - \\eta_{k}.\n Setting the derivative to zero and solving for \\rho_{kd} gives: \n    \\rho_{kd} = \\frac{1}{\\eta_{k}} \\sum_{n = 1}^{N} \\gamma_{nk} \\mathbf{x}_{nd}.\n The constraint on \\rho_{k} as a probability vector leads to \\eta_{k} = m \\sum_{n = 1}^{N} \\gamma_{nk}. Thus: \n    \\boxed{\n        \\rho_{kd}^{(t + 1)} = \\frac{\\sum_{n = 1}^{N} \\gamma_{nk} \\mathbf{x}_{nd}}{m \\sum_{n = 1}^{N} \\gamma_{nk}}.\n    }"
  },
  {
    "objectID": "posts/mixture-models/index.html#references",
    "href": "posts/mixture-models/index.html#references",
    "title": "Expectation - Maximisation algorithm and its applications for mixture models",
    "section": "4 References",
    "text": "4 References\n\n\nBishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B (Methodological) 39 (1): 1–22."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html",
    "href": "posts/vae-normalising-constant-matters/index.html",
    "title": "VAE: normalising constant matters",
    "section": "",
    "text": "Variational auto-encoder (VAE) is one of the most popular generative models in machine learning nowadays. However, the rapid development of the field has made many machine learning practitioners (or, maybe only me) focus too much on deep learning without paying much attention to some fundamentals, such as linear regression. That causes much confusion due to the discrepancy between the derivation and the practical implementation, in which the regularization of the loss, or specifically the Kullback-Leibler (KL) divergence, is weighted by some factor \\beta. I myself did experience and struggle at the beginning of my research. Even though weighting the KL divergence term by a factor $ $ could temporarily resolve the issue, I has been questioning why the balancing between reconstruction and KL divergence is necessary. Eventually, the answer is quite simple: the normalizing constant in the reconstruction loss (or negative log-likelihood) that has been often ignored. This ignorance is the main cause of the imbalance between the two losses."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html#variational-auto-encoder",
    "href": "posts/vae-normalising-constant-matters/index.html#variational-auto-encoder",
    "title": "VAE: normalising constant matters",
    "section": "1 Variational auto-encoder",
    "text": "1 Variational auto-encoder\nGiven data points \\mathbf{x} = \\{x_{i}\\}_{n=1}^{N}, the model of a VAE assumes that there is a corresponding latent variable \\mathbf{z} = \\{ z_{n} \\}_{n=1}^{N} that generates data \\mathbf{x}. In short, the objective function of a VAE is to minimize the variational-free energy (VFE) given as: \n    \\min_{q} \\underbrace{\\mathbb{E}_{q(\\mathbf{z})} \\left[ - \\ln p(\\mathbf{x} | \\mathbf{z}) \\right]}_{\\text{reconstruction loss}} + \\textcolor{red}{\\beta} \\mathrm{KL} \\left[ q(\\mathbf{z}) \\Vert p(\\mathbf{x}) \\right], \\tag{vfe}\n where q(\\mathbf{z}) is the variational distribution of the latent variable, and \\textcolor{red}{\\beta} = 1 is the weighting factor.\nIn practice, people often “specify” the reconstruction loss as mean squared error (MSE) or binary cross-entropy loss and use gradient descent to minimize VFE. With \\beta = 1 as in (vfe), the reconstruction of different images seem to be the same image (see Figure 1 (top)), whereas setting $ $ results in much better reconstructed images (see Figure 1 (bottom)).\n\n \n\nFigure 1. The reconstructed images from VAE with β = 1 (top) and β ≪ 1 (bottom). Source: stats.stackexchange.com\n\n\nThis does not make me satisfied, although some justifications for setting \\beta to some small value are made. For example: - Setting \\beta \\ll 1 leads to even a “further lower-bound”. Hence, maximizing this “further lower-bound” is still mathematically reasonable. However, this bound is very loose. Can we do something better? - One can cast the problem to a constrained optimization as in β-VAE paper. However, β in that case is the Lagrange multiplier, and should be obtained through the optimization. Is it mathematically correct if considering β as a hyper-parameter? I doubt that.\nLater on, I figure out that the main reason of the imbalance between the two losses is due to the “specification” of the reconstruction loss. Simply specifying the type of the loss -\\ln p(\\mathbf{x} \\vert \\mathbf{z}) as MSE or binary cross-entropy would ignore the normalizing constant, resulting in an incorrect reconstruction loss. The correct way is to specify the modeling assumption of the likelihood p(\\mathbf{x} \\vert \\mathbf{z}), which, in the case of VAE, goes back to linear regression.\nIn the following sections, f(\\mathbf{z}; \\theta) denotes the output of the decoder parameterized by a neural network with weight \\theta. Usually, f(\\mathbf{z}; \\theta) is assumed to be the reconstructed images, but this might not always true depending on the assumption used."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html#reconstruction-likelihood-with-gaussian-assumption",
    "href": "posts/vae-normalising-constant-matters/index.html#reconstruction-likelihood-with-gaussian-assumption",
    "title": "VAE: normalising constant matters",
    "section": "2 Reconstruction likelihood with Gaussian assumption",
    "text": "2 Reconstruction likelihood with Gaussian assumption\nThis corresponds to linear regression with Gaussian noise assumption.\nThe variable of interest \\mathbf{x} is assumed to be a deterministic function f(\\mathbf{z}; \\theta) with additional Gaussian noise, so that: \n    \\mathbf{x} = f(\\mathbf{z}; \\theta) + \\epsilon,\n where: \\epsilon \\sim \\mathcal{N}\\left( \\epsilon; 0, \\Lambda^{-1} \\right). Thus, the reconstruction likelihood can be written as: \n    p(\\mathbf{x} \\vert \\mathbf{z}, \\theta, \\Lambda) = \\mathcal{N}(\\mathbf{x}; f(\\mathbf{z}; \\theta), \\Lambda^{-1}) = \\prod_{n=1}^{N} \\mathcal{N}(x_{n}; f(z_{n}; \\theta), \\Lambda^{-1}).\n Hence, the negative log-likelihood, or the reconstruction loss in the VAE, can be expressed as: \n    -\\ln p(\\mathbf{x} \\vert \\mathbf{z}, \\theta, \\Lambda) = - \\frac{N}{2} \\ln \\frac{\\Lambda}{2 \\pi} + \\Lambda \\times \\frac{1}{2} \\underbrace{\\sum_{n=1}^{N} \\left[ x_{n} - f(z_{n}; \\theta) \\right]^{2}}_{N \\times \\text{MSE}}. \\tag{nll-G}\n\n\nNote that current practice uses only MSE, which ignores the first term and the scaling factor relating to the noise precision \\Lambda.\n\nUnder this modeling approach, the decoder would consist of 2 networks: one for mean \\bar{x} = f(z; \\theta) and the other for noise precision \\Lambda = g(z; \\phi). Of course, one can consider \\Lambda as a hyper-parameter to simplify further the implementation.\nThe “full” loss function of a VAE is, therefore, presented as: \n    \\boxed{\n    \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\frac{N}{2} \\ln(2\\pi) - \\frac{N}{2} \\ln \\Lambda + \\frac{\\Lambda}{2} \\sum_{n=1}^{N} \\left[ x_{n} - f(z_{n}; \\theta) \\right]^{2} \\right] + \\mathrm{KL} \\left[ q(\\mathbf{z}) \\Vert p(\\mathbf{x}) \\right]. \\tag{vfe-G}\n    }\n\nAfter training, one can pass an image to the encoder h(.; \\phi) and decoder to get the predicted mean and precision. The reconstructed images can then be obtained as: \n    \\hat{x} \\sim \\mathcal{N}(x; f(z; \\theta), \\Lambda), \\text{where } z = h(x; \\phi).\n Although this approach is easy to understand, one drawback is the unbounded support of the Gaussian distribution, resulting in reconstructed pixel intensity values out of the desired range [0, 1]. Consequently, when visualizing, the pixels that are out of that range will be truncated to 0 or 1, potentially making the reconstructed images blurrier."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html#reconstruction-likelihood-with-continuous-bernoulli-assumption",
    "href": "posts/vae-normalising-constant-matters/index.html#reconstruction-likelihood-with-continuous-bernoulli-assumption",
    "title": "VAE: normalising constant matters",
    "section": "3 Reconstruction likelihood with continuous Bernoulli assumption",
    "text": "3 Reconstruction likelihood with continuous Bernoulli assumption\nThis corresponding to linear regression in [0, 1] (not $ {0, 1 } $ as in logistic regression), and hence, the words “continuous Bernoulli”.\nThis modeling approach is not as intuitive as the one with Gaussian assumption, but please bear with me for a moment.\nThe likelihood of interest, p(\\mathbf{x} \\vert \\mathbf{z}), is assumed to be a continuous Bernoulli distribution: \n    p(\\mathbf{x} \\vert \\mathbf{z}) = \\mathcal{CB}(\\mathbf{x}; f(\\mathbf{z}; \\theta)) = \\prod_{n=1}^{N} \\underbrace{C \\left( f(z_{n}; \\theta) \\right)}_{\\text{normalizing const.}}  \\underbrace{\\left[ f(z_{n}; \\theta) \\right]^{x_{n}} \\left[ 1 - f(z_{n}; \\theta) \\right]^{1 - x_{n}}}_{\\text{Bernoulli pdf}},\n and $f(z_{n}; )) , n {1, , N } $.\nNote that: - the usage of continuous Bernoulli distribution is due to the fact that VAE tries to regress the pixel intensity x_{n} which falls in [0, 1], not $ {0, 1 } $ as in classification, - the pdf of a continuous Bernoulli distribution differs from a Bernoulli distribution at the normalizing constant term, - the output of the decoder now is not the mean of the reconstructed pixel intensity as in the case of Gaussian distribution, - due to the assumption of the continuous Bernoulli distribution, the last layer of the decoder must be activated by sigmoid function to ensure the output falling in $[0, 1] $.\nThe negative log-likelihood, or reconstruction loss, can be easily derived as: \n    - \\ln p(\\mathbf{x} \\vert \\mathbf{z}) = \\sum_{n=1}^{N} \\underbrace{ - \\left[ x_{n} \\ln f(z_{n}; \\theta) + (1 - x_{n}) \\ln \\left[1 - f(z_{n}; \\theta) \\right] \\right]}_{\\text{binary cross-entropy}} - \\underbrace{\\ln C \\left( f(z_{n}; \\theta) \\right)}_{\\text{log normalizing const.}}. \\tag{nll-CB}\n\n\nCurrent practice uses binary cross-entropy loss only, corresponding to Bernoulli distribution. To me, that practice is not correct, since the learning is to infer the parameter of the Bernoulli distribution, which is the probability when the outcome is 1. In that case, the pixel intensity is in $ {0, 1 } $, not $[0, 1] $. This explains why VAE using binary cross-entropy loss often works well for gray-scale, but not color, images.\n\nSubstituting (nll-CB) into (vfe) gives the “full” objective function for VAE: \n    \\boxed{\n        \\begin{aligned}\n        & - \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\sum_{n=1}^{N} x_{n} \\ln f(z_{n}; \\theta) + (1 - x_{n}) \\ln \\left[1 - f(z_{n}; \\theta) \\right] \\right. \\\\\n        & \\quad \\left. + \\ln C \\left( f(z_{n}; \\theta) \\right) \\right] + \\mathrm{KL} \\left[ q(\\mathbf{z}) \\Vert p(\\mathbf{x}) \\right].\n        \\end{aligned}\n        \\tag{vfe-CB}\n    }\n\nNote that after training, direct plotting f(z; \\theta) as the pixel intensity might result in an incorrect reconstructed image, since the mean of the continuous Bernoulli distribution is not equal to its parameter. To reconstruct an image x, one needs to pass that image through the encoder and decoder, and then: \n    \\hat{x} \\sim \\mathcal{CB}\\left(x; f(z; \\theta) \\right),\n and plot \\hat{x} to visualize the reconstructed image."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html#conclusion",
    "href": "posts/vae-normalising-constant-matters/index.html#conclusion",
    "title": "VAE: normalising constant matters",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nVAE is often considered as a basic generative model. However, most machine learning practitioners often learn by memorization about the “type” of reconstruction loss. This leads to the weighting trick in the implementation. Understanding the nature of the reconstruction loss as the log-likelihood in linear regression allows one to obtain the “full” objective function without applying any weighting tricks. Hopefully, this post would be useful to save time for ones who start to practise machine learning."
  },
  {
    "objectID": "posts/vae-normalising-constant-matters/index.html#references",
    "href": "posts/vae-normalising-constant-matters/index.html#references",
    "title": "VAE: normalising constant matters",
    "section": "5 References",
    "text": "5 References\n\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S. and Lerchner, A., 2016. β-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representation.\nLoaiza-Ganem, G. and Cunningham, J.P., 2019. The continuous Bernoulli: fixing a pervasive error in variational autoencoders. In Advances in Neural Information Processing Systems (pp. 13287-13297)."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Expectation - Maximisation algorithm and its applications for mixture models\n\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2022\n\n\nCuong Nguyen\n\n\n\n\n\n\n  \n\n\n\n\nBias - variance decomposition\n\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2022\n\n\nCuong Nguyen\n\n\n\n\n\n\n  \n\n\n\n\nFrom hyper-parameter optimisation to meta-learning\n\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2021\n\n\nCuong Nguyen\n\n\n\n\n\n\n  \n\n\n\n\nOuter product approximation of Hessian matrix\n\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2021\n\n\nCuong Nguyen\n\n\n\n\n\n\n  \n\n\n\n\nPAC-Bayes bounds for generalisation error\n\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2020\n\n\nCuong Nguyen\n\n\n\n\n\n\n  \n\n\n\n\nVAE: normalising constant matters\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2020\n\n\nCuong Nguyen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#biography",
    "href": "about.html#biography",
    "title": "About",
    "section": "Biography",
    "text": "Biography\n\nI am a research associate at the School of Computer Science, The University of Adelaide. I received my Ph.D. in Computer Science from The University of Adelaide in 2022, an M.Phil. in Electronic Engineering also from The University of Adelaide in 2018, and a B.S. in Mechanical Engineering from Portland State University in 2012."
  },
  {
    "objectID": "about.html#areas-of-specialism",
    "href": "about.html#areas-of-specialism",
    "title": "About",
    "section": "Areas of specialism",
    "text": "Areas of specialism\nMachine learning"
  },
  {
    "objectID": "about.html#current-and-previous-roles",
    "href": "about.html#current-and-previous-roles",
    "title": "About",
    "section": "Current and previous roles",
    "text": "Current and previous roles\n\n\n\n\n\nResearch associate\n The University of Adelaide\n Apr. 2021 - present\n South Australia, AU\n\n\n\n\n\nProcess & equipment engineer\n Intel Products Vietnam\n July 2012 - July 2015\n Ho Chi Minh, VN"
  },
  {
    "objectID": "about.html#my-qualifications",
    "href": "about.html#my-qualifications",
    "title": "About",
    "section": "My qualifications",
    "text": "My qualifications\n\n\n\n\n\nPh.D. in Computer Science\n The University of Adelaide\n Apr. 2018 - Mar. 2022\n South Australia, AU\n\n\n\n\n\nM.Phil. in Electronic Engineering\n The University of Adelaide\n Aug. 2015 - Jan. 2018\n South Australia, AU\n\n\n\n\n\nB.S. in Mechanical Engineering\n Portland State University\n Jul. 2010 - Jun. 2012\n Oregon, USA"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Courses taught at The University of Adelaide\n\n\n\n\n\n\n\n\n\nYear\nSemester\nCourse\nLevel\nRole\n\n\n\n\n2023\n2\nGrand Challenges in Computer Science\nUndergraduate\nIntructor\n\n\n2022\n2\nGrand Challenges in Computer Science\nUndergraduate\nIntructor\n\n\n2021\n1\nFoundations on Computer Science\nMaster\nTutor\n\n\n2021\n2\nFoundations on Computer Science\nMaster\nTutor"
  },
  {
    "objectID": "teaching.html#the-university-of-adelaide",
    "href": "teaching.html#the-university-of-adelaide",
    "title": "Teaching",
    "section": "",
    "text": "Courses taught at The University of Adelaide\n\n\n\n\n\n\n\n\n\nYear\nSemester\nCourse\nLevel\nRole\n\n\n\n\n2023\n2\nGrand Challenges in Computer Science\nUndergraduate\nIntructor\n\n\n2022\n2\nGrand Challenges in Computer Science\nUndergraduate\nIntructor\n\n\n2021\n1\nFoundations on Computer Science\nMaster\nTutor\n\n\n2021\n2\nFoundations on Computer Science\nMaster\nTutor"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Hello, my name is Cường Cao Nguyễn (pronounced similarly to: Ker-uhng Kao Ner-win). I am a research associate at the School of Computer Science, The University of Adelaide working with Professor Gustavo Carneiro and Dr Toan Do.\nMy main research interest lies in the modelling, formulating and understanding machine learning algorithms and their applications in medical data analysis."
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publication",
    "section": "",
    "text": "Please see the list of my publications at my Google Scholar page"
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html",
    "href": "posts/Gauss-Newton-matrix/index.html",
    "title": "Outer product approximation of Hessian matrix",
    "section": "",
    "text": "Hessian matrix is heavily studied in the optimization community. The purpose is to utilize the second order derivative to optimize a function of interest (also known as Newton’s method). In machine learning, especially Bayesian inference, Hessian matrix can be found in some applications, such as Laplace’s method which approximates a distribution by a Gaussian distribution. Although Hessian matrix provides additional information which improves the convergence rate in optimization or reduces a complicated distribution to a Gaussian distribution, calculating a Hessian matrix often increases computation complexity. In neural networks where the number of model parameters is very large, Hessian matrix is often intractable due to the limited computation and memory.\nMany efficient approximations of Hessian matrix have been developed to either reduce the running time complexity or decompose the Hessian matrix to reduce the amount of memory storage. Hessian-free approaches which utilizes the Hessian-vector product are also attracted much research interest. This post will present an approximation of Hessian matrix using the outer product. Note that this approximation represents an approximated Hessian matrix by a set of matrices whose sizes are reasonable to store in GPU memory. The trade-off is that the running time complexity to obtain the Hessian matrix is still quadractic. Note that this approximation is also known as Gauss-Newton matrix."
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html#notations",
    "href": "posts/Gauss-Newton-matrix/index.html#notations",
    "title": "Outer product approximation of Hessian matrix",
    "section": "1 Notations",
    "text": "1 Notations\nBefore going into details, let’s define some notations used:\n\n\\{x_{i}, t_{i}\\}_{i = 1}^{N} is the input and label of data-point i-th,\n\\mathbf{w} \\in \\mathbb{R}^{W} is the parameter of the model of interest, or the weight of a neural network,\n\\ell(.) \\in \\mathbb{R} is the loss function, e.g. MSE or cross-entropy,\n\\mathbf{f}(x_{i}, \\mathbf{w}) \\in \\mathbb{R}^{C} is the pre-nonlinearity output of the neural network at the final layer that has C hidden units,\n\\sigma\\left[ \\mathbf{f}\\left(x_{i}, \\mathbf{w}\\right) \\right] \\in \\mathbb{R}^{C} is the activation output at the final layer. For example, in regression, \\sigma(z) = z is the identity function, or in logistic regression, \\sigma(.) is the sigmoid function, while in multi-class classification, \\sigma(.) is the softmax function,\n\nThe loss function of interest is defined as the sum of losses over each data point: \nL = \\sum_{i = 1}^{N} \\ell\\left( \\sigma(\\mathbf{f}(x_{i}, \\mathbf{w}), t_{i}\\right).\n Note that in the following, we will omit the notation of the label t_{i} from the loss \\ell(.) to make the notation unclutered."
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html#derivation-of-the-approximated-hessian-matrix",
    "href": "posts/Gauss-Newton-matrix/index.html#derivation-of-the-approximated-hessian-matrix",
    "title": "Outer product approximation of Hessian matrix",
    "section": "2 Derivation of the approximated Hessian matrix",
    "text": "2 Derivation of the approximated Hessian matrix\nAn element of the Hessian matrix can then be written as: \n\\begin{aligned}\n\\mathbf{H}_{jk} & = \\frac{\\partial}{\\partial\\mathbf{w}_{k}} \\left( \\frac{\\partial L}{\\partial \\mathbf{w}_{j}} \\right) = \\frac{\\partial}{\\partial\\mathbf{w}_{k}} \\left( \\sum_{i=1}^{N} \\frac{\\partial \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{w}_{j}} \\right) \\\\\n& = \\frac{\\partial}{\\partial \\mathbf{w}_{k}} \\left( \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\frac{\\partial\\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})} \\frac{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j}} \\right) \\quad \\text{\\textcolor{ForestGreen}{(chain rule)}}\\\\\n& = \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\frac{\\partial}{\\partial \\mathbf{w}_{k}} \\left( \\frac{\\partial \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})} \\frac{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j}} \\right).\n\\end{aligned}\n\nApplying the chain rule for the first term gives: \n\\begin{aligned}\n\\mathbf{H}_{jk} & = \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\left[ \\sum_{l=1}^{C} \\left( \\frac{\\partial^{2} \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w}) \\, \\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})} \\frac{\\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{k}} \\right) \\frac{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j}} \\right] \\\\\n& \\qquad \\qquad \\quad + \\frac{\\partial \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})} \\frac{\\partial^{2} \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j} \\, \\partial \\mathbf{w}_{k}}.\n\\end{aligned}\n\nRearranging gives: \n\\begin{aligned}\n\\mathbf{H}_{jk} & = \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\frac{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j}} \\sum_{l=1}^{C} \\frac{\\partial^{2} \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w}) \\, \\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})} \\frac{\\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{k}} \\\\\n& \\quad + \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\underbrace{\\frac{\\partial \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}}_{\\approx 0} \\frac{\\partial^{2} \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j} \\, \\partial \\mathbf{w}_{k}}.\n\\end{aligned}\n\nNear the optimum, the scalar \\mathbf{f}_{c} would be very closed to its target \\mathbf{t}_{ic}. Hence, the derivative of the loss w.r.t. \\mathbf{f}_{c} is very small, and we can approximate the Hessian as: \n\\mathbf{H}_{jk} \\approx \\sum_{i=1}^{N} \\sum_{c=1}^{C} \\frac{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{j}} \\sum_{l=1}^{C} \\frac{\\partial^{2} \\ell \\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w}) \\right)\\right]}{\\partial \\mathbf{f}_{c} (x_{i}, \\mathbf{w}) \\, \\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})} \\frac{\\partial \\mathbf{f}_{l}(x_{i}, \\mathbf{w})}{\\partial \\mathbf{w}_{k}}.\n\nRewriting this with matrix notation yields a much simpler formulation: \n\\boxed{\n\\mathbf{H} \\approx \\sum_{i=1}^{N} \\mathbf{J}_{fi}^{\\top} \\mathbf{H}_{\\sigma i} \\mathbf{J}_{fi},\n}\n where: \n\\begin{aligned}\n\\mathbf{J}_{fi} & = \\nabla_{\\mathbf{w}} \\mathbf{f}(x_{i}, \\mathbf{w}) \\in \\mathbb{R}^{C \\times W} \\quad \\text{\\textcolor{ForestGreen}{(Jacobian matrix of \\textbf{f} w.r.t. \\textbf{w})}}\\\\\n\\mathbf{H}_{\\sigma i} & = \\nabla_{\\mathbf{f}}^{2} \\ell\\left[ \\sigma \\left( \\mathbf{f}(x_{i}, \\mathbf{w} \\right) \\right] \\in \\mathbb{R}^{C \\times C} \\quad \\text{\\textcolor{ForestGreen}{(Hessian of loss w.r.t. \\textbf{f})}}.\n\\end{aligned}\n\nNote that the Hessian matrix \\mathbf{H}_{\\sigma} can be manually calculated.\n\nRemark. Instead of storing the Hessian matrix \\mathbf{H} with size {W \\times W} which needs a large amount of memory, we can store the two matrices \\{\\mathbf{J}_{fi}, \\mathbf{H}_{\\sigma i}\\}_{i=1}^{N}. This will reduce the amount of memory required. Of course, the trade-off is the increasing of the computation when performing the multiplication to obtain the Hessian matrix \\mathbf{H}.\n\nThe following section will present how to calculate the matrix \\mathbf{H}_{\\sigma} for some commonly-used losses."
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html#derivation-for-mathbfh_sigma",
    "href": "posts/Gauss-Newton-matrix/index.html#derivation-for-mathbfh_sigma",
    "title": "Outer product approximation of Hessian matrix",
    "section": "3 Derivation for \\mathbf{H}_{\\sigma}",
    "text": "3 Derivation for \\mathbf{H}_{\\sigma}\n\n3.1 Mean square error in regression\nIn the regression:\n\nC = 1\n\\sigma(.) is the identity function\n\\ell(f(x_{i}, \\mathbf{w}) = \\frac{1}{2} \\left( f(x_{i}, \\mathbf{w}) - t_{i} \\right)^{2}.\n\nHence, \\mathbf{H}_{\\sigma} = \\mathbf{I}_{1}, resulting in: \n\\boxed{\n    \\mathbf{H} = \\sum_{i=1}^{N} \\mathbf{J}_{fi}^{\\top} \\mathbf{J}_{fi},\n}\n which agrees with the results in (Bishop and Nasrabadi 2006 - Eq.(5.84)).\n\n\n3.2 Logistic regression\nIn this case:\n\nC = 1\n\\sigma(.) is the sigmoid function\n\\ell(\\sigma(f(x_{i}, \\mathbf{w})) = - t_{i} \\ln \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) - (1 - t_{i}) \\ln \\left( 1 - \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\right).\n\nThe first derivative is expressed as: \n\\frac{\\partial \\ell(\\sigma(f(x_{i}, \\mathbf{w}))}{\\partial f(x_{i}, \\mathbf{w})} = - t_{i} \\left( 1 - \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\right) + (1 - t_{i}) \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) = \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) - t_{i}.\n\nThe second derivative is therefore: \n\\frac{\\partial^{2} \\ell(\\sigma(f(x_{i}, \\mathbf{w}))}{\\partial f(x_{i}, \\mathbf{w})^{2}} = \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\left[ 1 - \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\right].\n\nHence: \n\\boxed{\n    \\mathbf{H} \\approx \\sum_{i=1}^{n} \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\left[ 1 - \\sigma \\left( f(x_{i}, \\mathbf{w}) \\right) \\right] \\mathbf{J}_{fi}^{\\top} \\mathbf{J}_{fi},\n}\n which agrees with the result derived in the literature (Bishop and Nasrabadi 2006 - Eq. (5.85)).\n\n\n3.3 Cross entropy loss in classification\nIn this case:\n\n\\sigma(\\mathbf{f}) is the softmax function,\n\\ell(\\sigma(\\mathbf{f}(x_{i}, \\mathbf{w}))) = -\\sum_{c=1}^{C} \\mathbf{t}_{ic} \\ln \\sigma_{c}(\\mathbf{f}(x_{i}, \\mathbf{w})).\n\nAccording to the definition of the softmax function: \n    \\sigma_{c} \\left( \\mathbf{f} \\right) = \\frac{\\exp(\\mathbf{f}_{c})}{\\sum_{k=1}^{C} \\exp(\\mathbf{f}_{k})}.\n\nHence, the derivative can be written as: \n    \\frac{\\partial \\sigma_{c}(\\mathbf{f})}{\\partial \\mathbf{f}_{c}} = \\frac{\\exp(\\mathbf{f}_{c}) \\sum_{k=1}^{C} \\exp(\\mathbf{f}_{k}) - \\exp(2 \\mathbf{f}_{c})}{\\left[ \\sum_{k=1}^{C} \\exp(\\mathbf{f}_{k}) \\right]^{2}} = \\sigma_{c}(\\mathbf{f}) \\left[ 1 - \\sigma_{c}(\\mathbf{f}) \\right],\n and \n    \\frac{\\partial \\sigma_{c}(\\mathbf{f})}{\\partial \\mathbf{f}_{k}} = - \\sigma_{c}(\\mathbf{f}) \\sigma_{k}(\\mathbf{f}), \\forall k \\neq j.\n\nAn element of the Jacobian vector of the loss w.r.t. \\mathbf{f} can be written as: \n\\begin{aligned}\n    \\frac{\\partial \\ell(\\sigma(\\mathbf{f}(x_{i}, \\mathbf{w})))}{\\partial \\mathbf{f}_{c}(x_{i}, \\mathbf{w})} & = - \\sum_{k=1}^{C} \\frac{\\mathbf{t}_{ik}}{\\sigma_{k}(\\mathbf{f})} \\frac{\\partial \\sigma_{k}(\\mathbf{f})}{\\partial \\mathbf{f}_{c}} \\\\\n    & = - \\mathbf{t}_{ic} \\left[ 1 - \\sigma_{c}(\\mathbf{f}) \\right] + \\sum_{\\substack{k=1\\\\k \\neq c}}^{C} \\mathbf{t}_{ik} \\sigma_{c}(\\mathbf{f}) \\\\\n    & = - \\mathbf{t}_{ic} + \\sigma_{c}(\\mathbf{f}) \\underbrace{\\sum_{k=1}^{C} \\mathbf{t}_{ik}}_{1}\\\\\n    & = \\sigma_{c}(\\mathbf{f}) - \\mathbf{t}_{ic}.\n\\end{aligned}\n\nHence, the Jacobian vector can be expressed as: \n    \\nabla_{\\mathbf{f}} \\ell(\\sigma(\\mathbf{f}(x_{i}, \\mathbf{w}))) = \\sigma(\\mathbf{f}(x_{i}, \\mathbf{w})) - \\mathbf{t}_{i}.\n\nThe Hessian matrix is given as: \n    \\nabla_{\\mathbf{f}}^{2} \\ell(\\sigma(\\mathbf{f}(x_{i}, \\mathbf{w}))) = \\nabla_{\\mathbf{f}} \\sigma(\\mathbf{f}(x_{i}, \\mathbf{w})).\n\nOr, in the explicit matrix form: \n    \\mathbf{H}_{\\sigma} = \\begin{bmatrix}\n    \\sigma_{1}(\\mathbf{f}) \\left[ 1 - \\sigma_{1}(\\mathbf{f}) \\right] & - \\sigma_{1}(\\mathbf{f}) \\sigma_{2}(\\mathbf{f}) & - \\sigma_{1}(\\mathbf{f}) \\sigma_{3}(\\mathbf{f}) & \\ldots & - \\sigma_{1}(\\mathbf{f}) \\sigma_{C}(\\mathbf{f})\\\\\n    - \\sigma_{2}(\\mathbf{f}) \\sigma_{1}(\\mathbf{f}) & \\sigma_{2}(\\mathbf{f}) \\left[ 1 - \\sigma_{2}(\\mathbf{f}) \\right] & - \\sigma_{2}(\\mathbf{f}) \\sigma_{3}(\\mathbf{f}) & \\ldots & - \\sigma_{2}(\\mathbf{f}) \\sigma_{C}(\\mathbf{f})\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n    - \\sigma_{C}(\\mathbf{f}) \\sigma_{1}(\\mathbf{f}) & - \\sigma_{C}(\\mathbf{f}) \\sigma_{2}(\\mathbf{f}) & - \\sigma_{C}(\\mathbf{f}) \\sigma_{3}(\\mathbf{f}) & \\ldots & \\sigma_{C}(\\mathbf{f}) \\left[ 1 - \\sigma_{C}(\\mathbf{f}) \\right]\n    \\end{bmatrix}."
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html#conclusion",
    "href": "posts/Gauss-Newton-matrix/index.html#conclusion",
    "title": "Outer product approximation of Hessian matrix",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nIn this post, we derive an approximation of the Hessian matrix. The Gauss-Newton matrix is a good approximation since it is positive-definite and more efficient to store under the form of a set of smaller matrices. Of course, we have not got away from the curse of dimensionality since the running time complexity to obtain the Hessian matrix is still quadratic w.r.t. the number of the model parameters. One final note is that one should use the approximated Hessian matrix with care since the approximation is assumed to be near the minimal value of the considered loss function."
  },
  {
    "objectID": "posts/Gauss-Newton-matrix/index.html#references",
    "href": "posts/Gauss-Newton-matrix/index.html#references",
    "title": "Outer product approximation of Hessian matrix",
    "section": "5 References",
    "text": "5 References\n\n\nBishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer."
  },
  {
    "objectID": "posts/PAC-Bayes-bounds/index.html",
    "href": "posts/PAC-Bayes-bounds/index.html",
    "title": "PAC-Bayes bounds for generalisation error",
    "section": "",
    "text": "Properly approaximately correct (PAC) learning is a part of statistical machine learning which has been a fundamental course for most of graduate programs in machine learning. Its main idea is to upper-bound the true risk (or generalisation error) by the empirical risk with certain confidence level. In other words, it is often written in the following form: \n\\Pr (\\text{true risk} \\le \\text{empirical risk} + r(m, \\delta)) \\ge 1 - \\delta\n where \\Pr(A) is the probability of event A, \\delta \\in (0, 1] is the confidence parameter, and r(m, \\delta) – a function of sample size m and the confidence parameter \\delta – is the regularization that is satisfied: \n\\lim_{m \\to +\\infty} r(m, \\delta) = 0.\n PAC-Bayes upper generalization bound is a kind of PAC learning. It was firstly proposed in 1999 McAllester (1999), and has attracted much of research interest. There has been many subsequent improvements made to tighten further this classic PAC-Bayes bound or to extend it to more general loss functions. However, the classic PAC-Bayes theorem is still the backbone. In this post, I will show how to prove this interesting theorem."
  },
  {
    "objectID": "posts/PAC-Bayes-bounds/index.html#auxillary-lemmas",
    "href": "posts/PAC-Bayes-bounds/index.html#auxillary-lemmas",
    "title": "PAC-Bayes bounds for generalisation error",
    "section": "1 Auxillary lemmas",
    "text": "1 Auxillary lemmas\nTo prove the classic PAC-Bayes theorem, we need two auxilliary lemmas shown below.\n\n1.1 Change of measure inequality for Kullback-Leibler divergence\n\nLemma 1 (Banerjee 2006 - Lemma 1) For any measurable function \\phi(h) on a set of predictor under consideration \\mathcal{H}, and any distributions P and Q on \\mathcal{H}, the following inequality holds: \n\\mathbb{E}_{Q} [\\phi(h)] \\le \\mathrm{KL} [Q \\Vert P] + \\ln \\mathbb{E}_{P} [\\exp(\\phi(h))].\n Further, \n\\sup_{\\phi} \\mathbb{E}_{Q} [\\phi(h)] - \\ln \\mathbb{E}_{P} [\\exp(\\phi(h))] = \\mathrm{KL} [Q \\Vert P].\n\n\n\nProof. For any measurable function \\phi(h), the following holds: \n\\begin{aligned}\n    \\mathbb{E}_{Q} [\\phi(h)] & = \\mathbb{E}_{Q} \\left[ \\ln \\left( \\exp(\\phi(h)) \\frac{Q(h)}{P(h)} \\frac{P(h)}{Q(h)} \\right) \\right] \\\\\n    & = \\mathrm{KL} [Q \\Vert P] + \\mathbb{E}_{Q} \\left[ \\ln \\left( \\exp(\\phi(h)) \\frac{P(h)}{Q(h)} \\right) \\right] \\\\\n    & \\le \\mathrm{KL} [Q \\Vert P] + \\ln \\mathbb{E}_{Q} \\left[ \\exp(\\phi(h)) \\frac{P(h)}{Q(h)} \\right] \\\\\n    & \\qquad \\text{(Jensen's inequality)}\\\\\n    & = \\mathrm{KL} [Q \\Vert P] + \\ln \\mathbb{E}_{P} \\left[ \\exp(\\phi(h)) \\right].\n\\end{aligned}\n\nFor the second part of the lemma, we need to examine the equality condition of the Jensen’s inequality. Since \\ln(x) is a strictly concave function for x &gt; 0, it follows that the equality holds when: \n\\begin{aligned}\n    \\exp \\left( \\phi(h) \\right) & \\frac{P(h)}{Q(h)} = 1 \\\\\n    \\iff \\phi(h) & = \\ln \\left[ \\frac{Q(h)}{P(h)} \\right].\n\\end{aligned}\n With this choice of \\phi(h), we can verify that the equality does hold.\nThis completes the proof.\n\n\n\n1.2 Concentration inequality\n\nLemma 2 (Shalev-Shwartz and Ben-David 2014 - Exercise 31.1) Let X be a random variable that satisfies: \\mathrm{Pr} (X \\ge \\epsilon) \\le e^{-2m \\epsilon^{2}}. Prove that \n\\mathbb{E} \\left[ e^{2(m - 1) X^{2}} \\right] \\le m.\n\n\n\nProof. Since the assumption is expressed in term of probability, while the conclusion is written in form of an expectation, what we need to do first is to try to present the expectation in terms of probability.\nFor simplicity, let Y = e^{2(m - 1) X^{2}}. Since X \\in [0, +\\infty), then Y \\in [1, +\\infty) and Y can be presented as: \nY = \\int_{1}^{+\\infty} \\pmb{1}(Y \\ge t) \\, \\mathrm{d}t + 1,\n where \\pmb{1}(A) is the indication function of event A. Note that the integral above is the area of a rectangle with height as 1 and the width Y - 1.\nOne important property of the indication function is that: \n\\mathbb{E} \\left[ \\pmb{1}(Y \\ge t) \\right] = \\mathrm{Pr}(Y \\ge t).\n This allows to express the expectation of interest as: \n\\begin{aligned}\n\\mathbb{E}[Y] & = \\mathbb{E} \\left[ \\int_{1}^{+\\infty} \\pmb{1}(Y \\ge t) \\, \\mathrm{d}t \\right] + 1 \\\\\n& = \\int_{1}^{+\\infty} \\mathbb{E} [\\pmb{1}(Y \\ge t)] \\, \\mathrm{d}t + 1 \\quad \\text{(Fubini's theorem)} \\\\\n& = \\int_{1}^{+\\infty} \\mathrm{Pr}(Y \\ge t) \\, \\mathrm{d}t + 1.\n\\end{aligned}\n Or: \n\\mathbb{E} \\left[ e^{2(m - 1) X^{2}} \\right] = \\int_{1}^{+\\infty} \\mathrm{Pr}( e^{2(m - 1) X^{2}} \\ge x) \\, \\mathrm{d}x + 1.\n\nWe then make a change of variable from x to \\epsilon to utilize the given inequality in the assumption. Let’s define: \nx = e^{2(m - 1) \\epsilon^{2}}.\n Since \\epsilon is assumed to be non-negative, we can express it as: \n\\epsilon = \\sqrt{\\frac{\\ln x}{2(m - 1)}},\n and: \n\\mathrm{d}x = 4(m - 1) \\epsilon \\, e^{2(m - 1) \\epsilon^{2}} \\, \\mathrm{d} \\epsilon.\n\nThe expectation of interest can, therefore, be written as: \n\\begin{aligned}\n    \\mathbb{E} \\left[ e^{2(m - 1) X^{2}} \\right] & = \\int_{0}^{+\\infty} \\mathrm{Pr} \\left( e^{2(m - 1) X^{2}} \\ge e^{2(m - 1) \\epsilon^{2}} \\right) 4(m - 1) \\epsilon \\, e^{2(m - 1) \\epsilon^{2}} \\, \\mathrm{d} \\epsilon  + 1\\\\\n    & = \\int_{0}^{+\\infty} \\mathrm{Pr} \\underbrace{\\left( X \\ge \\epsilon \\right)}_{\\le e^{-2m\\epsilon^{2}}} 4(m - 1) \\epsilon \\, e^{2(m - 1) \\epsilon^{2}} \\, \\mathrm{d} \\epsilon + 1\\\\\n    & \\le 4(m - 1) \\int_{0}^{+\\infty} \\epsilon \\, e^{-2 \\epsilon^{2}} \\, \\mathrm{d} \\epsilon + 1 = m.\n\\end{aligned}"
  },
  {
    "objectID": "posts/PAC-Bayes-bounds/index.html#pac-bayes-bound",
    "href": "posts/PAC-Bayes-bounds/index.html#pac-bayes-bound",
    "title": "PAC-Bayes bounds for generalisation error",
    "section": "2 PAC-Bayes bound",
    "text": "2 PAC-Bayes bound\n\nTheorem 1 Let D be an arbitrary distribution over an example domain Z. Let \\mathcal{H} be a hypothesis class, \\ell: \\mathcal{H} \\times Z \\to [0, 1] be a loss function, \\pi be a prior distribution over \\mathcal{H}, and \\delta \\in (0, 1]. If S = \\{z_j\\}_{j=1}^{m} is an i.i.d. training set sampled according to D, then for any “posterior” Q over \\mathcal{H}, the following holds: \n\\mathrm{Pr} \\left( \\mathbb{E}_{z_{j} \\sim D} \\mathbb{E}_{h \\sim Q} \\left[ \\ell(h, z_{j}) \\right] \\le \\mathbb{E}_{z_{j} \\sim S} \\mathbb{E}_{h \\sim Q} \\left[ \\ell(h, z_{j}) \\right] + \\sqrt{\\frac{\\mathrm{KL} [Q \\Vert \\pi] + \\frac{\\ln m}{\\delta}}{2(m - 1)}} \\right) \\ge 1 - \\delta.\n\n\n\nProof. We define some notations to ease the proving: - L = \\mathbb{E}_{z_{j} \\sim D} \\left[ \\ell(h, z_{j}) \\right] - \\hat{L} = \\mathbb{E}_{z_{j} \\sim S} \\left[ \\ell(h, z_{j}) \\right] = \\frac{1}{m} \\sum_{j=1}^{m} \\ell(h, z_{j}) - \\Delta L = L - \\hat{L}\nApplying Lemma 1 with P(h) = \\pi (h) and \\phi(h) = 2(m - 1) (\\Delta L)^{2} gives: \n2(m - 1) \\mathbb{E}_{Q} \\left[ (\\Delta L)^{2} \\right] - \\mathrm{KL} [Q \\Vert \\pi] \\le \\textcolor{purple}{\\ln \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right]}.\n\\tag{1}\nWe upper-bound the last term in the RHS (highlighted in purple color) by Lemma 2. To do that, we consider the empirical loss on each observable data point l(h, z_{j}) as a random variable in [0, 1] with true and empirical means L and \\hat{L}, respectively. Following the Hoeffding’s inequality gives: \n\\begin{aligned}\n\\mathrm{Pr} \\left( \\Delta L \\ge \\epsilon \\right) & = \\mathrm{Pr} \\left( L - \\hat{L} \\ge \\epsilon \\right)\\\\\n& \\le \\mathrm{Pr} \\left( | L - \\hat{L} | \\ge \\epsilon \\right)\\\\\n& \\le \\exp(-2m \\epsilon^{2}), \\quad \\epsilon \\ge 0.\n\\end{aligned}\n According to Lemma 2, this implies: \n\\mathbb{E}_{S} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right] \\le m.\n Taking the expectation w.r.t. h \\sim \\pi(h) on both sides and applying Fubini’s theorem (to interchange the 2 expectations) gives: \n\\begin{aligned}\n& \\mathbb{E}_{S} \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right] \\le \\mathbb{E}_{\\pi} \\left[ m \\right] = m\\\\\n& \\implies \\ln \\mathbb{E}_{S} \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right] \\le \\ln m\\\\\n& \\implies \\mathbb{E}_{S} \\textcolor{purple}{\\ln \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right]} \\le \\ln m.\n\\end{aligned}\n Note that the last implication is due to Jensen’s inequality.\nWe then apply Markov’s inequality for the term highlighted in purple: \n\\begin{aligned}\n\\mathrm{Pr} \\left( \\textcolor{purple}{\\ln \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right]} \\ge \\varepsilon \\right) & \\le \\frac{\\mathbb{E}_{S} \\textcolor{purple}{\\ln \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right]}}{\\varepsilon} \\\\\n& \\le \\frac{\\ln m}{\\varepsilon}.\n\\end{aligned}\n\nThis implies: \n\\mathrm{Pr} \\left( \\textcolor{purple}{\\ln \\mathbb{E}_{\\pi} \\left[\\exp \\left( 2(m - 1) (\\Delta L)^{2} \\right) \\right]} \\le \\varepsilon \\right) \\ge 1 - \\frac{\\ln m}{\\varepsilon}.\n\\tag{2}\nCombining the results in Equation 1 and Equation 2 gives: \n\\mathrm{Pr} \\left( 2(m - 1) \\mathbb{E}_{Q} \\left[ (\\Delta L)^{2} \\right] - \\mathrm{KL} [Q \\Vert \\pi] \\le \\varepsilon \\right) \\ge 1 - \\frac{\\ln m}{\\varepsilon}.\n\nThis is equivalent to: \n\\mathrm{Pr} \\left( \\mathbb{E}_{Q} \\left[ (\\Delta L)^{2} \\right] \\le \\frac{\\mathrm{KL} [Q \\Vert \\pi] + \\varepsilon}{2(m - 1)} \\right) \\ge 1 - \\frac{\\ln m}{\\varepsilon}.\n\\tag{3}\nNote that squared function is a strictly concave function, resulting in: \n\\mathbb{E}_{Q} \\left[ (\\Delta L)^{2} \\right] \\ge \\left( \\mathbb{E}_{Q} \\left[ \\Delta L \\right] \\right)^{2}.\n\nHence, Equation 3 can be written as: \n\\mathrm{Pr} \\left( \\mathbb{E}_{Q} \\left[ \\Delta L \\right] \\le \\sqrt{\\frac{\\mathrm{KL} [Q \\Vert \\pi] + \\varepsilon}{2(m - 1)}} \\right) \\ge 1 - \\frac{\\ln m}{\\varepsilon}.\n\nSeting \\delta = \\frac{\\ln m}{\\varepsilon}, and expanding \\Delta L according to its definition complete the proof."
  },
  {
    "objectID": "posts/PAC-Bayes-bounds/index.html#discussion",
    "href": "posts/PAC-Bayes-bounds/index.html#discussion",
    "title": "PAC-Bayes bounds for generalisation error",
    "section": "3 Discussion",
    "text": "3 Discussion\nAFAIK, the result in Theorem 1 is a seminal PAC-Bayes bound in the literature of PAC learning. Readers could refer subsequent derivations of tighter PAC-Bayes bounds developed later."
  },
  {
    "objectID": "posts/PAC-Bayes-bounds/index.html#references",
    "href": "posts/PAC-Bayes-bounds/index.html#references",
    "title": "PAC-Bayes bounds for generalisation error",
    "section": "4 References",
    "text": "4 References\n\n\nBanerjee, Arindam. 2006. “On Bayesian Bounds.” In International Conference on Machine Learning, 81–88.\n\n\nMcAllester, David A. 1999. “PAC-Bayesian Model Averaging.” In Conference on Computational Learning Theory, 164–70.\n\n\nShalev-Shwartz, Shai, and Shai Ben-David. 2014. Understanding Machine Learning: From Theory to Algorithms. Cambridge university press."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html",
    "href": "posts/bias-variance-decomposition/index.html",
    "title": "Bias - variance decomposition",
    "section": "",
    "text": "Bias and variance decomposition is one of the key tools to understand machine learning. However, conventional discussion about bias - variance decomposition revolves around the square loss (also known as mean square error). It is unclear whether such decomposition is still valid for some common loss functions, such as 0-1 loss or cross-entropy loss used in classification. This post is to present the decomposition for those losses following the unified framework of bias and variance decomposition from (Domingos 2000), its extended study on Bregman divergence with un-bounded support from (Pfau 2013) and the special case about Kullback-Leibler (KL) divergence (Heskes 1998)."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#notations",
    "href": "posts/bias-variance-decomposition/index.html#notations",
    "title": "Bias - variance decomposition",
    "section": "1 Notations",
    "text": "1 Notations\nThe notations are similar to the ones in (Domingos 2000), but for C-class classification.\n\nNotations used in the bias-variance decomposition.\n\n\n\n\n\n\nNotation\nDescription\n\n\n\n\n\\mathbf{x}\nan input instance in \\mathcal{X} \\subseteq \\in \\mathbb{R}^{d}\n\n\n\\Delta_{K}\nthe K-dimensional simplex \\equiv \\{\\mathbf{v} \\in \\mathbb{R}^{K + 1}_{+}: \\mathbf{v}^{\\top} \\pmb{1} = 1\\}\n\n\n\\Delta_{K}\nthe K-dimensional simplex \\equiv \\{\\mathbf{v} \\in \\mathbb{R}^{K + 1}_{+}: \\mathbf{v}^{\\top} \\pmb{1} = 1\\}\n\n\n\\mathbf{t}\na label instance: \\mathbf{t} \\sim p(\\mathbf{t} | \\mathbf{x}), for example: (i) one-hot vector if p(\\mathbf{t} | \\mathbf{x}) is a categorical distribution, or (ii) soft-label if p(\\mathbf{t} | \\mathbf{x}) is a Dirichlet or logistic normal distribution\n\n\n\\ell\nloss function \\ell: \\Delta_{C - 1} \\times \\Delta_{C - 1} \\to [0, +\\infty], e.g. 0-1 loss or cross-entropy loss\n\n\n\\mathbf{y}\npredicted label distribution: \\mathbf{y} = f(\\mathbf{x}) \\in \\Delta_{C - 1}\n\n\n\\mathcal{D}\nthe set of training sets"
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#terminologies",
    "href": "posts/bias-variance-decomposition/index.html#terminologies",
    "title": "Bias - variance decomposition",
    "section": "2 Terminologies",
    "text": "2 Terminologies\n\nDefinition 1 The optimal prediction \\mathbf{y}_{*} \\in \\Delta_{C - 1} of a target \\mathbf{t} is defined as follows: \n    \\mathbf{y}_{*} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell \\left( \\mathbf{t}, \\mathbf{y}^{\\prime} \\right) \\right].\n\n\n\nDefinition 2 The main model prediction for a loss function, \\ell, and the set of training sets, \\mathcal{D}, is defined as: \n    \\mathbf{y}_{m} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{\\mathcal{D}} \\left[ \\ell \\left(\\mathbf{y}, \\mathbf{y}^{\\prime} \\right) \\right].\n\n\n\nRemark. The defintions of optimal and main model predictions above assume that the loss function \\ell is symmetric in terms of the input arguments. For asymmetric loss function, such as Bregmand divergence or cross-entropy, the definitions of such predictions might be slightly changed at the order of the input arguments.\n\nGiven the definitions of \\mathbf{y}_{*} and \\mathbf{y}_{m}, the bias, variance and noise can be defined following the unified framework proposed in (Domingos 2000) as follows:\n\nDefinition 3 The bias of a learner on an example \\mathbf{x} is defined as: B(\\mathbf{x}) = \\ell \\left( \\mathbf{y}_{*}, \\mathbf{y}_{m} \\right).\n\n\nDefinition 4 The variance of a learner on an example \\mathbf{x} is defined as: V(\\mathbf{x}) = \\mathbb{E}_{\\mathcal{D}} \\left[ \\ell \\left( \\mathbf{y}_{m}, \\mathbf{y} \\right) \\right].\n\n\nDefinition 5 The noise of an example \\mathbf{x} is defined as: N(\\mathbf{x}) = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell(\\mathbf{t}, \\mathbf{y}_{*}) \\right].\n\nThe definitions of bias and variance above are quite intuitive comparing to other definitions in the literature. As \\mathbf{y}_{m} is the main model prediction, the bias B(\\mathbf{x}) measures the systematic deviation (loss) from the optimal (or true) label \\mathbf{y}_{*}, while the variance V(\\mathbf{x}) measures the loss induced due to the fluctuations of each model prediction \\mathbf{y} on different training datasets around the main prediction \\mathbf{y}_{m}. In addition, as the loss \\ell is non-negative, both the bias and variance are also non-negative.\nGiven the defintions of bias, variance and noise above, the unified decomposition proposed in (Domingos 2000) can be expressed as: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] & = \\textcolor{Crimson}{\\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m})} + c_{1} \\, \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}}[\\ell(\\mathbf{y}, \\mathbf{y}_{m})]} + c_{2} \\, \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})}[\\ell(\\mathbf{t}, \\mathbf{y_{*}})]} \\\\\n        & = \\textcolor{Crimson}{B(\\mathbf{x})} + c_{1} \\, \\textcolor{MidnightBlue}{V(\\mathbf{x})} + c_{2} \\, \\textcolor{Green}{N(\\mathbf{x})},\n    \\end{aligned}\n\\tag{1} where c_{1} and c_{2} are two scalars. For example, in MSE, c_{1} = c_{2} = 1.\nOf course, not all losses would satisfy the decomposition in Equation 1. However, as shown in (Domingos 2000 - Theorem 7), such decomposition can be used to bound the expected loss as long as the loss is metric. Nevertheless, in this post, we dicuss the composition on some common loss functions, such as 0-1 loss and Bregman divergence which includes MSE and Kullback-Leibler (KL) divergence."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#square-loss",
    "href": "posts/bias-variance-decomposition/index.html#square-loss",
    "title": "Bias - variance decomposition",
    "section": "3 Square loss",
    "text": "3 Square loss\nTo warm-up, we discuss a wellknown bias-variance decomposition in the literature. It is applied for MSE or square loss. Here, we use the notations of vectors instead of scalars as often seen in conventional analysis. We will derive a general decomposition for Bregman divergence in which MSE is a particular case in a later section.\n\nTheorem 1 When the loss is the square loss: \\ell(\\mathbf{y}_{1}, \\mathbf{y}_{2}) = || \\mathbf{y}_{1} - \\mathbf{y}_{2}||_{2}^{2}, then the expected loss on several training sets can be decomposed into: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\ell(\\mathbf{t}, \\mathbf{y}) & = \\textcolor{Crimson}{\\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m})} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [ \\ell(\\mathbf{y}_{m}, \\mathbf{y})]} + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ \\ell( \\mathbf{t}, \\mathbf{y}_{*} )]} \\\\\n        \\text{or: } \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y} ||_{2}^{2} & = \\underbrace{\\textcolor{Crimson}{|| \\mathbf{y}_{*} - \\mathbf{y}_{m} ||_{2}^{2}}}_{\\text{bias}} + \\underbrace{\\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} || \\mathbf{y}_{m} - \\mathbf{y} ||_{2}^{2}}}_{\\text{variance}} + \\underbrace{\\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y}_{*} ||_{2}^{2}}}_{\\text{noise}}.\n    \\end{aligned}\n\n\n\n\nPlease refer to the detailed proof here\n\n\nProof. Given the square loss, the optimal prediction can be determined as: \n    \\begin{aligned}\n        & \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y}^{\\prime} ||_{2}^{2} \\ge || \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\mathbf{t} \\right] - \\mathbf{y}^{\\prime} ||_{2}^{2} \\ge 0 \\quad \\text{(Jensen's inequality on L2-norm)}\\\\\n        \\implies & \\mathbf{y}_{*} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y}^{\\prime} ||_{2}^{2} = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}].\n    \\end{aligned}\n Similarly, the main model prediction can be obtained as: \\mathbf{y}_{m} = \\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}].\nThe expected loss can then be written as: \n    \\begin{aligned}\n        & \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y} ||_{2}^{2} \\\\\n        & = \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} (\\mathbf{t} - \\mathbf{y})^{\\top} (\\mathbf{t} - \\mathbf{y}) \\\\\n        & = \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left( (\\mathbf{t} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) + (\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}]) + (\\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}] - \\mathbf{y}) \\right)^{\\top} \\left( (\\mathbf{t} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) \\right. \\\\\n        & \\quad \\left. + (\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}]) + (\\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}] - \\mathbf{y}) \\right) \\\\\n        & = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] ||_{2}^{2} + || \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}] ||_{2}^{2} + \\mathbb{E}_{\\mathcal{D}} || \\mathbb{E}_{\\mathcal{D}} [\\mathbf{y}] - \\mathbf{y} ||_{2}^{2} \\\\\n        & = \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} || \\mathbf{t} - \\mathbf{y}_{*} ||_{2}^{2}} + \\textcolor{Crimson}{|| \\mathbf{y}_{*} - \\mathbf{y}_{m} ||_{2}^{2}} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} || \\mathbf{y}_{m} - \\mathbf{y} ||_{2}^{2}}.\n    \\end{aligned}"
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#loss",
    "href": "posts/bias-variance-decomposition/index.html#loss",
    "title": "Bias - variance decomposition",
    "section": "4 0-1 loss",
    "text": "4 0-1 loss\n\nDefinition 6 The 0-1 loss is defined as: \n    \\ell(\\mathbf{y}_{1}, \\mathbf{y}_{2}) = \\Bbb{1} (\\mathbf{y}_{1}, \\mathbf{y}_{2}) = \\begin{cases}\n        0 & \\text{if } \\mathbf{y}_{1} = \\mathbf{y}_{2},\\\\\n        1 & \\text{if } \\mathbf{y}_{1} \\neq \\mathbf{y}_{2}.\n    \\end{cases}\n\n\n\n4.1 Binary classification\n\nTheorem 2 ((Domingos 2000 - Theorem 2)) The expected 0-1 loss in a binary classification setting can be written as: \n    \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell(\\mathbf{t}, \\mathbf{y}) \\right] = \\textcolor{Crimson}{\\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m})} + \\textcolor{Brown}{c} \\, \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} \\left[ \\mathbf{y}, \\mathbf{y}_{m} \\right]} + \\left[ 2 p_{\\mathcal{D}}(\\mathbf{y} = \\mathbf{y}_{*}) - 1 \\right]  \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell(\\mathbf{t}, \\mathbf{y}_{*}) \\right]},\n where: \n    \\textcolor{Brown}{c} = \\begin{cases}\n        1 & \\text{if } \\mathbf{y}_{m} = \\mathbf{y}_{*}\\\\\n        -1 & \\text{otherwise}.\n    \\end{cases}\n\n\n\n\nThe proof is copied in (Domingos 2000 - Theorem 2) for a self-contained discussion.\n\n\nProof. To prove the theorem, we calculate \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] and \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{t}, \\mathbf{y})], then combine both of them to complete the proof.\nFirst, we proceed to prove the followings: \n    \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}) + c_{0} \\, \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y}_{*})],\n\\tag{2} with c_{0} = 1 if \\mathbf{y} = \\mathbf{y}_{*} and c_{0} = -1 if \\mathbf{y} \\neq \\mathbf{y}_{*}.\nIf \\mathbf{y} = \\mathbf{y}_{*}, then Equation 2 is trivially true with c_{0} = 1. We next prove Equation 2 when \\mathbf{y} \\neq \\mathbf{y}_{*}. Since there are only two classes, if \\mathbf{y} \\neq \\mathbf{y}_{*} and \\mathbf{t} \\neq \\mathbf{y}_{*}, then \\mathbf{y} = \\mathbf{t} and vice versa. And since two events are quivalent, p(\\mathbf{y} = \\mathbf{t}) = p(\\mathbf{t} \\neq \\mathbf{y}_{*}). The expected 0-1 loss w.r.t. \\mathbf{t} can be written as: \n    \\begin{aligned}\n        \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] & = p(\\mathbf{t} = \\mathbf{y})\\\\\n        & = 1 - p(\\mathbf{t} \\neq \\mathbf{y}) \\\\\n        & = 1 - p (\\mathbf{t} = \\mathbf{y}_{*}) \\\\\n        & = 1 - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ \\ell(\\mathbf{t}, \\mathbf{y}_{*}) ]\\\\\n        & = \\ell(\\mathbf{y}_{*}, \\mathbf{y}) - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ \\ell(\\mathbf{t}, \\mathbf{y}_{*}) ].\n    \\end{aligned}\n This proves Equation 2.\nNext, we show that: \n    \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}_{*}, \\mathbf{y})] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m}) + \\textcolor{Brown}{c} \\, \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}, \\mathbf{y}_{m})].\n\\tag{3}\nIf \\mathbf{y}_{m} = \\mathbf{y}_{*}, then Equation 3 is trivially true with \\textcolor{Brown}{c} = 1. If \\mathbf{y}_{m} \\neq \\mathbf{y}_{*}, then \\mathbf{y}_{m} \\neq \\mathbf{y} implies that \\mathbf{y} = \\mathbf{y}_{*} and vice-versa. Thus, the expected 0-1 loss w.r.t. different training set can be expressed as: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}_{*}, \\mathbf{y})] & = p(\\mathbf{y} \\neq \\mathbf{y}_{*}) = 1 - p(\\mathbf{y} = \\mathbf{y}_{*}) = 1 - p(\\mathbf{y}_{m} \\neq \\mathbf{y})\\\\\n        & = 1 - \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}_{m}, \\mathbf{y})] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m}) - \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}_{m}, \\mathbf{y})].\n    \\end{aligned}\n\nThus, it proves Equation 3.\nFinally, we can combine both results in Equation 2 and Equation 3 to prove the theorem. Taking the expectation w.r.t. \\mathcal{D} on both sides of Equation 2 gives: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell(\\mathbf{t}, \\mathbf{y}) \\right] & = \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{t}, \\mathbf{y})] + c_{0} \\, \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y}_{*})]\\\\\n        & = \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{t}, \\mathbf{y})] + c_{0} \\, \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y}_{*})].\n    \\end{aligned}\n\nAnd since: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} [c_{0}] & = p(\\mathbf{y} = \\mathbf{y}_{*}) - p (\\mathbf{y} \\neq \\mathbf{y}_{*} = 2 p(\\mathbf{y} = \\mathbf{y}_{*}) - 1,\n    \\end{aligned}\n we can then obtain the result of the theorem by using Equation 3.\n\n\n\n\n4.2 Multi-class classification\n\nTheorem 3 The expected loss for 0-1 loss in a multiclass classification can be decomposed into: \n    \\begin{aligned}\n        & \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ \\ell(\\mathbf{t}, \\mathbf{y}) \\right] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m}) + \\textcolor{Blue}{c} \\, \\mathbb{E}_{\\mathcal{D}} \\left[ \\mathbf{y}, \\mathbf{y}_{m} \\right] \\\\\n        & \\quad + [ 2 p_{\\mathcal{D}} (\\mathbf{y} = \\mathbf{y}_{*}) - p_{\\mathcal{D}} (\\mathbf{y} \\neq \\mathbf{y}_{*}) p_{\\mathbf{t}}(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} \\neq \\mathbf{t}) ]  \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ \\ell(\\mathbf{t}, \\mathbf{y}_{*}) ],\n    \\end{aligned}\n where: \n    c = \\begin{cases}\n        1 & \\text{if } \\mathbf{y}_{m} = \\mathbf{y}_{*}\\\\\n        -p_{\\mathcal{D}} (\\mathbf{y} = \\mathbf{y}_{*} | \\mathbf{y} \\neq \\mathbf{y}_{m}) & \\text{otherwise}.\n    \\end{cases}\n\n\n\n\nThe proof is copied in (Domingos 2000 - Theorem 3) for a self-contained discussion.\n\n\nProof. The proof is similar to the binary classification where we decompose \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] and \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{t}, \\mathbf{y})]. The key difference is that when \\mathbf{y} \\neq \\mathbf{y}_{*} and \\mathbf{t} \\neq \\mathbf{y}_{*} no longer imply that \\mathbf{y} = \\mathbf{t}. Similarly, \\mathbf{y}_{m} \\neq \\mathbf{y}_{*} and \\mathbf{y}_{m} \\neq \\mathbf{y} no longer imply \\mathbf{y} = \\mathbf{y}_{*}.\nNow, we want to prove the following decomposition: \n    \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}) + c_{0} \\, \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y}_{*})],\n\\tag{4} where: \n    c_{0} = \\begin{cases}\n        -p(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} \\neq \\mathbf{t}) & \\text{when } \\mathbf{y} \\neq \\mathbf{y}_{*}\\\\\n        1 & \\text{when } \\mathbf{y} = \\mathbf{y}_{*}.\n    \\end{cases}\n\nWhen \\mathbf{y} = \\mathbf{y}_{*}, Equation 4 is trivially true with c_{0} = 1.\nWhen \\mathbf{y} \\neq \\mathbf{y}_{*}, the following fact is true: p(\\mathbf{y} = \\mathbf{t}| \\mathbf{y}_{*} = \\mathbf{t}, \\mathbf{y} \\neq \\mathbf{y}_{*}) = 0. To simplify the notation, the condition \\mathbf{y} \\neq \\mathbf{y}_{*} is omitted. Thus, applying the sum rule on the probability of predicted label gives: \n    \\begin{aligned}\n        p(\\mathbf{y} = \\mathbf{t}) & = \\underbrace{p(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} = \\mathbf{t})}_{0} \\, p(\\mathbf{y}_{*} + \\mathbf{t}) + p(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} \\neq \\mathbf{t}) \\, p(\\mathbf{y}_{*} \\neq \\mathbf{t}) \\\\\n        & = p(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} \\neq \\mathbf{t}) \\, p(\\mathbf{y}_{*} \\neq \\mathbf{t}).\n    \\end{aligned}\n\nThe expected loss w.r.t. \\mathbf{t} can be written as: \n    \\begin{aligned}\n        \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y})] & = p(\\mathbf{y} \\neq \\mathbf{t}) = 1 - p(\\mathbf{y} = \\mathbf{t})\\\\\n        & = 1 \\underbrace{- p(\\mathbf{y} = \\mathbf{t} | \\mathbf{y}_{*} \\neq \\mathbf{t})}_{c_{0}} \\, p(\\mathbf{y}_{*} \\neq \\mathbf{t})\\\\\n        & = \\ell(\\mathbf{y}_{*}, \\mathbf{y}) + c_{0} \\, \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\ell(\\mathbf{t}, \\mathbf{y}_{*})].\n    \\end{aligned}\n This proves Equation 4.\nSimilarly, one can prove the decomposition for the expected loss w.r.t. \\mathcal{D}: \n    \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}_{*}, \\mathbf{y})] = \\ell(\\mathbf{y}_{*}, \\mathbf{y}_{m}) + \\textcolor{Brown}{c} \\, \\mathbb{E}_{\\mathcal{D}} [\\ell(\\mathbf{y}, \\mathbf{y}_{m})].\n\\tag{5}\nCombining the results in Equation 4 and Equation 5 in a similar manner in the case of binary classification completes the proof."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#bregman-divergence",
    "href": "posts/bias-variance-decomposition/index.html#bregman-divergence",
    "title": "Bias - variance decomposition",
    "section": "5 Bregman divergence",
    "text": "5 Bregman divergence\nThe derivation and discussion in this section is extracted from (Pfau 2013) with some modification to make notations consistent.\n\nDefinition 7 If F: \\mathcal{Y} \\to \\mathbb{R} is a strictly convex differentiable function, then Bregman divergence derived from F is a function D_{F}: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}_{+} defined as: \n    D_{F} (\\mathbf{t}, \\mathbf{y}) = F(\\mathbf{t}) - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\, (\\mathbf{t} - \\mathbf{y}).\n\n\n\nRemark. Given the defintion, Bregman divergence is not symmetric. It does not satisfy the triangle inequality. Thus, it is not a metric.\n\nSome examples of Bregman divergence:\n\nSquared Euclidean distance or square loss: D_{F}(\\mathbf{t}, \\mathbf{y}) = || \\mathbf{t} - \\mathbf{y} ||_{2}^{2} which is derived from the convex function F(\\mathbf{y}) = || \\mathbf{y} ||_{2}^{2}\nThe squared Mahalanobis distance: \n  D_{F}(\\mathbf{t}, \\mathbf{y}) = \\frac{1}{2} (\\mathbf{t} - \\mathbf{y})^{\\top} \\mathbf{Q} (\\mathbf{t} - \\mathbf{y})\n which is generated from the convex function: F(\\mathbf{y}) = \\frac{1}{2} \\mathbf{y}^{\\top} \\mathbf{Q} \\mathbf{y}\nThe KL divergence: \n  D_{F}(\\mathbf{t}, \\mathbf{y}) = \\mathrm{KL} [p(\\mathbf{t} | \\mathbf{x}) || \\mathbf{y}] = \\sum_{c = 1}^{C} p(\\mathbf{t} = \\mathrm{one-hot}(c) | \\mathbf{x}) \\frac{p(\\mathbf{t} = \\mathrm{one-hot}(c) | \\mathbf{x})}{\\mathbf{y}_{c}}\n which is generated from the negative entropy: \n  F(\\mathbf{y}) = \\sum_{c = 1}^{C} \\mathbf{y}_{c} \\ln \\mathbf{y}_{c}.\n\n\n\n5.1 Some properties of Bregman divergence\nThis sub-section presents some properties of Bregman divergence, which can then be used in the bias-variance decomposition. Note that the notation \\mathbf{y}_{*}, \\mathbf{y} and \\mathbf{y}_{m} used in this section do not need to be label distribution, but can simply be the output of a model (without any normalization, e.g. no softmax). The case for label distributions will be considered in the subsequent section where the loss function is KL divergence.\n\nLemma 1 (Part 1 of Lemma 0.1 in (Pfau 2013)) The mean prediction for Bregman divergence with un-bounded support has the following property: \n    \\mathbf{y}_{m} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}^{\\prime}, \\mathbf{y})] \\Leftrightarrow \\nabla F(\\mathbf{y}_{m}) = \\mathbb{E}_{\\mathcal{D}} [ \\nabla F(\\mathbf{y}) ].\n\n\n\n\nDetailed proof\n\n\nProof.  \n\nNecessary\nWhen \\mathbf{y}_{m} is a minimizer of \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}^{\\prime}, \\mathbf{y})] w.r.t. \\mathbf{y}^{\\prime}, the necessary condition of such statement is that its gradient is zero: \n    \\begin{aligned}\n        \\nabla_{\\mathbf{y}_{m}} \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}_{m}, \\mathbf{y})] & = \\nabla_{\\mathbf{y}_{m}} \\mathbb{E}_{\\mathcal{D}} [ F(\\mathbf{y}_{m}) - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\, (\\mathbf{y}_{m} - \\mathbf{y}) ] \\\\\n        & = \\nabla_{\\mathbf{y}_{m}} F(\\mathbf{y}_{m}) - \\nabla_{\\mathbf{y}_{m}} \\mathbb{E}_{\\mathcal{D}} [ \\nabla^{\\top} F(\\mathbf{y}) \\, \\mathbf{y}_{m}]\\\\\n        & = \\nabla_{\\mathbf{y}_{m}} F(\\mathbf{y}_{m}) - \\mathbb{E}_{\\mathcal{D}} [ \\nabla F(\\mathbf{y}) ] = 0.\n    \\end{aligned}\n \n    \\implies \\nabla F(\\mathbf{y}_{m}) = \\mathbb{E}_{\\mathcal{D}} [ \\nabla F(\\mathbf{y}) ].\n\n\n\nSufficient\nSimilar to the necessary condition, one can easily show that \\nabla_{\\mathbf{y}_{m}} F(\\mathbf{y}_{m}) = \\mathbb{E}_{\\mathcal{D}} [ \\nabla F(\\mathbf{y}) ] implies that \\nabla_{\\mathbf{y}_{m}} \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}_{m}, \\mathbf{y})] = 0 (assume that \\mathbf{y}_{m} is independent from \\mathcal{D}). And since D_{F} is convex in its first argument \\mathbf{y}_{m} (one property of Bregman divergence), \\mathbf{y}_{m} is unique and the minimizer of \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}^{\\prime}, \\mathbf{y})].\n\n\nNote\nThe lemma only holds for Bregman divergence with un-bounded support, e.g. F is MSE. Otherwise, the gradient of \\mathbb{E}_{\\mathcal{D}} [D_{F} (\\mathbf{y}_{m}, \\mathbf{y})] w.r.t. the first argument would not be zero, but the Lagrangean that consists of the additional constraints would. This will be presented in the subsequent section where the loss function is the KL divergence.\n\n\n\n\nLemma 2 (Part 2 of Lemma 0.1 in (Pfau 2013)) The optimal prediction of Bregman divergence can be expressed as: \n    \\mathbf{y}_{*} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F} (\\mathbf{t}, \\mathbf{y}^{\\prime})] = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}].\n\n\n\n\nDetailed proof\n\n\nProof. The proof is quite straight-forward. One can calculate the gradient and solve for the root of the gradient as follows: \n    \\begin{aligned}\n        \\nabla_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F} (\\mathbf{t}, \\mathbf{y}^{\\prime})] & = \\nabla_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ F(\\mathbf{t}) - F(\\mathbf{y}^{\\prime}) - \\nabla^{\\top} F(\\mathbf{y}^{\\prime}) \\, (\\mathbf{t} - \\mathbf{y}^{\\prime}) ]\\\\\n        & = - \\nabla_{\\mathbf{y}^{\\prime}} F(\\mathbf{y}^{\\prime}) - \\nabla^{2} F(\\mathbf{y}^{\\prime}) \\times \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] + \\nabla^{2} F(\\mathbf{y}^{\\prime}) \\times \\mathbf{y}^{\\prime} + \\nabla_{\\mathbf{y}^{\\prime}} F(\\mathbf{y}^{\\prime}) \\\\\n        & = \\nabla^{2} F(\\mathbf{y}^{\\prime}) (\\mathbf{y}^{\\prime} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) = 0\n    \\end{aligned}\n And since F(.) is strictly convex, its Hessian matrix \\nabla^{2} F(\\mathbf{y}^{\\prime}) is positive definite and invertible. Hence, one can imply that: \n    \\mathbf{y}^{\\prime} = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}].\n\n\n\n\nLemma 3 (Part 1 of Theorem 0.1 in (Pfau 2013)) The expected Bregman divergences w.r.t. the set of training sets \\mathcal{D} have the following exact decomposition: \n    \\mathbb{E}_{\\mathcal{D}} [ D_{F} (\\mathbf{y}^{\\prime}, \\mathbf{y})] = D_{F}(\\mathbf{y}^{\\prime}, \\mathbf{y}_{m}) + \\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}_{m}, \\mathbf{y})],\n where: \\mathbf{y}_{m} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}^{\\prime}, \\mathbf{y})] is the mean prediction of the model of interest, and \\mathbf{y}^{\\prime} is a (random) prediction that is independent from \\mathcal{D}.\n\n\n\nDetailed proof\n\n\nProof. The is quite straight-forward: \n    \\begin{aligned}\n        & D_{F}(\\mathbf{y}^{\\prime}, \\mathbf{y}_{m}) + \\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}_{m}, \\mathbf{y})] \\\\\n        & = F(\\mathbf{y}^{\\prime}) - F(\\mathbf{y}_{m}) - \\nabla^{\\top} F(\\mathbf{y}_{m}) \\times (\\mathbf{y}^{\\prime} - \\mathbf{y}_{m}) + \\mathbb{E}_{\\mathcal{D}} [F(\\mathbf{y}_{m}) - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbf{y}_{m} - \\mathbf{y})] \\\\\n        & = F(\\mathbf{y}^{\\prime}) - \\nabla^{\\top} F(\\mathbf{y}_{m}) \\times (\\mathbf{y}^{\\prime} - \\mathbf{y}_{m}) - \\mathbb{E}_{\\mathcal{D}} [ F(\\mathbf{y}) + \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbf{y}_{m} - \\mathbf{y})]\\\\\n        & = F(\\mathbf{y}^{\\prime}) - \\mathbb{E}_{\\mathcal{D}} [ \\nabla^{\\top} F(\\mathbf{y}) ] \\times (\\mathbf{y}^{\\prime} - \\mathbf{y}_{m}) - \\mathbb{E}_{\\mathcal{D}} [ F(\\mathbf{y}) + \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbf{y}_{m} - \\mathbf{y})] \\\\\n        & = \\mathbb{E}_{\\mathcal{D}} [ F(\\mathbf{y}^{\\prime}) - F(\\mathbf{y}) - \\mathbb{E}_{\\mathcal{D}} [ \\nabla^{\\top} F(\\mathbf{y}) ] \\times (\\mathbf{y}^{\\prime} - \\mathbf{y}_{m} + \\mathbf{y}_{m} - \\mathbf{y}) ]\\\\\n        & = \\mathbb{E}_{\\mathcal{D}} [ D_{F} (\\mathbf{y}^{\\prime}, \\mathbf{y})].\n    \\end{aligned}\n The third inequality is due to Lemma 1.\n\n\n\nLemma 4 (Part 2 of Theorem 0.1 in (Pfau 2013)) The expected Bregman divergences w.r.t. the underlying label distribution p(\\mathbf{t} | \\mathbf{x}) have the following exact decomposition: \n    \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ D_{F} (\\mathbf{t}, \\mathbf{y})] = D_{F}(\\mathbf{y}_{*}, \\mathbf{y}) + \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F}(\\mathbf{t}, \\mathbf{y}_{*})],\n where \\mathbf{y}_{*} = \\arg\\min_{\\mathbf{y}^{\\prime}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F} (\\mathbf{t}, \\mathbf{y}^{\\prime})] = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] is the optimal prediction in Lemma 2.\n\n\n\nDetailed proof\n\n\nProof. The proof is quite straight-forward: \n    \\begin{aligned}\n        & D_{F}(\\mathbf{y}_{*}, \\mathbf{y}) + \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F}(\\mathbf{t}, \\mathbf{y}_{*})] \\\\\n        & = F(\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbf{y}) \\\\\n        & \\quad + \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [F(\\mathbf{t}) - F(\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) - \\nabla^{\\top} F(\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) \\times (\\mathbf{t} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}])] \\\\\n        & = - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbf{y}) + \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [F(\\mathbf{t}) - \\nabla^{\\top} F(\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}]) \\times (\\mathbf{t} - \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}])] \\\\\n        & = - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [\\mathbf{t}] - \\mathbf{y}) + \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [F(\\mathbf{t})]\\\\\n        & = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ F(\\mathbf{t}) - F(\\mathbf{y}) - \\nabla^{\\top} F(\\mathbf{y}) \\times (\\mathbf{t} - \\mathbf{y})] \\\\\n        & = \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ D_{F} (\\mathbf{t}, \\mathbf{y})].\n    \\end{aligned}\n\n\n\n\n\n5.2 Decomposition for Bregman divergence\nThe main result of bias-variance decomposition can be shown in the following:\n\nTheorem 4 The expected Bregman divergence on a set of training set \\mathcal{D} can be decomposed into: \n\\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F} (\\mathbf{t}, \\mathbf{y})] = \\textcolor{Crimson}{D_{F} (\\mathbf{y}_{*}, \\mathbf{y}_{m})} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}_{m}, \\mathbf{y})]} + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} \\left[ D_{F} (\\mathbf{t}, \\mathbf{y}_{*}) \\right]}.\n\n\n\n\nDetailed proof\n\n\nProof. The proof is a consequence of the previous lemma: \n    \\begin{aligned}\n        \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F} (\\mathbf{t}, \\mathbf{y})] & = \\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}_{*}, \\mathbf{y}) + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F}(\\mathbf{t}, \\mathbf{y}_{*})]} ] \\\\\n        & = \\mathbb{E}_{\\mathcal{D}}[ D_{F}(\\mathbf{y}_{*}, \\mathbf{y})] + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F}(\\mathbf{t}, \\mathbf{y}_{*})]}\\\\\n        & = \\textcolor{Crimson}{D_{F} (\\mathbf{y}_{*}, \\mathbf{y}_{m})} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [D_{F}(\\mathbf{y}_{m}, \\mathbf{y})]} + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [D_{F}(\\mathbf{t}, \\mathbf{y}_{*})]}.\n    \\end{aligned}\n The first equality is due to Lemma 4 and the last equality of the above equation is due to Lemma 3.\n\n\n\n5.2.1 Square loss\nAs MSE or square loss is a special instance of Bregman divergence, one can apply Theorem 4 to obtain the result for MSE as shown in Theorem 1."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#kullback-leibler-divergence",
    "href": "posts/bias-variance-decomposition/index.html#kullback-leibler-divergence",
    "title": "Bias - variance decomposition",
    "section": "6 Kullback-Leibler divergence",
    "text": "6 Kullback-Leibler divergence\nKL divergence is a special case of Bregman divergence. However, the analysis done for the Bregman divergence presented in this post is considered on un-bounded support, where the support space for the KL divergence is the probability space. In addition, KL divergence is used to measure the difference between 2 distributions. Such differences result in a different in terms of bias-variance decomposition.\nIn this section, \\mathbf{y}_{*}, \\mathbf{y} and \\mathbf{y}_{m} are label distributions or probabilities. They will be replaced by p(\\mathbf{t} | \\mathbf{x}), \\hat{p}(\\mathbf{t} | \\mathbf{x}) and p_{m}(\\mathbf{t} | \\mathbf{x}), respectively, to make the formulation easier to understand.\n\nLemma 5 (Main model prediction - Eq. (2.3) in (Heskes 1998)) The main model prediction when the loss is the KL divergence has the following property: \n    p_{m}(\\mathbf{t} | \\mathbf{x}) = \\arg\\min_{q(\\mathbf{t} | \\mathbf{x})} \\mathbb{E}_{\\mathcal{D}} [\\mathrm{KL} [q(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x})]] \\Rightarrow p_{m}(\\mathbf{t} | \\mathbf{x}) = \\frac{1}{Z} \\exp \\left[ \\mathbb{E}_{\\mathcal{D}} [\\ln \\hat{p}(\\mathbf{t} | \\mathbf{x})] \\right],\n where Z is a normalization constant independent of model prediction \\hat{p}(\\mathbf{t} | \\mathbf{x}).\n\n\n\nDetailed proof\n\n\nProof. The proof is similar to Lemma 1, except the constraint \\sum_{\\mathbf{t}} p_{m}(\\mathbf{t} | \\mathbf{x}) = 1 is taken into account. More specifically, the Lagrangean can be written as: \n    \\mathsf{L} = \\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [ p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x})]] + \\lambda (\\pmb{1}^{\\top} p_{m}(\\mathbf{t} | \\mathbf{x}) - 1),\n where \\lambda is the Lagrange multiplier.\nAt the optimal point, the gradient of the Lagrangean is zero: \n    \\begin{aligned}\n        \\nabla_{p_{m}(\\mathbf{t} | \\mathbf{x})} \\mathsf{L} & = \\ln p_{m}(\\mathbf{t} | \\mathbf{x}) - \\mathbb{E}_{\\mathcal{D}} [ \\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] + \\lambda = 0\\\\\n        & \\Rightarrow \\ln p_{m}(\\mathbf{t} | \\mathbf{x}) = \\mathbb{E}_{\\mathcal{D}} [ \\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] - \\lambda\\\\\n        & \\Rightarrow p_{m}(\\mathbf{t} | \\mathbf{x}) = \\underbrace{\\frac{1}{\\exp(\\lambda)}}_{\\frac{1}{Z}} \\exp[\\mathbb{E}_{\\mathcal{D}} [ \\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ]].\n    \\end{aligned}\n Actually, the normalization constant Z is the negative variance: \n    \\ln Z \\times \\pmb{1} = \\mathbb{E}_{\\mathcal{D}} [ \\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] - \\ln p_{m}(\\mathbf{t} | \\mathbf{x})] = \\mathbb{E}_{\\mathcal{D}} \\left[ \\ln \\frac{\\hat{p}(\\mathbf{t} | \\mathbf{x})}{p_{m}(\\mathbf{t} | \\mathbf{x})} \\right].\n Note that: \n    \\ln Z = \\mathbb{E}_{p_{m}(\\mathbf{t} | \\mathbf{x})} [ \\ln Z \\times \\pmb{1}].\n Thus: \n        \\ln Z = \\mathbb{E}_{p_{m}(\\mathbf{t} | \\mathbf{x})} \\mathbb{E}_{\\mathcal{D}} \\left[ \\ln \\frac{\\hat{p}(\\mathbf{t} | \\mathbf{x})}{p_{m}(\\mathbf{t} | \\mathbf{x})} \\right] = - \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} \\left[ \\mathrm{KL} [p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x})] \\right]}.\n\n\n\n\nTheorem 5 (Decomposition for KL divergence) The bias-variance decomposition for KL divergence can be presented as: \n    \\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [p(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x})] ] = \\textcolor{Crimson}{\\mathrm{KL} [ p(\\mathbf{t} | \\mathbf{x}) || p_{m}(\\mathbf{t} | \\mathbf{x}) ]} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [ p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] ]}.\n\n\n\nProof. The proof is quite straight-forward from Lemma 5.\n\nThe result in Theorem 5 does not consist of an intrinsic noise since the loss defined by KL divergence is based on the true label distribution instead of each sample \\mathbf{t}. To obtain the wellknown form of bias-variance decomposition based on label \\mathbf{t}, the negative log likelihood -\\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) is used as the loss function. Note that p_{m}(\\mathbf{t} | \\mathbf{x}) is still defined with KL divergence as the loss function.\nFrom Lemma 5, one can obtain: \n    \\mathbb{E}_{\\mathcal{D}} [ -\\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] = -\\ln p_{m}(\\mathbf{t} | \\mathbf{x}) + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [ p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] ]}.\n\nThus, the negative log-likelihood can be written as: \n    \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ -\\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] = -\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ \\ln p_{m}(\\mathbf{t} | \\mathbf{x})] + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [ p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] ]}.\n\nOr: \n    \\mathbb{E}_{\\mathcal{D}} \\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})} [ -\\ln \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] = \\textcolor{Crimson}{\\mathrm{KL}[p(\\mathbf{t} | \\mathbf{x}) || p_{m}(\\mathbf{t} | \\mathbf{x})]} + \\textcolor{MidnightBlue}{\\mathbb{E}_{\\mathcal{D}} [ \\mathrm{KL} [ p_{m}(\\mathbf{t} | \\mathbf{x}) || \\hat{p}(\\mathbf{t} | \\mathbf{x}) ] ]} + \\textcolor{Green}{\\mathbb{E}_{p(\\mathbf{t} | \\mathbf{x})}[-\\ln p(\\mathbf{t} | \\mathbf{x})]}.\n\nThe bias-variance decomposition for negative log-likelihood in this case consists of an intrinsic noise term which equals to the Shannon entropy of the true label distribution p(\\mathbf{t} | \\mathbf{x})."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#conclusion",
    "href": "posts/bias-variance-decomposition/index.html#conclusion",
    "title": "Bias - variance decomposition",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nIn general, the bias - variance decomposition might not be always in the form of bias, variance and noise as commonly seen in MSE. Here, we show that different loss function might have a different decomposition. Nevertheless, the two most common loss functions, i.e., MSE and KL divergence, share a similar form. Note that, one needs to be careful when applying such bias - variance decomposition due to their difference in terms of main model prediction and optimal label."
  },
  {
    "objectID": "posts/bias-variance-decomposition/index.html#references",
    "href": "posts/bias-variance-decomposition/index.html#references",
    "title": "Bias - variance decomposition",
    "section": "8 References",
    "text": "8 References\n\n\nDomingos, Pedro. 2000. “A Unified Bias-Variance Decomposition.” In International Conference on Machine Learning, 231–38.\n\n\nHeskes, Tom. 1998. “Bias/Variance Decompositions for Likelihood-Based Estimators.” Neural Computation 10 (6): 1425–33.\n\n\nPfau, David. 2013. “A Generalized Bias-Variance Decomposition for Bregman Divergences.” Unpublished Manuscript."
  }
]