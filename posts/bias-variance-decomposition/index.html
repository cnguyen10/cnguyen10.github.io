<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Cuong Nguyen">
<meta name="dcterms.date" content="2022-05-03">

<title>Probabilita ML - Bias - variance decomposition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../robot.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NLRVZL0JSR"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-NLRVZL0JSR', { 'anonymize_ip': true});
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>


  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta name="twitter:title" content="Probabilita ML - Bias - variance decomposition">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://cnguyen10.github.io/posts/bias-variance-decomposition/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Probabilita ML</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> <i class="bi bi-pencil-square" role="img">
</i> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#notations" id="toc-notations" class="nav-link active" data-scroll-target="#notations"><span class="header-section-number">1</span> Notations</a></li>
  <li><a href="#terminologies" id="toc-terminologies" class="nav-link" data-scroll-target="#terminologies"><span class="header-section-number">2</span> Terminologies</a></li>
  <li><a href="#square-loss" id="toc-square-loss" class="nav-link" data-scroll-target="#square-loss"><span class="header-section-number">3</span> Square loss</a></li>
  <li><a href="#loss" id="toc-loss" class="nav-link" data-scroll-target="#loss"><span class="header-section-number">4</span> 0-1 loss</a>
  <ul class="collapse">
  <li><a href="#binary-classification" id="toc-binary-classification" class="nav-link" data-scroll-target="#binary-classification"><span class="header-section-number">4.1</span> Binary classification</a></li>
  <li><a href="#multi-class-classification" id="toc-multi-class-classification" class="nav-link" data-scroll-target="#multi-class-classification"><span class="header-section-number">4.2</span> Multi-class classification</a></li>
  </ul></li>
  <li><a href="#bregman-divergence" id="toc-bregman-divergence" class="nav-link" data-scroll-target="#bregman-divergence"><span class="header-section-number">5</span> Bregman divergence</a>
  <ul class="collapse">
  <li><a href="#some-properties-of-bregman-divergence" id="toc-some-properties-of-bregman-divergence" class="nav-link" data-scroll-target="#some-properties-of-bregman-divergence"><span class="header-section-number">5.1</span> Some properties of Bregman divergence</a></li>
  <li><a href="#decomposition-for-bregman-divergence" id="toc-decomposition-for-bregman-divergence" class="nav-link" data-scroll-target="#decomposition-for-bregman-divergence"><span class="header-section-number">5.2</span> Decomposition for Bregman divergence</a></li>
  </ul></li>
  <li><a href="#kullback-leibler-divergence" id="toc-kullback-leibler-divergence" class="nav-link" data-scroll-target="#kullback-leibler-divergence"><span class="header-section-number">6</span> Kullback-Leibler divergence</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">7</span> Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">8</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Bias - variance decomposition</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Cuong Nguyen <a href="https://orcid.org/0000-0003-2672-6291" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 3, 2022</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Bias and variance decomposition is one of the key tools to understand machine learning. However, conventional discussion about bias - variance decomposition revolves around the square loss (also known as mean square error). It is unclear whether such decomposition is still valid for some common loss functions, such as 0-1 loss or cross-entropy loss used in classification. This post is to present the decomposition for those losses following the <em>unified</em> framework of bias and variance decomposition from <span class="citation" data-cites="domingos2000unified">(<a href="#ref-domingos2000unified" role="doc-biblioref">Domingos 2000</a>)</span>, its extended study on <em>Bregman divergence</em> with <em>un-bounded support</em> from <span class="citation" data-cites="pfau2013generalized">(<a href="#ref-pfau2013generalized" role="doc-biblioref">Pfau 2013</a>)</span> and the special case about Kullback-Leibler (KL) divergence <span class="citation" data-cites="heskes1998bias">(<a href="#ref-heskes1998bias" role="doc-biblioref">Heskes 1998</a>)</span>.</p>
<section id="notations" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="notations"><span class="header-section-number">1</span> Notations</h2>
<p>The notations are similar to the ones in <span class="citation" data-cites="domingos2000unified">(<a href="#ref-domingos2000unified" role="doc-biblioref">Domingos 2000</a>)</span>, but for <span class="math inline">C</span>-class classification.</p>
<table class="table-striped table-hover caption-top table">
<caption>Notations used in the bias-variance decomposition.</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th>Notation</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\mathbf{x}</span></td>
<td>an input instance in <span class="math inline">\mathcal{X} \subseteq \in \mathbb{R}^{d}</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\Delta_{K}</span></td>
<td>the <span class="math inline">K</span>-dimensional simplex <span class="math inline">\equiv \{\mathbf{v} \in \mathbb{R}^{K + 1}_{+}: \mathbf{v}^{\top} \pmb{1} = 1\}</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\Delta_{K}</span></td>
<td>the <span class="math inline">K</span>-dimensional simplex <span class="math inline">\equiv \{\mathbf{v} \in \mathbb{R}^{K + 1}_{+}: \mathbf{v}^{\top} \pmb{1} = 1\}</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\mathbf{t}</span></td>
<td>a label instance: <span class="math inline">\mathbf{t} \sim p(\mathbf{t} | \mathbf{x})</span>, for example: (i) one-hot vector if <span class="math inline">p(\mathbf{t} | \mathbf{x})</span> is a categorical distribution, or (ii) soft-label if <span class="math inline">p(\mathbf{t} | \mathbf{x})</span> is a Dirichlet or logistic normal distribution</td>
</tr>
<tr class="odd">
<td><span class="math inline">\ell</span></td>
<td>loss function <span class="math inline">\ell: \Delta_{C - 1} \times \Delta_{C - 1} \to [0, +\infty]</span>, e.g.&nbsp;0-1 loss or cross-entropy loss</td>
</tr>
<tr class="even">
<td><span class="math inline">\mathbf{y}</span></td>
<td>predicted label distribution: <span class="math inline">\mathbf{y} = f(\mathbf{x}) \in \Delta_{C - 1}</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\mathcal{D}</span></td>
<td>the set of training sets</td>
</tr>
</tbody>
</table>
</section>
<section id="terminologies" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="terminologies"><span class="header-section-number">2</span> Terminologies</h2>
<div id="def-optimal-prediction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1</strong></span> The optimal prediction <span class="math inline">\mathbf{y}_{*} \in \Delta_{C - 1}</span> of a target <span class="math inline">\mathbf{t}</span> is defined as follows: <span class="math display">
    \mathbf{y}_{*} = \arg\min_{\mathbf{y}^{\prime}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} \left[ \ell \left( \mathbf{t}, \mathbf{y}^{\prime} \right) \right].
</span></p>
</div>
<div id="def-main-model-prediction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2</strong></span> The main model prediction for a loss function, <span class="math inline">\ell</span>, and the set of training sets, <span class="math inline">\mathcal{D}</span>, is defined as: <span class="math display">
    \mathbf{y}_{m} = \arg\min_{\mathbf{y}^{\prime}} \mathbb{E}_{\mathcal{D}} \left[ \ell \left(\mathbf{y}, \mathbf{y}^{\prime} \right) \right].
</span></p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>The defintions of <em>optimal</em> and <em>main model</em> predictions above assume that the loss function <span class="math inline">\ell</span> is symmetric in terms of the input arguments. For asymmetric loss function, such as Bregmand divergence or cross-entropy, the definitions of such predictions might be slightly changed at the order of the input arguments.</p>
</div>
<p>Given the definitions of <span class="math inline">\mathbf{y}_{*}</span> and <span class="math inline">\mathbf{y}_{m}</span>, the bias, variance and noise can be defined following the <em>unified</em> framework proposed in <span class="citation" data-cites="domingos2000unified">(<a href="#ref-domingos2000unified" role="doc-biblioref">Domingos 2000</a>)</span> as follows:</p>
<div id="def-bias" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3</strong></span> The bias of a learner on an example <span class="math inline">\mathbf{x}</span> is defined as: <span class="math inline">B(\mathbf{x}) = \ell \left( \mathbf{y}_{*}, \mathbf{y}_{m} \right)</span>.</p>
</div>
<div id="def-variance" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4</strong></span> The variance of a learner on an example <span class="math inline">\mathbf{x}</span> is defined as: <span class="math inline">V(\mathbf{x}) = \mathbb{E}_{\mathcal{D}} \left[ \ell \left( \mathbf{y}_{m}, \mathbf{y} \right) \right]</span>.</p>
</div>
<div id="def-noise" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5</strong></span> The noise of an example <span class="math inline">\mathbf{x}</span> is defined as: <span class="math inline">N(\mathbf{x}) = \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} \left[ \ell(\mathbf{t}, \mathbf{y}_{*}) \right]</span>.</p>
</div>
<p>The definitions of bias and variance above are quite intuitive comparing to other definitions in the literature. As <span class="math inline">\mathbf{y}_{m}</span> is the <em>main</em> model prediction, the bias <span class="math inline">B(\mathbf{x})</span> measures the systematic deviation (loss) from the <em>optimal</em> (or true) label <span class="math inline">\mathbf{y}_{*}</span>, while the variance <span class="math inline">V(\mathbf{x})</span> measures the loss induced due to the fluctuations of each model prediction <span class="math inline">\mathbf{y}</span> on different training datasets around the <em>main</em> prediction <span class="math inline">\mathbf{y}_{m}</span>. In addition, as the loss <span class="math inline">\ell</span> is non-negative, both the bias and variance are also non-negative.</p>
<p>Given the defintions of bias, variance and noise above, the unified decomposition proposed in <span class="citation" data-cites="domingos2000unified">(<a href="#ref-domingos2000unified" role="doc-biblioref">Domingos 2000</a>)</span> can be expressed as: <span id="eq-unified_decomposition"><span class="math display">
    \begin{aligned}
        \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y})] &amp; = \textcolor{Crimson}{\ell(\mathbf{y}_{*}, \mathbf{y}_{m})} + c_{1} \, \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}}[\ell(\mathbf{y}, \mathbf{y}_{m})]} + c_{2} \, \textcolor{Green}{\mathbb{E}_{p(\mathbf{t} | \mathbf{x})}[\ell(\mathbf{t}, \mathbf{y_{*}})]} \\
        &amp; = \textcolor{Crimson}{B(\mathbf{x})} + c_{1} \, \textcolor{MidnightBlue}{V(\mathbf{x})} + c_{2} \, \textcolor{Green}{N(\mathbf{x})},
    \end{aligned}
\tag{1}</span></span> where <span class="math inline">c_{1}</span> and <span class="math inline">c_{2}</span> are two scalars. For example, in MSE, <span class="math inline">c_{1} = c_{2} = 1</span>.</p>
<p>Of course, not all losses would satisfy the decomposition in <a href="#eq-unified_decomposition" class="quarto-xref">Equation&nbsp;1</a>. However, as shown in <span class="citation" data-cites="domingos2000unified">(<a href="#ref-domingos2000unified" role="doc-biblioref">Domingos 2000</a> - Theorem 7)</span>, such decomposition can be used to bound the expected loss as long as the loss is metric. Nevertheless, in this post, we dicuss the composition on some common loss functions, such as 0-1 loss and Bregman divergence which includes MSE and Kullback-Leibler (KL) divergence.</p>
</section>
<section id="square-loss" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="square-loss"><span class="header-section-number">3</span> Square loss</h2>
<p>To warm-up, we discuss a wellknown bias-variance decomposition in the literature. It is applied for MSE or square loss. Here, we use the notations of vectors instead of scalars as often seen in conventional analysis. We will derive a general decomposition for Bregman divergence in which MSE is a particular case in a later section.</p>
<div id="thm-mse" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1</strong></span> When the loss is the square loss: <span class="math inline">\ell(\mathbf{y}_{1}, \mathbf{y}_{2}) = || \mathbf{y}_{1} - \mathbf{y}_{2}||_{2}^{2}</span>, then the expected loss on several training sets can be decomposed into: <span class="math display">
    \begin{aligned}
        \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} \ell(\mathbf{t}, \mathbf{y}) &amp; = \textcolor{Crimson}{\ell(\mathbf{y}_{*}, \mathbf{y}_{m})} + \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} [ \ell(\mathbf{y}_{m}, \mathbf{y})]} + \textcolor{Green}{\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ \ell( \mathbf{t}, \mathbf{y}_{*} )]} \\
        \text{or: } \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} || \mathbf{t} - \mathbf{y} ||_{2}^{2} &amp; = \underbrace{\textcolor{Crimson}{|| \mathbf{y}_{*} - \mathbf{y}_{m} ||_{2}^{2}}}_{\text{bias}} + \underbrace{\textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} || \mathbf{y}_{m} - \mathbf{y} ||_{2}^{2}}}_{\text{variance}} + \underbrace{\textcolor{Green}{\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} || \mathbf{t} - \mathbf{y}_{*} ||_{2}^{2}}}_{\text{noise}}.
    \end{aligned}
</span></p>
</div>
<details>
<summary>
Please refer to the detailed proof here
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Given the square loss, the <em>optimal</em> prediction can be determined as: <span class="math display">
    \begin{aligned}
        &amp; \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} || \mathbf{t} - \mathbf{y}^{\prime} ||_{2}^{2} \ge || \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} \left[ \mathbf{t} \right] - \mathbf{y}^{\prime} ||_{2}^{2} \ge 0 \quad \text{(Jensen's inequality on L2-norm)}\\
        \implies &amp; \mathbf{y}_{*} = \arg\min_{\mathbf{y}^{\prime}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} || \mathbf{t} - \mathbf{y}^{\prime} ||_{2}^{2} = \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}].
    \end{aligned}
</span> Similarly, the <em>main</em> model prediction can be obtained as: <span class="math inline">\mathbf{y}_{m} = \mathbb{E}_{\mathcal{D}} [\mathbf{y}]</span>.</p>
<p>The expected loss can then be written as: <span class="math display">
    \begin{aligned}
        &amp; \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} || \mathbf{t} - \mathbf{y} ||_{2}^{2} \\
        &amp; = \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} (\mathbf{t} - \mathbf{y})^{\top} (\mathbf{t} - \mathbf{y}) \\
        &amp; = \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} \left( (\mathbf{t} - \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}]) + (\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}] - \mathbb{E}_{\mathcal{D}} [\mathbf{y}]) + (\mathbb{E}_{\mathcal{D}} [\mathbf{y}] - \mathbf{y}) \right)^{\top} \left( (\mathbf{t} - \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}]) \right. \\
        &amp; \quad \left. + (\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}] - \mathbb{E}_{\mathcal{D}} [\mathbf{y}]) + (\mathbb{E}_{\mathcal{D}} [\mathbf{y}] - \mathbf{y}) \right) \\
        &amp; = \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} || \mathbf{t} - \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}] ||_{2}^{2} + || \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}] - \mathbb{E}_{\mathcal{D}} [\mathbf{y}] ||_{2}^{2} + \mathbb{E}_{\mathcal{D}} || \mathbb{E}_{\mathcal{D}} [\mathbf{y}] - \mathbf{y} ||_{2}^{2} \\
        &amp; = \textcolor{Green}{\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} || \mathbf{t} - \mathbf{y}_{*} ||_{2}^{2}} + \textcolor{Crimson}{|| \mathbf{y}_{*} - \mathbf{y}_{m} ||_{2}^{2}} + \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} || \mathbf{y}_{m} - \mathbf{y} ||_{2}^{2}}.
    \end{aligned}
</span></p>
</div>
</details>
</section>
<section id="loss" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="loss"><span class="header-section-number">4</span> 0-1 loss</h2>
<div id="def-0-1-loss" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6</strong></span> The 0-1 loss is defined as: <span class="math display">
    \ell(\mathbf{y}_{1}, \mathbf{y}_{2}) = \Bbb{1} (\mathbf{y}_{1}, \mathbf{y}_{2}) = \begin{cases}
        0 &amp; \text{if } \mathbf{y}_{1} = \mathbf{y}_{2},\\
        1 &amp; \text{if } \mathbf{y}_{1} \neq \mathbf{y}_{2}.
    \end{cases}
</span></p>
</div>
<section id="binary-classification" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="binary-classification"><span class="header-section-number">4.1</span> Binary classification</h3>
<div id="thm-binary-0-1-loss" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2</strong></span> (<span class="citation" data-cites="domingos2000unified">(<a href="#ref-domingos2000unified" role="doc-biblioref">Domingos 2000</a> - Theorem 2)</span>) The expected 0-1 loss in a <strong>binary classification</strong> setting can be written as: <span class="math display">
    \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} \left[ \ell(\mathbf{t}, \mathbf{y}) \right] = \textcolor{Crimson}{\ell(\mathbf{y}_{*}, \mathbf{y}_{m})} + \textcolor{Brown}{c} \, \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} \left[ \mathbf{y}, \mathbf{y}_{m} \right]} + \left[ 2 p_{\mathcal{D}}(\mathbf{y} = \mathbf{y}_{*}) - 1 \right]  \textcolor{Green}{\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} \left[ \ell(\mathbf{t}, \mathbf{y}_{*}) \right]},
</span> where: <span class="math display">
    \textcolor{Brown}{c} = \begin{cases}
        1 &amp; \text{if } \mathbf{y}_{m} = \mathbf{y}_{*}\\
        -1 &amp; \text{otherwise}.
    \end{cases}
</span></p>
</div>
<details>
<summary>
The proof is copied in <span class="citation" data-cites="domingos2000unified">(<a href="#ref-domingos2000unified" role="doc-biblioref">Domingos 2000</a> - Theorem 2)</span> for a self-contained discussion.
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To prove the theorem, we calculate <span class="math inline">\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y})]</span> and <span class="math inline">\mathbb{E}_{\mathcal{D}} [\ell(\mathbf{t}, \mathbf{y})]</span>, then combine both of them to complete the proof.</p>
<p>First, we proceed to prove the followings: <span id="eq-expected_01_wrt_t"><span class="math display">
    \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y})] = \ell(\mathbf{y}_{*}, \mathbf{y}) + c_{0} \, \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y}_{*})],
\tag{2}</span></span> with <span class="math inline">c_{0} = 1</span> if <span class="math inline">\mathbf{y} = \mathbf{y}_{*}</span> and <span class="math inline">c_{0} = -1</span> if <span class="math inline">\mathbf{y} \neq \mathbf{y}_{*}</span>.</p>
<p>If <span class="math inline">\mathbf{y} = \mathbf{y}_{*}</span>, then <a href="#eq-expected_01_wrt_t" class="quarto-xref">Equation&nbsp;2</a> is trivially true with <span class="math inline">c_{0} = 1</span>. We next prove <a href="#eq-expected_01_wrt_t" class="quarto-xref">Equation&nbsp;2</a> when <span class="math inline">\mathbf{y} \neq \mathbf{y}_{*}</span>. Since there are only two classes, if <span class="math inline">\mathbf{y} \neq \mathbf{y}_{*}</span> and <span class="math inline">\mathbf{t} \neq \mathbf{y}_{*}</span>, then <span class="math inline">\mathbf{y} = \mathbf{t}</span> and vice versa. And since two events are quivalent, <span class="math inline">p(\mathbf{y} = \mathbf{t}) = p(\mathbf{t} \neq \mathbf{y}_{*})</span>. The expected 0-1 loss w.r.t. <span class="math inline">\mathbf{t}</span> can be written as: <span class="math display">
    \begin{aligned}
        \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y})] &amp; = p(\mathbf{t} = \mathbf{y})\\
        &amp; = 1 - p(\mathbf{t} \neq \mathbf{y}) \\
        &amp; = 1 - p (\mathbf{t} = \mathbf{y}_{*}) \\
        &amp; = 1 - \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ \ell(\mathbf{t}, \mathbf{y}_{*}) ]\\
        &amp; = \ell(\mathbf{y}_{*}, \mathbf{y}) - \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ \ell(\mathbf{t}, \mathbf{y}_{*}) ].
    \end{aligned}
</span> This proves <a href="#eq-expected_01_wrt_t" class="quarto-xref">Equation&nbsp;2</a>.</p>
<p>Next, we show that: <span id="eq-expected_01_wrt_D"><span class="math display">
    \mathbb{E}_{\mathcal{D}} [\ell(\mathbf{y}_{*}, \mathbf{y})] = \ell(\mathbf{y}_{*}, \mathbf{y}_{m}) + \textcolor{Brown}{c} \, \mathbb{E}_{\mathcal{D}} [\ell(\mathbf{y}, \mathbf{y}_{m})].
\tag{3}</span></span></p>
<p>If <span class="math inline">\mathbf{y}_{m} = \mathbf{y}_{*}</span>, then <a href="#eq-expected_01_wrt_D" class="quarto-xref">Equation&nbsp;3</a> is trivially true with <span class="math inline">\textcolor{Brown}{c} = 1</span>. If <span class="math inline">\mathbf{y}_{m} \neq \mathbf{y}_{*}</span>, then <span class="math inline">\mathbf{y}_{m} \neq \mathbf{y}</span> implies that <span class="math inline">\mathbf{y} = \mathbf{y}_{*}</span> and vice-versa. Thus, the expected 0-1 loss w.r.t. different training set can be expressed as: <span class="math display">
    \begin{aligned}
        \mathbb{E}_{\mathcal{D}} [\ell(\mathbf{y}_{*}, \mathbf{y})] &amp; = p(\mathbf{y} \neq \mathbf{y}_{*}) = 1 - p(\mathbf{y} = \mathbf{y}_{*}) = 1 - p(\mathbf{y}_{m} \neq \mathbf{y})\\
        &amp; = 1 - \mathbb{E}_{\mathcal{D}} [\ell(\mathbf{y}_{m}, \mathbf{y})] = \ell(\mathbf{y}_{*}, \mathbf{y}_{m}) - \mathbb{E}_{\mathcal{D}} [\ell(\mathbf{y}_{m}, \mathbf{y})].
    \end{aligned}
</span></p>
<p>Thus, it proves <a href="#eq-expected_01_wrt_D" class="quarto-xref">Equation&nbsp;3</a>.</p>
<p>Finally, we can combine both results in <a href="#eq-expected_01_wrt_t" class="quarto-xref">Equation&nbsp;2</a> and <a href="#eq-expected_01_wrt_D" class="quarto-xref">Equation&nbsp;3</a> to prove the theorem. Taking the expectation w.r.t. <span class="math inline">\mathcal{D}</span> on both sides of <a href="#eq-expected_01_wrt_t" class="quarto-xref">Equation&nbsp;2</a> gives: <span class="math display">
    \begin{aligned}
        \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} \left[ \ell(\mathbf{t}, \mathbf{y}) \right] &amp; = \mathbb{E}_{\mathcal{D}} [\ell(\mathbf{t}, \mathbf{y})] + c_{0} \, \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y}_{*})]\\
        &amp; = \mathbb{E}_{\mathcal{D}} [\ell(\mathbf{t}, \mathbf{y})] + c_{0} \, \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y}_{*})].
    \end{aligned}
</span></p>
<p>And since: <span class="math display">
    \begin{aligned}
        \mathbb{E}_{\mathcal{D}} [c_{0}] &amp; = p(\mathbf{y} = \mathbf{y}_{*}) - p (\mathbf{y} \neq \mathbf{y}_{*} = 2 p(\mathbf{y} = \mathbf{y}_{*}) - 1,
    \end{aligned}
</span> we can then obtain the result of the theorem by using <a href="#eq-expected_01_wrt_D" class="quarto-xref">Equation&nbsp;3</a>.</p>
</div>
</details>
</section>
<section id="multi-class-classification" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="multi-class-classification"><span class="header-section-number">4.2</span> Multi-class classification</h3>
<div id="thm-multiclass-0-1-loss" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3</strong></span> The expected loss for 0-1 loss in a multiclass classification can be decomposed into: <span class="math display">
    \begin{aligned}
        &amp; \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} \left[ \ell(\mathbf{t}, \mathbf{y}) \right] = \ell(\mathbf{y}_{*}, \mathbf{y}_{m}) + \textcolor{Blue}{c} \, \mathbb{E}_{\mathcal{D}} \left[ \mathbf{y}, \mathbf{y}_{m} \right] \\
        &amp; \quad + [ 2 p_{\mathcal{D}} (\mathbf{y} = \mathbf{y}_{*}) - p_{\mathcal{D}} (\mathbf{y} \neq \mathbf{y}_{*}) p_{\mathbf{t}}(\mathbf{y} = \mathbf{t} | \mathbf{y}_{*} \neq \mathbf{t}) ]  \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ \ell(\mathbf{t}, \mathbf{y}_{*}) ],
    \end{aligned}
</span> where: <span class="math display">
    c = \begin{cases}
        1 &amp; \text{if } \mathbf{y}_{m} = \mathbf{y}_{*}\\
        -p_{\mathcal{D}} (\mathbf{y} = \mathbf{y}_{*} | \mathbf{y} \neq \mathbf{y}_{m}) &amp; \text{otherwise}.
    \end{cases}
</span></p>
</div>
<details>
<summary>
The proof is copied in <span class="citation" data-cites="domingos2000unified">(<a href="#ref-domingos2000unified" role="doc-biblioref">Domingos 2000</a> - Theorem 3)</span> for a self-contained discussion.
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is similar to the binary classification where we decompose <span class="math inline">\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y})]</span> and <span class="math inline">\mathbb{E}_{\mathcal{D}} [\ell(\mathbf{t}, \mathbf{y})]</span>. The key difference is that when <span class="math inline">\mathbf{y} \neq \mathbf{y}_{*}</span> and <span class="math inline">\mathbf{t} \neq \mathbf{y}_{*}</span> no longer imply that <span class="math inline">\mathbf{y} = \mathbf{t}</span>. Similarly, <span class="math inline">\mathbf{y}_{m} \neq \mathbf{y}_{*}</span> and <span class="math inline">\mathbf{y}_{m} \neq \mathbf{y}</span> no longer imply <span class="math inline">\mathbf{y} = \mathbf{y}_{*}</span>.</p>
<p>Now, we want to prove the following decomposition: <span id="eq-expected_01_wrt_t_multiclass"><span class="math display">
    \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y})] = \ell(\mathbf{y}_{*}, \mathbf{y}) + c_{0} \, \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y}_{*})],
\tag{4}</span></span> where: <span class="math display">
    c_{0} = \begin{cases}
        -p(\mathbf{y} = \mathbf{t} | \mathbf{y}_{*} \neq \mathbf{t}) &amp; \text{when } \mathbf{y} \neq \mathbf{y}_{*}\\
        1 &amp; \text{when } \mathbf{y} = \mathbf{y}_{*}.
    \end{cases}
</span></p>
<p>When <span class="math inline">\mathbf{y} = \mathbf{y}_{*}</span>, <a href="#eq-expected_01_wrt_t_multiclass" class="quarto-xref">Equation&nbsp;4</a> is trivially true with <span class="math inline">c_{0} = 1</span>.</p>
<p>When <span class="math inline">\mathbf{y} \neq \mathbf{y}_{*}</span>, the following fact is true: <span class="math inline">p(\mathbf{y} = \mathbf{t}| \mathbf{y}_{*} = \mathbf{t}, \mathbf{y} \neq \mathbf{y}_{*}) = 0</span>. To simplify the notation, the condition <span class="math inline">\mathbf{y} \neq \mathbf{y}_{*}</span> is omitted. Thus, applying the sum rule on the probability of predicted label gives: <span class="math display">
    \begin{aligned}
        p(\mathbf{y} = \mathbf{t}) &amp; = \underbrace{p(\mathbf{y} = \mathbf{t} | \mathbf{y}_{*} = \mathbf{t})}_{0} \, p(\mathbf{y}_{*} + \mathbf{t}) + p(\mathbf{y} = \mathbf{t} | \mathbf{y}_{*} \neq \mathbf{t}) \, p(\mathbf{y}_{*} \neq \mathbf{t}) \\
        &amp; = p(\mathbf{y} = \mathbf{t} | \mathbf{y}_{*} \neq \mathbf{t}) \, p(\mathbf{y}_{*} \neq \mathbf{t}).
    \end{aligned}
</span></p>
<p>The expected loss w.r.t. <span class="math inline">\mathbf{t}</span> can be written as: <span class="math display">
    \begin{aligned}
        \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y})] &amp; = p(\mathbf{y} \neq \mathbf{t}) = 1 - p(\mathbf{y} = \mathbf{t})\\
        &amp; = 1 \underbrace{- p(\mathbf{y} = \mathbf{t} | \mathbf{y}_{*} \neq \mathbf{t})}_{c_{0}} \, p(\mathbf{y}_{*} \neq \mathbf{t})\\
        &amp; = \ell(\mathbf{y}_{*}, \mathbf{y}) + c_{0} \, \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\ell(\mathbf{t}, \mathbf{y}_{*})].
    \end{aligned}
</span> This proves <a href="#eq-expected_01_wrt_t_multiclass" class="quarto-xref">Equation&nbsp;4</a>.</p>
<p>Similarly, one can prove the decomposition for the expected loss w.r.t. <span class="math inline">\mathcal{D}</span>: <span id="eq-expected_01_wrt_D_multiclass"><span class="math display">
    \mathbb{E}_{\mathcal{D}} [\ell(\mathbf{y}_{*}, \mathbf{y})] = \ell(\mathbf{y}_{*}, \mathbf{y}_{m}) + \textcolor{Brown}{c} \, \mathbb{E}_{\mathcal{D}} [\ell(\mathbf{y}, \mathbf{y}_{m})].
\tag{5}</span></span></p>
<p>Combining the results in <a href="#eq-expected_01_wrt_t_multiclass" class="quarto-xref">Equation&nbsp;4</a> and <a href="#eq-expected_01_wrt_D_multiclass" class="quarto-xref">Equation&nbsp;5</a> in a similar manner in the case of binary classification completes the proof.</p>
</div>
</details>
</section>
</section>
<section id="bregman-divergence" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="bregman-divergence"><span class="header-section-number">5</span> Bregman divergence</h2>
<p>The derivation and discussion in this section is extracted from <span class="citation" data-cites="pfau2013generalized">(<a href="#ref-pfau2013generalized" role="doc-biblioref">Pfau 2013</a>)</span> with some modification to make notations consistent.</p>
<div id="def-bregman-divergence" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7</strong></span> If <span class="math inline">F: \mathcal{Y} \to \mathbb{R}</span> is a strictly convex differentiable function, then Bregman divergence derived from <span class="math inline">F</span> is a function <span class="math inline">D_{F}: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}_{+}</span> defined as: <span class="math display">
    D_{F} (\mathbf{t}, \mathbf{y}) = F(\mathbf{t}) - F(\mathbf{y}) - \nabla^{\top} F(\mathbf{y}) \, (\mathbf{t} - \mathbf{y}).
</span></p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Given the defintion, Bregman divergence is not symmetric. It does not satisfy the triangle inequality. Thus, it is not a metric.</p>
</div>
<p>Some examples of Bregman divergence:</p>
<ul>
<li>Squared Euclidean distance or square loss: <span class="math inline">D_{F}(\mathbf{t}, \mathbf{y}) = || \mathbf{t} - \mathbf{y} ||_{2}^{2}</span> which is derived from the convex function <span class="math inline">F(\mathbf{y}) = || \mathbf{y} ||_{2}^{2}</span></li>
<li>The squared Mahalanobis distance: <span class="math display">
  D_{F}(\mathbf{t}, \mathbf{y}) = \frac{1}{2} (\mathbf{t} - \mathbf{y})^{\top} \mathbf{Q} (\mathbf{t} - \mathbf{y})
</span> which is generated from the convex function: <span class="math inline">F(\mathbf{y}) = \frac{1}{2} \mathbf{y}^{\top} \mathbf{Q} \mathbf{y}</span></li>
<li>The KL divergence: <span class="math display">
  D_{F}(\mathbf{t}, \mathbf{y}) = \mathrm{KL} [p(\mathbf{t} | \mathbf{x}) || \mathbf{y}] = \sum_{c = 1}^{C} p(\mathbf{t} = \mathrm{one-hot}(c) | \mathbf{x}) \frac{p(\mathbf{t} = \mathrm{one-hot}(c) | \mathbf{x})}{\mathbf{y}_{c}}
</span> which is generated from the negative entropy: <span class="math display">
  F(\mathbf{y}) = \sum_{c = 1}^{C} \mathbf{y}_{c} \ln \mathbf{y}_{c}.
</span></li>
</ul>
<section id="some-properties-of-bregman-divergence" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="some-properties-of-bregman-divergence"><span class="header-section-number">5.1</span> Some properties of Bregman divergence</h3>
<p>This sub-section presents some properties of Bregman divergence, which can then be used in the bias-variance decomposition. Note that the notation <span class="math inline">\mathbf{y}_{*}, \mathbf{y}</span> and <span class="math inline">\mathbf{y}_{m}</span> used in this section do not need to be label distribution, but can simply be the output of a model (without any normalization, e.g.&nbsp;no <em>softmax</em>). The case for label distributions will be considered in the subsequent section where the loss function is KL divergence.</p>
<div id="lem-bregman-mean-prediction" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1</strong></span> (Part 1 of Lemma 0.1 in <span class="citation" data-cites="pfau2013generalized">(<a href="#ref-pfau2013generalized" role="doc-biblioref">Pfau 2013</a>)</span>) The <em>mean prediction</em> for Bregman divergence with <strong>un-bounded support</strong> has the following property: <span class="math display">
    \mathbf{y}_{m} = \arg\min_{\mathbf{y}^{\prime}} \mathbb{E}_{\mathcal{D}} [D_{F} (\mathbf{y}^{\prime}, \mathbf{y})] \Leftrightarrow \nabla F(\mathbf{y}_{m}) = \mathbb{E}_{\mathcal{D}} [ \nabla F(\mathbf{y}) ].
</span></p>
</div>
<details>
<summary>
Detailed proof
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>&nbsp;</p>
<section id="necessary" class="level4" data-number="5.1.1">
<h4 data-number="5.1.1" class="anchored" data-anchor-id="necessary"><span class="header-section-number">5.1.1</span> Necessary</h4>
<p>When <span class="math inline">\mathbf{y}_{m}</span> is a minimizer of <span class="math inline">\mathbb{E}_{\mathcal{D}} [D_{F} (\mathbf{y}^{\prime}, \mathbf{y})]</span> w.r.t. <span class="math inline">\mathbf{y}^{\prime}</span>, the necessary condition of such statement is that its gradient is zero: <span class="math display">
    \begin{aligned}
        \nabla_{\mathbf{y}_{m}} \mathbb{E}_{\mathcal{D}} [D_{F} (\mathbf{y}_{m}, \mathbf{y})] &amp; = \nabla_{\mathbf{y}_{m}} \mathbb{E}_{\mathcal{D}} [ F(\mathbf{y}_{m}) - F(\mathbf{y}) - \nabla^{\top} F(\mathbf{y}) \, (\mathbf{y}_{m} - \mathbf{y}) ] \\
        &amp; = \nabla_{\mathbf{y}_{m}} F(\mathbf{y}_{m}) - \nabla_{\mathbf{y}_{m}} \mathbb{E}_{\mathcal{D}} [ \nabla^{\top} F(\mathbf{y}) \, \mathbf{y}_{m}]\\
        &amp; = \nabla_{\mathbf{y}_{m}} F(\mathbf{y}_{m}) - \mathbb{E}_{\mathcal{D}} [ \nabla F(\mathbf{y}) ] = 0.
    \end{aligned}
</span> <span class="math display">
    \implies \nabla F(\mathbf{y}_{m}) = \mathbb{E}_{\mathcal{D}} [ \nabla F(\mathbf{y}) ].
</span></p>
</section>
<section id="sufficient" class="level4" data-number="5.1.2">
<h4 data-number="5.1.2" class="anchored" data-anchor-id="sufficient"><span class="header-section-number">5.1.2</span> Sufficient</h4>
<p>Similar to the necessary condition, one can easily show that <span class="math inline">\nabla_{\mathbf{y}_{m}} F(\mathbf{y}_{m}) = \mathbb{E}_{\mathcal{D}} [ \nabla F(\mathbf{y}) ]</span> implies that <span class="math inline">\nabla_{\mathbf{y}_{m}} \mathbb{E}_{\mathcal{D}} [D_{F} (\mathbf{y}_{m}, \mathbf{y})] = 0</span> (assume that <span class="math inline">\mathbf{y}_{m}</span> is independent from <span class="math inline">\mathcal{D}</span>). And since <span class="math inline">D_{F}</span> is convex in its first argument <span class="math inline">\mathbf{y}_{m}</span> (one property of Bregman divergence), <span class="math inline">\mathbf{y}_{m}</span> is unique and the minimizer of <span class="math inline">\mathbb{E}_{\mathcal{D}} [D_{F} (\mathbf{y}^{\prime}, \mathbf{y})]</span>.</p>
</section>
<section id="note" class="level4" data-number="5.1.3">
<h4 data-number="5.1.3" class="anchored" data-anchor-id="note"><span class="header-section-number">5.1.3</span> Note</h4>
<p>The lemma only holds for Bregman divergence with <b>un-bounded support</b>, e.g.&nbsp;<span class="math inline">F</span> is MSE. Otherwise, the gradient of <span class="math inline">\mathbb{E}_{\mathcal{D}} [D_{F} (\mathbf{y}_{m}, \mathbf{y})]</span> w.r.t. the first argument would not be zero, but the Lagrangean that consists of the additional constraints would. This will be presented in the subsequent section where the loss function is the KL divergence.</p>
</section>
</div>
</details>
<div id="lem-bregman-optimal-prediction" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2</strong></span> (Part 2 of Lemma 0.1 in <span class="citation" data-cites="pfau2013generalized">(<a href="#ref-pfau2013generalized" role="doc-biblioref">Pfau 2013</a>)</span>) The <em>optimal prediction</em> of Bregman divergence can be expressed as: <span class="math display">
    \mathbf{y}_{*} = \arg\min_{\mathbf{y}^{\prime}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [D_{F} (\mathbf{t}, \mathbf{y}^{\prime})] = \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}].
</span></p>
</div>
<details>
<summary>
Detailed proof
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is quite straight-forward. One can calculate the gradient and solve for the root of the gradient as follows: <span class="math display">
    \begin{aligned}
        \nabla_{\mathbf{y}^{\prime}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [D_{F} (\mathbf{t}, \mathbf{y}^{\prime})] &amp; = \nabla_{\mathbf{y}^{\prime}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ F(\mathbf{t}) - F(\mathbf{y}^{\prime}) - \nabla^{\top} F(\mathbf{y}^{\prime}) \, (\mathbf{t} - \mathbf{y}^{\prime}) ]\\
        &amp; = - \nabla_{\mathbf{y}^{\prime}} F(\mathbf{y}^{\prime}) - \nabla^{2} F(\mathbf{y}^{\prime}) \times \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}] + \nabla^{2} F(\mathbf{y}^{\prime}) \times \mathbf{y}^{\prime} + \nabla_{\mathbf{y}^{\prime}} F(\mathbf{y}^{\prime}) \\
        &amp; = \nabla^{2} F(\mathbf{y}^{\prime}) (\mathbf{y}^{\prime} - \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}]) = 0
    \end{aligned}
</span> And since <span class="math inline">F(.)</span> is strictly convex, its Hessian matrix <span class="math inline">\nabla^{2} F(\mathbf{y}^{\prime})</span> is positive definite and invertible. Hence, one can imply that: <span class="math display">
    \mathbf{y}^{\prime} = \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}].
</span></p>
</div>
</details>
<div id="lem-expected-bregman-divergence" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 3</strong></span> (Part 1 of Theorem 0.1 in <span class="citation" data-cites="pfau2013generalized">(<a href="#ref-pfau2013generalized" role="doc-biblioref">Pfau 2013</a>)</span>) The expected Bregman divergences w.r.t. the set of training sets <span class="math inline">\mathcal{D}</span> have the following exact decomposition: <span class="math display">
    \mathbb{E}_{\mathcal{D}} [ D_{F} (\mathbf{y}^{\prime}, \mathbf{y})] = D_{F}(\mathbf{y}^{\prime}, \mathbf{y}_{m}) + \mathbb{E}_{\mathcal{D}} [D_{F}(\mathbf{y}_{m}, \mathbf{y})],
</span> where: <span class="math inline">\mathbf{y}_{m} = \arg\min_{\mathbf{y}^{\prime}} \mathbb{E}_{\mathcal{D}} [D_{F}(\mathbf{y}^{\prime}, \mathbf{y})]</span> is the <em>mean prediction</em> of the model of interest, and <span class="math inline">\mathbf{y}^{\prime}</span> is a (random) prediction that is independent from <span class="math inline">\mathcal{D}</span>.</p>
</div>
<details>
<summary>
Detailed proof
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The is quite straight-forward: <span class="math display">
    \begin{aligned}
        &amp; D_{F}(\mathbf{y}^{\prime}, \mathbf{y}_{m}) + \mathbb{E}_{\mathcal{D}} [D_{F}(\mathbf{y}_{m}, \mathbf{y})] \\
        &amp; = F(\mathbf{y}^{\prime}) - F(\mathbf{y}_{m}) - \nabla^{\top} F(\mathbf{y}_{m}) \times (\mathbf{y}^{\prime} - \mathbf{y}_{m}) + \mathbb{E}_{\mathcal{D}} [F(\mathbf{y}_{m}) - F(\mathbf{y}) - \nabla^{\top} F(\mathbf{y}) \times (\mathbf{y}_{m} - \mathbf{y})] \\
        &amp; = F(\mathbf{y}^{\prime}) - \nabla^{\top} F(\mathbf{y}_{m}) \times (\mathbf{y}^{\prime} - \mathbf{y}_{m}) - \mathbb{E}_{\mathcal{D}} [ F(\mathbf{y}) + \nabla^{\top} F(\mathbf{y}) \times (\mathbf{y}_{m} - \mathbf{y})]\\
        &amp; = F(\mathbf{y}^{\prime}) - \mathbb{E}_{\mathcal{D}} [ \nabla^{\top} F(\mathbf{y}) ] \times (\mathbf{y}^{\prime} - \mathbf{y}_{m}) - \mathbb{E}_{\mathcal{D}} [ F(\mathbf{y}) + \nabla^{\top} F(\mathbf{y}) \times (\mathbf{y}_{m} - \mathbf{y})] \\
        &amp; = \mathbb{E}_{\mathcal{D}} [ F(\mathbf{y}^{\prime}) - F(\mathbf{y}) - \mathbb{E}_{\mathcal{D}} [ \nabla^{\top} F(\mathbf{y}) ] \times (\mathbf{y}^{\prime} - \mathbf{y}_{m} + \mathbf{y}_{m} - \mathbf{y}) ]\\
        &amp; = \mathbb{E}_{\mathcal{D}} [ D_{F} (\mathbf{y}^{\prime}, \mathbf{y})].
    \end{aligned}
</span> The third inequality is due to <a href="#lem-bregman-mean-prediction" class="quarto-xref">Lemma&nbsp;1</a>.</p>
</div>
</details>
<div id="lem-expected-bregman-divergence-t" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 4</strong></span> (Part 2 of Theorem 0.1 in <span class="citation" data-cites="pfau2013generalized">(<a href="#ref-pfau2013generalized" role="doc-biblioref">Pfau 2013</a>)</span>) The expected Bregman divergences w.r.t. the underlying label distribution <span class="math inline">p(\mathbf{t} | \mathbf{x})</span> have the following exact decomposition: <span class="math display">
    \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ D_{F} (\mathbf{t}, \mathbf{y})] = D_{F}(\mathbf{y}_{*}, \mathbf{y}) + \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [D_{F}(\mathbf{t}, \mathbf{y}_{*})],
</span> where <span class="math inline">\mathbf{y}_{*} = \arg\min_{\mathbf{y}^{\prime}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [D_{F} (\mathbf{t}, \mathbf{y}^{\prime})] = \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}]</span> is the <em>optimal prediction</em> in <a href="#lem-bregman-optimal-prediction" class="quarto-xref">Lemma&nbsp;2</a>.</p>
</div>
<details>
<summary>
Detailed proof
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is quite straight-forward: <span class="math display">
    \begin{aligned}
        &amp; D_{F}(\mathbf{y}_{*}, \mathbf{y}) + \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [D_{F}(\mathbf{t}, \mathbf{y}_{*})] \\
        &amp; = F(\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}]) - F(\mathbf{y}) - \nabla^{\top} F(\mathbf{y}) \times (\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}] - \mathbf{y}) \\
        &amp; \quad + \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [F(\mathbf{t}) - F(\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}]) - \nabla^{\top} F(\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}]) \times (\mathbf{t} - \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}])] \\
        &amp; = - F(\mathbf{y}) - \nabla^{\top} F(\mathbf{y}) \times (\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}] - \mathbf{y}) + \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [F(\mathbf{t}) - \nabla^{\top} F(\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}]) \times (\mathbf{t} - \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}])] \\
        &amp; = - F(\mathbf{y}) - \nabla^{\top} F(\mathbf{y}) \times (\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [\mathbf{t}] - \mathbf{y}) + \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [F(\mathbf{t})]\\
        &amp; = \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ F(\mathbf{t}) - F(\mathbf{y}) - \nabla^{\top} F(\mathbf{y}) \times (\mathbf{t} - \mathbf{y})] \\
        &amp; = \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ D_{F} (\mathbf{t}, \mathbf{y})].
    \end{aligned}
</span></p>
</div>
</details>
</section>
<section id="decomposition-for-bregman-divergence" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="decomposition-for-bregman-divergence"><span class="header-section-number">5.2</span> Decomposition for Bregman divergence</h3>
<p>The main result of bias-variance decomposition can be shown in the following:</p>
<div id="thm-decomposition-bregma-div" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4</strong></span> The expected Bregman divergence on a set of training set <span class="math inline">\mathcal{D}</span> can be decomposed into: <span class="math display">
\mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [D_{F} (\mathbf{t}, \mathbf{y})] = \textcolor{Crimson}{D_{F} (\mathbf{y}_{*}, \mathbf{y}_{m})} + \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} [D_{F}(\mathbf{y}_{m}, \mathbf{y})]} + \textcolor{Green}{\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} \left[ D_{F} (\mathbf{t}, \mathbf{y}_{*}) \right]}.
</span></p>
</div>
<details>
<summary>
Detailed proof
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is a consequence of the previous lemma: <span class="math display">
    \begin{aligned}
        \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [D_{F} (\mathbf{t}, \mathbf{y})] &amp; = \mathbb{E}_{\mathcal{D}} [D_{F}(\mathbf{y}_{*}, \mathbf{y}) + \textcolor{Green}{\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [D_{F}(\mathbf{t}, \mathbf{y}_{*})]} ] \\
        &amp; = \mathbb{E}_{\mathcal{D}}[ D_{F}(\mathbf{y}_{*}, \mathbf{y})] + \textcolor{Green}{\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [D_{F}(\mathbf{t}, \mathbf{y}_{*})]}\\
        &amp; = \textcolor{Crimson}{D_{F} (\mathbf{y}_{*}, \mathbf{y}_{m})} + \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} [D_{F}(\mathbf{y}_{m}, \mathbf{y})]} + \textcolor{Green}{\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [D_{F}(\mathbf{t}, \mathbf{y}_{*})]}.
    \end{aligned}
</span> The first equality is due to <a href="#lem-expected-bregman-divergence-t" class="quarto-xref">Lemma&nbsp;4</a> and the last equality of the above equation is due to <a href="#lem-expected-bregman-divergence" class="quarto-xref">Lemma&nbsp;3</a>.</p>
</div>
</details>
<section id="square-loss-1" class="level4" data-number="5.2.1">
<h4 data-number="5.2.1" class="anchored" data-anchor-id="square-loss-1"><span class="header-section-number">5.2.1</span> Square loss</h4>
<p>As MSE or square loss is a special instance of Bregman divergence, one can apply <a href="#thm-decomposition-bregma-div" class="quarto-xref">Theorem&nbsp;4</a> to obtain the result for MSE as shown in <a href="#thm-mse" class="quarto-xref">Theorem&nbsp;1</a>.</p>
</section>
</section>
</section>
<section id="kullback-leibler-divergence" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="kullback-leibler-divergence"><span class="header-section-number">6</span> Kullback-Leibler divergence</h2>
<p>KL divergence is a special case of Bregman divergence. However, the analysis done for the Bregman divergence presented in this post is considered on <em>un-bounded</em> support, where the support space for the KL divergence is the probability space. In addition, KL divergence is used to measure the difference between 2 distributions. Such differences result in a different in terms of bias-variance decomposition.</p>
<p>In this section, <span class="math inline">\mathbf{y}_{*}, \mathbf{y}</span> and <span class="math inline">\mathbf{y}_{m}</span> are label distributions or probabilities. They will be replaced by <span class="math inline">p(\mathbf{t} | \mathbf{x}), \hat{p}(\mathbf{t} | \mathbf{x})</span> and <span class="math inline">p_{m}(\mathbf{t} | \mathbf{x})</span>, respectively, to make the formulation easier to understand.</p>
<div id="lem-kl-div-model-prediction" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 5</strong></span> (Main model prediction - Eq. (2.3) in <span class="citation" data-cites="heskes1998bias">(<a href="#ref-heskes1998bias" role="doc-biblioref">Heskes 1998</a>)</span>) The main model prediction when the loss is the KL divergence has the following property: <span class="math display">
    p_{m}(\mathbf{t} | \mathbf{x}) = \arg\min_{q(\mathbf{t} | \mathbf{x})} \mathbb{E}_{\mathcal{D}} [\mathrm{KL} [q(\mathbf{t} | \mathbf{x}) || \hat{p}(\mathbf{t} | \mathbf{x})]] \Rightarrow p_{m}(\mathbf{t} | \mathbf{x}) = \frac{1}{Z} \exp \left[ \mathbb{E}_{\mathcal{D}} [\ln \hat{p}(\mathbf{t} | \mathbf{x})] \right],
</span> where <span class="math inline">Z</span> is a normalization constant independent of model prediction <span class="math inline">\hat{p}(\mathbf{t} | \mathbf{x})</span>.</p>
</div>
<details>
<summary>
Detailed proof
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is similar to <a href="#lem-bregman-mean-prediction" class="quarto-xref">Lemma&nbsp;1</a>, except the constraint <span class="math inline">\sum_{\mathbf{t}} p_{m}(\mathbf{t} | \mathbf{x}) = 1</span> is taken into account. More specifically, the Lagrangean can be written as: <span class="math display">
    \mathsf{L} = \mathbb{E}_{\mathcal{D}} [ \mathrm{KL} [ p_{m}(\mathbf{t} | \mathbf{x}) || \hat{p}(\mathbf{t} | \mathbf{x})]] + \lambda (\pmb{1}^{\top} p_{m}(\mathbf{t} | \mathbf{x}) - 1),
</span> where <span class="math inline">\lambda</span> is the Lagrange multiplier.</p>
<p>At the optimal point, the gradient of the Lagrangean is zero: <span class="math display">
    \begin{aligned}
        \nabla_{p_{m}(\mathbf{t} | \mathbf{x})} \mathsf{L} &amp; = \ln p_{m}(\mathbf{t} | \mathbf{x}) - \mathbb{E}_{\mathcal{D}} [ \ln \hat{p}(\mathbf{t} | \mathbf{x}) ] + \lambda = 0\\
        &amp; \Rightarrow \ln p_{m}(\mathbf{t} | \mathbf{x}) = \mathbb{E}_{\mathcal{D}} [ \ln \hat{p}(\mathbf{t} | \mathbf{x}) ] - \lambda\\
        &amp; \Rightarrow p_{m}(\mathbf{t} | \mathbf{x}) = \underbrace{\frac{1}{\exp(\lambda)}}_{\frac{1}{Z}} \exp[\mathbb{E}_{\mathcal{D}} [ \ln \hat{p}(\mathbf{t} | \mathbf{x}) ]].
    \end{aligned}
</span> Actually, the normalization constant <span class="math inline">Z</span> is the negative variance: <span class="math display">
    \ln Z \times \pmb{1} = \mathbb{E}_{\mathcal{D}} [ \ln \hat{p}(\mathbf{t} | \mathbf{x}) ] - \ln p_{m}(\mathbf{t} | \mathbf{x})] = \mathbb{E}_{\mathcal{D}} \left[ \ln \frac{\hat{p}(\mathbf{t} | \mathbf{x})}{p_{m}(\mathbf{t} | \mathbf{x})} \right].
</span> Note that: <span class="math display">
    \ln Z = \mathbb{E}_{p_{m}(\mathbf{t} | \mathbf{x})} [ \ln Z \times \pmb{1}].
</span> Thus: <span class="math display">
        \ln Z = \mathbb{E}_{p_{m}(\mathbf{t} | \mathbf{x})} \mathbb{E}_{\mathcal{D}} \left[ \ln \frac{\hat{p}(\mathbf{t} | \mathbf{x})}{p_{m}(\mathbf{t} | \mathbf{x})} \right] = - \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} \left[ \mathrm{KL} [p_{m}(\mathbf{t} | \mathbf{x}) || \hat{p}(\mathbf{t} | \mathbf{x})] \right]}.
</span></p>
</div>
</details>
<div id="thm-kl-decomposition" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5</strong></span> (Decomposition for KL divergence) The bias-variance decomposition for KL divergence can be presented as: <span class="math display">
    \mathbb{E}_{\mathcal{D}} [ \mathrm{KL} [p(\mathbf{t} | \mathbf{x}) || \hat{p}(\mathbf{t} | \mathbf{x})] ] = \textcolor{Crimson}{\mathrm{KL} [ p(\mathbf{t} | \mathbf{x}) || p_{m}(\mathbf{t} | \mathbf{x}) ]} + \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} [ \mathrm{KL} [ p_{m}(\mathbf{t} | \mathbf{x}) || \hat{p}(\mathbf{t} | \mathbf{x}) ] ]}.
</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is quite straight-forward from <a href="#lem-kl-div-model-prediction" class="quarto-xref">Lemma&nbsp;5</a>.</p>
</div>
<p>The result in <a href="#thm-kl-decomposition" class="quarto-xref">Theorem&nbsp;5</a> does not consist of an intrinsic noise since the loss defined by KL divergence is based on the true label distribution instead of each sample <span class="math inline">\mathbf{t}</span>. To obtain the wellknown form of bias-variance decomposition based on label <span class="math inline">\mathbf{t}</span>, the negative log likelihood <span class="math inline">-\ln \hat{p}(\mathbf{t} | \mathbf{x})</span> is used as the loss function. Note that <span class="math inline">p_{m}(\mathbf{t} | \mathbf{x})</span> is still defined with KL divergence as the loss function.</p>
<p>From <a href="#lem-kl-div-model-prediction" class="quarto-xref">Lemma&nbsp;5</a>, one can obtain: <span class="math display">
    \mathbb{E}_{\mathcal{D}} [ -\ln \hat{p}(\mathbf{t} | \mathbf{x}) ] = -\ln p_{m}(\mathbf{t} | \mathbf{x}) + \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} [ \mathrm{KL} [ p_{m}(\mathbf{t} | \mathbf{x}) || \hat{p}(\mathbf{t} | \mathbf{x}) ] ]}.
</span></p>
<p>Thus, the negative log-likelihood can be written as: <span class="math display">
    \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ -\ln \hat{p}(\mathbf{t} | \mathbf{x}) ] = -\mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ \ln p_{m}(\mathbf{t} | \mathbf{x})] + \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} [ \mathrm{KL} [ p_{m}(\mathbf{t} | \mathbf{x}) || \hat{p}(\mathbf{t} | \mathbf{x}) ] ]}.
</span></p>
<p>Or: <span class="math display">
    \mathbb{E}_{\mathcal{D}} \mathbb{E}_{p(\mathbf{t} | \mathbf{x})} [ -\ln \hat{p}(\mathbf{t} | \mathbf{x}) ] = \textcolor{Crimson}{\mathrm{KL}[p(\mathbf{t} | \mathbf{x}) || p_{m}(\mathbf{t} | \mathbf{x})]} + \textcolor{MidnightBlue}{\mathbb{E}_{\mathcal{D}} [ \mathrm{KL} [ p_{m}(\mathbf{t} | \mathbf{x}) || \hat{p}(\mathbf{t} | \mathbf{x}) ] ]} + \textcolor{Green}{\mathbb{E}_{p(\mathbf{t} | \mathbf{x})}[-\ln p(\mathbf{t} | \mathbf{x})]}.
</span></p>
<p>The bias-variance decomposition for negative log-likelihood in this case consists of an intrinsic noise term which equals to the Shannon entropy of the true label distribution <span class="math inline">p(\mathbf{t} | \mathbf{x})</span>.</p>
</section>
<section id="conclusion" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">7</span> Conclusion</h2>
<p>In general, the bias - variance decomposition might not be always in the form of bias, variance and noise as commonly seen in MSE. Here, we show that different loss function might have a different decomposition. Nevertheless, the two most common loss functions, i.e., MSE and KL divergence, share a similar form. Note that, one needs to be careful when applying such bias - variance decomposition due to their difference in terms of <em>main model prediction</em> and <em>optimal label</em>.</p>
</section>
<section id="references" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="references"><span class="header-section-number">8</span> References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-domingos2000unified" class="csl-entry" role="listitem">
Domingos, Pedro. 2000. <span>A Unified Bias-Variance Decomposition.</span> In <em>International Conference on Machine Learning</em>, 23138.
</div>
<div id="ref-heskes1998bias" class="csl-entry" role="listitem">
Heskes, Tom. 1998. <span>Bias/Variance Decompositions for Likelihood-Based Estimators.</span> <em>Neural Computation</em> 10 (6): 142533.
</div>
<div id="ref-pfau2013generalized" class="csl-entry" role="listitem">
Pfau, David. 2013. <span>A Generalized Bias-Variance Decomposition for Bregman Divergences.</span> <em>Unpublished Manuscript</em>.
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{nguyen2022,
  author = {Nguyen, Cuong},
  title = {Bias - Variance Decomposition},
  date = {2022-05-03},
  url = {https://cnguyen10.github.io/posts/bias-variance-decomposition},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-nguyen2022" class="csl-entry quarto-appendix-citeas" role="listitem">
Nguyen, Cuong. 2022. <span>Bias - Variance Decomposition.</span> May
3, 2022. <a href="https://cnguyen10.github.io/posts/bias-variance-decomposition">https://cnguyen10.github.io/posts/bias-variance-decomposition</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/cnguyen10\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.algTitle || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        titlePrefix = el.dataset.algTitle;
        titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
        titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
      });
    })(document);
    </script>
  




</body></html>